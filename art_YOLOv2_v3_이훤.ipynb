{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3835f402",
   "metadata": {},
   "source": [
    "## 전체 파이프라인\n",
    "데이터 준비 -> YOLOv2 탐지기 학습(4클래스) -> 탐지 결과로 이미지 크롭 -> 크롭 이미지를 ResNet 분류기로 재분류(보정) -> .pt 모델 저장 + 테스트 2장 결과 저장 -> 추가 이미지 테스트"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6639131",
   "metadata": {},
   "source": [
    "# 데이터 준비"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89a32783",
   "metadata": {},
   "outputs": [],
   "source": [
    "266.AI 기반 아동 미술심리 진단을 위한 그림 데이터 구축/01-1.정식개방데이터\n",
    "    Training\n",
    "        01.원천데이터\n",
    "            TS_나무\n",
    "            TS_남자사람\n",
    "            TS_여자사람\n",
    "            TS_집\n",
    "        02.라벨링데이터\n",
    "            TL_나무\n",
    "            TL_남자사람\n",
    "            TL_여자사람\n",
    "            TL_집\n",
    "    Validation\n",
    "        01.원천데이터\n",
    "            VS_나무\n",
    "            VS_남자사람\n",
    "            VS_여자사람\n",
    "            VS_집\n",
    "        02.라벨링데이터\n",
    "            VL_나무\n",
    "            VL_남자사람\n",
    "            VL_여자사람\n",
    "            VL_집"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0849ae56",
   "metadata": {},
   "source": [
    "# 1. YOLOv2 탐지기 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bda010b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch 버전: 2.5.1+cu121\n",
      "CUDA 사용 가능 여부: True\n",
      "사용 중인 GPU 개수: 1\n",
      "현재 선택된 GPU: 0\n",
      "GPU 이름: NVIDIA GeForce RTX 4060 Laptop GPU\n",
      "GPU 행렬 곱 소요 시간: 0.0793초\n"
     ]
    }
   ],
   "source": [
    "# GPU 사용 가능 여부 점검\n",
    "import torch\n",
    "\n",
    "print(\"PyTorch 버전:\", torch.__version__)\n",
    "print(\"CUDA 사용 가능 여부:\", torch.cuda.is_available())\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(\"사용 중인 GPU 개수:\", torch.cuda.device_count())\n",
    "    print(\"현재 선택된 GPU:\", torch.cuda.current_device())\n",
    "    print(\"GPU 이름:\", torch.cuda.get_device_name(torch.cuda.current_device()))\n",
    "\n",
    "    # 간단한 연산으로 GPU 동작 테스트\n",
    "    x = torch.rand((5000, 5000), device='cuda')\n",
    "    y = torch.rand((5000, 5000), device='cuda')\n",
    "    torch.cuda.synchronize()\n",
    "    import time\n",
    "    start = time.time()\n",
    "    z = torch.matmul(x, y)\n",
    "    torch.cuda.synchronize()\n",
    "    print(\"GPU 행렬 곱 소요 시간: {:.4f}초\".format(time.time() - start))\n",
    "else:\n",
    "    print(\"⚠ GPU(CUDA)를 사용할 수 없습니다. CPU를 사용 중입니다.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7acd0f9",
   "metadata": {},
   "source": [
    "## 1-1. YOLOv2 학습을 위한 라벨 변환\n",
    "YOLOv2 포맷(txt)는 이미지 크기로 정규화한 객체의 중심 형식이다.\n",
    "라벨링 데이터를 읽어, 원천 데이터의 이미지를 찾아 YOLO txt를 만든다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7569a75b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training JSON→YOLO: 100%|██████████| 44800/44800 [08:29<00:00, 87.86file/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Training] 변환: 44800개 | 스킵(라벨없음): 0 | 스킵(이미지없음): 0 | 스킵(해상도확인실패): 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation JSON→YOLO: 100%|██████████| 5600/5600 [01:05<00:00, 85.15file/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Validation] 변환: 5600개 | 스킵(라벨없음): 0 | 스킵(이미지없음): 0 | 스킵(해상도확인실패): 0\n",
      "\n",
      "== 전체 요약 ==\n",
      "Training: 44800개 (no_label 0, no_img 0, no_wh 0)\n",
      "Validation: 5600개 (no_label 0, no_img 0, no_wh 0)\n",
      "✅ 완료: yolo_training/, yolo_validation/ 에 images/ + labels/ 생성\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# JSON → YOLO txt 변환 (집/나무/남자/여자 4클래스), 이미지 링크/복사까지\n",
    "from pathlib import Path\n",
    "import json, os, shutil\n",
    "from tqdm import tqdm\n",
    "\n",
    "# === 경로 루트 ===\n",
    "ROOT = Path(\"266.AI 기반 아동 미술심리 진단을 위한 그림 데이터 구축/01-1.정식개방데이터\")\n",
    "\n",
    "# === 고정 클래스 인덱스(모델/라벨 일치용) ===\n",
    "CLASSES = {\"tree\":0, \"man\":1, \"woman\":2, \"house\":3}\n",
    "\n",
    "# === JSON 내부 라벨명 → 우리가 쓸 대상 라벨 키 ===\n",
    "TARGET_KEYS = {\n",
    "    \"house\":  (\"집전체\",   \"house\"),\n",
    "    \"tree\":   (\"나무전체\", \"tree\"),\n",
    "    \"man\":    (\"사람전체\", \"man\"),\n",
    "    \"woman\":  (\"사람전체\", \"woman\"),\n",
    "}\n",
    "\n",
    "# === 스플릿별 실제 폴더 위치 매핑 ===\n",
    "SPLITS = {\n",
    "    \"Training\": {\n",
    "        \"origin\": ROOT / \"Training\" / \"01.원천데이터\",         # TS_나무, TS_남자사람, TS_여자사람, TS_집\n",
    "        \"label\":  ROOT / \"Training\" / \"02.라벨링데이터\",        # TL_나무, TL_남자사람, TL_여자사람, TL_집\n",
    "        \"out\":    Path(\"yolo_training\"),\n",
    "        \"origin_sub_prefix\": \"TS_\",   # 원천데이터 하위 폴더 접두사\n",
    "        \"label_sub_prefix\":  \"TL_\",   # 라벨링데이터 하위 폴더 접두사\n",
    "    },\n",
    "    \"Validation\": {\n",
    "        \"origin\": ROOT / \"Validation\" / \"01.원천데이터\",       # VS_나무, VS_남자사람, VS_여자사람, VS_집\n",
    "        \"label\":  ROOT / \"Validation\" / \"02.라벨링데이터\",      # VL_나무, VL_남자사람, VL_여자사람, VL_집\n",
    "        \"out\":    Path(\"yolo_validation\"),\n",
    "        \"origin_sub_prefix\": \"VS_\",\n",
    "        \"label_sub_prefix\":  \"VL_\",\n",
    "    }\n",
    "}\n",
    "\n",
    "# === 한국어 클래스명 매핑(폴더명용) ===\n",
    "KO_CLASS = {\"house\":\"집\", \"tree\":\"나무\", \"man\":\"남자사람\", \"woman\":\"여자사람\"}\n",
    "\n",
    "# === 속도/용량 옵션: 복사 대신 하드링크(같은 드라이브일 때 매우 빠름) ===\n",
    "USE_HARDLINK = True   # 같은 파일시스템이면 권장\n",
    "USE_SYMLINK  = False  # Windows에선 권한 필요할 수 있음\n",
    "SKIP_COPY    = False  # True면 이미지 링크/복사 생략(원본만 사용하고 싶을 때)\n",
    "\n",
    "def make_link_or_copy(src: Path, dst: Path):\n",
    "    if SKIP_COPY or dst.exists():\n",
    "        return\n",
    "    dst.parent.mkdir(parents=True, exist_ok=True)\n",
    "    try:\n",
    "        if USE_HARDLINK:\n",
    "            os.link(src, dst)     # 같은 드라이브/파티션이어야 함\n",
    "        elif USE_SYMLINK:\n",
    "            os.symlink(src, dst)\n",
    "        else:\n",
    "            shutil.copy2(src, dst)\n",
    "    except Exception:\n",
    "        # 실패 시 안전하게 복사\n",
    "        shutil.copy2(src, dst)\n",
    "\n",
    "def yolo_line(cls_idx, x, y, w, h, W, H):\n",
    "    # YOLO 포맷: class xc yc w h (0~1 정규화)\n",
    "    xc = (x + w/2) / W\n",
    "    yc = (y + h/2) / H\n",
    "    nw = w / W\n",
    "    nh = h / H\n",
    "    return f\"{cls_idx} {xc:.6f} {yc:.6f} {nw:.6f} {nh:.6f}\\n\"\n",
    "\n",
    "def parse_wh_from_json(meta):\n",
    "    # \"1280x1280\" 같은 문자열을 우선 신뢰(이미지 열지 않아도 됨 → 매우 빠름)\n",
    "    res = (meta or {}).get(\"img_resolution\") or \"\"\n",
    "    if \"x\" in res:\n",
    "        try:\n",
    "            w, h = res.split(\"x\")\n",
    "            return int(w), int(h)\n",
    "        except Exception:\n",
    "            return None, None\n",
    "    return None, None\n",
    "\n",
    "def index_origin_images(origin_root: Path):\n",
    "    # 원천데이터 전체를 1회 스캔해 stem → 경로 인덱스 생성 (확장자 혼용 대비)\n",
    "    stem2path = {}\n",
    "    for p in origin_root.rglob(\"*\"):\n",
    "        if p.is_file() and p.suffix.lower() in {\".jpg\",\".jpeg\",\".png\"}:\n",
    "            # 동일 stem이 여러 번 나오면 첫 번째만 사용(일반적으로 중복 없음)\n",
    "            stem2path.setdefault(p.stem, p)\n",
    "    return stem2path\n",
    "\n",
    "summary = {}\n",
    "for split_name, cfg in SPLITS.items():\n",
    "    origin_root = cfg[\"origin\"]\n",
    "    label_root  = cfg[\"label\"]\n",
    "    out_root    = cfg[\"out\"]\n",
    "    out_img_dir = out_root / \"images\"\n",
    "    out_lbl_dir = out_root / \"labels\"\n",
    "    out_img_dir.mkdir(parents=True, exist_ok=True)\n",
    "    out_lbl_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    if not origin_root.exists() or not label_root.exists():\n",
    "        print(f\"[{split_name}] 경로 확인 필요 - origin:{origin_root.exists()} | label:{label_root.exists()}\")\n",
    "        continue\n",
    "\n",
    "    # 1) 원천데이터 인덱스(한 번만 스캔)\n",
    "    stem2path = index_origin_images(origin_root)\n",
    "\n",
    "    # 2) 라벨 JSON 경로 모으기\n",
    "    json_files = []\n",
    "    for cls in [\"house\",\"man\",\"tree\",\"woman\"]:\n",
    "        ko = KO_CLASS[cls]\n",
    "        # 스플릿에 맞는 접두사 폴더(TL_/VL_) 찾기\n",
    "        label_dir = label_root / f\"{cfg['label_sub_prefix']}{ko}\"\n",
    "        if label_dir.exists():\n",
    "            json_files += sorted(label_dir.glob(\"*.json\"))\n",
    "        else:\n",
    "            print(f\"[{split_name}] 라벨 폴더 없음: {label_dir}\")\n",
    "\n",
    "    # 3) 변환 루프\n",
    "    written, skip_no_label, skip_no_img, skip_no_wh = 0, 0, 0, 0\n",
    "\n",
    "    for js in tqdm(json_files, desc=f\"{split_name} JSON→YOLO\", unit=\"file\"):\n",
    "        try:\n",
    "            data = json.loads(js.read_text(encoding=\"utf-8\"))\n",
    "        except Exception:\n",
    "            continue\n",
    "\n",
    "        # (a) 대상 클래스/라벨 키 결정\n",
    "        # 현재 json이 어느 폴더(집/나무/남자/여자)에서 왔는지로 클래스 판정\n",
    "        if \"집\" in js.parts:\n",
    "            key_label, class_name = TARGET_KEYS[\"house\"]\n",
    "        elif \"나무\" in js.parts:\n",
    "            key_label, class_name = TARGET_KEYS[\"tree\"]\n",
    "        elif \"남자사람\" in js.parts:\n",
    "            key_label, class_name = TARGET_KEYS[\"man\"]\n",
    "        elif \"여자사람\" in js.parts:\n",
    "            key_label, class_name = TARGET_KEYS[\"woman\"]\n",
    "        else:\n",
    "            # 혹시 모를 예외: 파일명에 한글 클래스가 안 보이면 meta.class 참고\n",
    "            meta_cls = (data.get(\"annotations\") or {}).get(\"class\", \"\")\n",
    "            if meta_cls == \"집\":\n",
    "                key_label, class_name = TARGET_KEYS[\"house\"]\n",
    "            elif meta_cls == \"나무\":\n",
    "                key_label, class_name = TARGET_KEYS[\"tree\"]\n",
    "            elif meta_cls == \"남자사람\":\n",
    "                key_label, class_name = TARGET_KEYS[\"man\"]\n",
    "            elif meta_cls == \"여자사람\":\n",
    "                key_label, class_name = TARGET_KEYS[\"woman\"]\n",
    "            else:\n",
    "                continue\n",
    "        cls_idx = CLASSES[class_name]\n",
    "\n",
    "        # (b) 타깃 박스(집전체/나무전체/사람전체)만 추출\n",
    "        boxes = []\n",
    "        for b in (data.get(\"annotations\") or {}).get(\"bbox\", []):\n",
    "            if b.get(\"label\") == key_label:\n",
    "                boxes.append(b)\n",
    "        if not boxes:\n",
    "            skip_no_label += 1\n",
    "            continue\n",
    "\n",
    "        # (c) 이미지 찾기: stem 기반(라벨 파일명과 동일 stem)\n",
    "        stem = js.stem\n",
    "        img_path = stem2path.get(stem)\n",
    "        if img_path is None:\n",
    "            skip_no_img += 1\n",
    "            continue\n",
    "\n",
    "        # (d) 이미지 크기: JSON의 img_resolution 사용(빠름)\n",
    "        W, H = parse_wh_from_json(data.get(\"meta\") or {})\n",
    "        if not W or not H:\n",
    "            # 해상도 정보가 없으면 이미지 열어 크기 확인(느리지만 호환)\n",
    "            try:\n",
    "                from PIL import Image\n",
    "                with Image.open(img_path) as im:\n",
    "                    W, H = im.size\n",
    "            except Exception:\n",
    "                skip_no_wh += 1\n",
    "                continue\n",
    "\n",
    "        # (e) YOLO 라벨 작성(경계 클램프)\n",
    "        lines = []\n",
    "        for b in boxes:\n",
    "            x = float(b[\"x\"]); y = float(b[\"y\"])\n",
    "            w = float(b[\"w\"]); h = float(b[\"h\"])\n",
    "            x = max(0, min(x, W-1))\n",
    "            y = max(0, min(y, H-1))\n",
    "            w = max(1, min(w, W - x))\n",
    "            h = max(1, min(h, H - y))\n",
    "            lines.append(yolo_line(cls_idx, x, y, w, h, W, H))\n",
    "        if not lines:\n",
    "            skip_no_label += 1\n",
    "            continue\n",
    "\n",
    "        # (f) 저장: 이미지 링크/복사 + 라벨 txt\n",
    "        out_img = out_img_dir / img_path.name\n",
    "        out_lbl = out_lbl_dir / (img_path.stem + \".txt\")\n",
    "        make_link_or_copy(img_path, out_img)\n",
    "        out_lbl.write_text(\"\".join(lines), encoding=\"utf-8\")\n",
    "        written += 1\n",
    "\n",
    "    summary[split_name] = dict(\n",
    "        written=written,\n",
    "        skip_no_label=skip_no_label,\n",
    "        skip_no_img=skip_no_img,\n",
    "        skip_no_wh=skip_no_wh\n",
    "    )\n",
    "    print(f\"\\n[{split_name}] 변환: {written}개 | 스킵(라벨없음): {skip_no_label} | 스킵(이미지없음): {skip_no_img} | 스킵(해상도확인실패): {skip_no_wh}\")\n",
    "\n",
    "print(\"\\n== 전체 요약 ==\")\n",
    "for k, v in summary.items():\n",
    "    print(f\"{k}: {v['written']}개 (no_label {v['skip_no_label']}, no_img {v['skip_no_img']}, no_wh {v['skip_no_wh']})\")\n",
    "print(\"✅ 완료: yolo_training/, yolo_validation/ 에 images/ + labels/ 생성\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e43f800",
   "metadata": {},
   "source": [
    "## 1-2. 환경/경로/클래스 설정 + 하이퍼파라미터"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "01fd89b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEVICE: cuda\n"
     ]
    }
   ],
   "source": [
    "# ===== 셀 1: 설정/하이퍼파라미터 =====\n",
    "import os, math, json, random, time\n",
    "from pathlib import Path\n",
    "from typing import List, Tuple\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image, ImageDraw\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.ops import nms\n",
    "\n",
    "# 재현성\n",
    "SEED = 42\n",
    "random.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED); torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "# 경로 (변환된 YOLO 데이터)\n",
    "TRAIN_ROOT = Path(\"yolo_training\")\n",
    "VAL_ROOT   = Path(\"yolo_validation\")\n",
    "IMG_DIRNAME = \"images\"\n",
    "LBL_DIRNAME = \"labels\"\n",
    "\n",
    "# 클래스\n",
    "CLASS2IDX = {\"tree\":0, \"man\":1, \"woman\":2, \"house\":3}\n",
    "IDX2CLASS = {v:k for k,v in CLASS2IDX.items()}\n",
    "NUM_CLASSES = len(CLASS2IDX)\n",
    "\n",
    "# 입력/그리드\n",
    "IMG_SIZE  = 416\n",
    "GRID_SIZE = 13\n",
    "STRIDE    = IMG_SIZE // GRID_SIZE\n",
    "\n",
    "# 앵커 개수\n",
    "NUM_ANCHORS = 5\n",
    "\n",
    "# 하이퍼파라미터\n",
    "BATCH_SIZE    = 16\n",
    "EPOCHS        = 30            # 빠르게 확인하려면 5~10으로 시작\n",
    "LR            = 1e-3\n",
    "WEIGHT_DECAY  = 5e-4\n",
    "WARMUP_EPOCHS = 2\n",
    "VAL_EVERY     = 3             # 검증 주기(에폭 단위) – 시간 절약용\n",
    "\n",
    "LAMBDA_COORD  = 5.0\n",
    "LAMBDA_NOOBJ  = 0.5\n",
    "IGNORE_IOU    = 0.5\n",
    "\n",
    "# 디바이스\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"DEVICE:\", DEVICE)\n",
    "\n",
    "# 가속 옵션\n",
    "torch.backends.cudnn.benchmark = True   # 입력 크기 고정 시 속도↑\n",
    "cv2.setNumThreads(0)                    # OpenCV 스레드 줄여 워커와 경쟁↓\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5d3b271c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(44800, 5600)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ===== 셀 2: Dataset/전처리 + DataLoader =====\n",
    "from typing import Optional\n",
    "\n",
    "def letterbox(im: np.ndarray, new_size=416, color=(114,114,114)):\n",
    "    h, w = im.shape[:2]\n",
    "    scale = min(new_size / h, new_size / w)\n",
    "    nh, nw = int(round(h * scale)), int(round(w * scale))\n",
    "    im_resized = cv2.resize(im, (nw, nh), interpolation=cv2.INTER_LINEAR)\n",
    "    top = (new_size - nh) // 2\n",
    "    bottom = new_size - nh - top\n",
    "    left = (new_size - nw) // 2\n",
    "    right = new_size - nw - left\n",
    "    im_padded = cv2.copyMakeBorder(im_resized, top, bottom, left, right, cv2.BORDER_CONSTANT, value=color)\n",
    "    return im_padded, scale, left, top\n",
    "\n",
    "def load_labels(txt_path: Path):\n",
    "    boxes = []\n",
    "    if not txt_path.exists():\n",
    "        return boxes\n",
    "    with open(txt_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            ss = line.strip().split()\n",
    "            if len(ss) != 5:\n",
    "                continue\n",
    "            cls = int(ss[0]); xc = float(ss[1]); yc = float(ss[2]); w = float(ss[3]); h = float(ss[4])\n",
    "            boxes.append([cls, xc, yc, w, h])\n",
    "    return boxes\n",
    "\n",
    "# 견고한 이미지 로더: cv2 → imdecode → PIL\n",
    "def imread_robust(path: Path) -> Optional[np.ndarray]:\n",
    "    img = cv2.imread(str(path))\n",
    "    if img is not None:\n",
    "        return img\n",
    "    try:\n",
    "        data = np.fromfile(str(path), dtype=np.uint8)\n",
    "        img  = cv2.imdecode(data, cv2.IMREAD_COLOR)\n",
    "        if img is not None:\n",
    "            return img\n",
    "    except Exception:\n",
    "        pass\n",
    "    try:\n",
    "        img = Image.open(path).convert(\"RGB\")\n",
    "        img = np.array(img)[:, :, ::-1].copy()  # RGB→BGR\n",
    "        return img\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "class YOLODataset(Dataset):\n",
    "    def __init__(self, root: Path, img_size=416, augment=False):\n",
    "        self.img_dir = root / IMG_DIRNAME\n",
    "        self.lbl_dir = root / LBL_DIRNAME\n",
    "        self.img_paths = sorted([p for p in self.img_dir.glob(\"*\") if p.suffix.lower() in [\".jpg\",\".jpeg\",\".png\"]])\n",
    "        self.img_size = img_size\n",
    "        self.augment = augment\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.img_paths[idx]\n",
    "        lbl_path = (self.lbl_dir / img_path.stem).with_suffix(\".txt\")\n",
    "\n",
    "        img = imread_robust(img_path)\n",
    "        if img is None:\n",
    "            raise FileNotFoundError(f\"[imread_robust 실패] {img_path}\")\n",
    "\n",
    "        img, scale, padw, padh = letterbox(img, self.img_size)\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB).astype(np.float32) / 255.0\n",
    "\n",
    "        labels = load_labels(lbl_path)\n",
    "        labels = np.array(labels, dtype=np.float32) if labels else np.zeros((0,5), dtype=np.float32)\n",
    "\n",
    "        img = torch.from_numpy(img).permute(2,0,1)\n",
    "        labels = torch.from_numpy(labels)\n",
    "        return img_path.name, img, labels\n",
    "\n",
    "def collate_fn(batch):\n",
    "    names, imgs, labels = zip(*batch)\n",
    "    imgs = torch.stack(imgs, 0)\n",
    "    return names, imgs, labels\n",
    "\n",
    "train_ds = YOLODataset(TRAIN_ROOT, img_size=IMG_SIZE, augment=True)\n",
    "val_ds   = YOLODataset(VAL_ROOT,   img_size=IMG_SIZE, augment=False)\n",
    "\n",
    "# DataLoader – 윈도우면 2~4 권장\n",
    "NUM_WORKERS = max(2, min(8, (os.cpu_count() or 8) // 2))\n",
    "train_dl = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True,\n",
    "                      num_workers=NUM_WORKERS, pin_memory=True,\n",
    "                      persistent_workers=True, prefetch_factor=2,\n",
    "                      collate_fn=collate_fn)\n",
    "val_dl   = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False,\n",
    "                      num_workers=max(2, NUM_WORKERS//2), pin_memory=True,\n",
    "                      persistent_workers=True, prefetch_factor=2,\n",
    "                      collate_fn=collate_fn)\n",
    "\n",
    "len(train_ds), len(val_ds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "00525837",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "수집한 w,h 개수: 30001 (파일 30000/44800)\n",
      "앵커(픽셀, WxH):\n",
      " [[ 87.1    143.9751]\n",
      " [117.3249 228.1502]\n",
      " [163.1502 280.8   ]\n",
      " [219.3751 333.1249]\n",
      " [297.7    380.9   ]]\n"
     ]
    }
   ],
   "source": [
    "# ===== 셀 3: 빠른 앵커 계산 (라벨 txt만, 멀티스레드) =====\n",
    "import random\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "LABELS_DIR = TRAIN_ROOT / LBL_DIRNAME   # yolo_training/labels\n",
    "IMG_SIZE_  = IMG_SIZE\n",
    "K          = NUM_ANCHORS\n",
    "MAX_FILES  = 30000       # 파일 샘플 상한(속도용)\n",
    "MAX_BOXES  = 300000      # 전체 박스 상한(속도/메모리용)\n",
    "WORKERS    = min(32, (os.cpu_count() or 8) * 2)\n",
    "\n",
    "all_txts = [p for p in LABELS_DIR.iterdir() if p.suffix.lower()==\".txt\"]\n",
    "random.shuffle(all_txts)\n",
    "txts = all_txts[:MAX_FILES]\n",
    "\n",
    "def parse_wh(txt_path: Path):\n",
    "    wh_local = []\n",
    "    try:\n",
    "        with open(txt_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            for line in f:\n",
    "                ss = line.strip().split()\n",
    "                if len(ss)==5:\n",
    "                    w = float(ss[3]) * IMG_SIZE_\n",
    "                    h = float(ss[4]) * IMG_SIZE_\n",
    "                    if w>0 and h>0:\n",
    "                        wh_local.append((w,h))\n",
    "    except Exception:\n",
    "        pass\n",
    "    return wh_local\n",
    "\n",
    "wh_list = []\n",
    "with ThreadPoolExecutor(max_workers=WORKERS) as ex:\n",
    "    futures = [ex.submit(parse_wh, p) for p in txts]\n",
    "    for fu in as_completed(futures):\n",
    "        wh_list.extend(fu.result())\n",
    "        if len(wh_list) >= MAX_BOXES:\n",
    "            break\n",
    "\n",
    "wh = np.array(wh_list, dtype=np.float32)\n",
    "print(f\"수집한 w,h 개수: {len(wh)} (파일 {len(txts)}/{len(all_txts)})\")\n",
    "\n",
    "def iou_wh(wh1, wh2):\n",
    "    w1, h1 = wh1[:,0][:,None], wh1[:,1][:,None]\n",
    "    w2, h2 = wh2[:,0][None,:], wh2[:,1][None,:]\n",
    "    inter  = np.minimum(w1, w2) * np.minimum(h1, h2)\n",
    "    area1  = w1*h1; area2 = w2*h2\n",
    "    return inter / (area1 + area2 - inter + 1e-9)\n",
    "\n",
    "def kmeanspp_init(data, k):\n",
    "    centroids = [data[np.random.randint(len(data))]]\n",
    "    for _ in range(1, k):\n",
    "        d2 = np.min([np.sum((data - c)**2, axis=1) for c in centroids], axis=0)\n",
    "        probs = d2 / (d2.sum() + 1e-9)\n",
    "        idx = np.random.choice(len(data), p=probs)\n",
    "        centroids.append(data[idx])\n",
    "    return np.stack(centroids, axis=0)\n",
    "\n",
    "if len(wh) < K:\n",
    "    print(\"라벨이 부족해서 기본 앵커 사용\")\n",
    "    anchors = np.array([[12,16],[19,36],[40,28],[36,75],[76,55]], dtype=np.float32)\n",
    "else:\n",
    "    centroids = kmeanspp_init(wh, K)\n",
    "    for _ in range(25):  # 이터레이션 단축(보통 충분)\n",
    "        iou = iou_wh(wh, centroids)\n",
    "        clusters = np.argmax(iou, axis=1)\n",
    "        new_centroids = []\n",
    "        changed = False\n",
    "        for ki in range(K):\n",
    "            pts = wh[clusters==ki]\n",
    "            if len(pts)==0:\n",
    "                new_centroids.append(centroids[ki])\n",
    "            else:\n",
    "                med = np.median(pts, axis=0)\n",
    "                new_centroids.append(med)\n",
    "                if np.any(np.abs(med - centroids[ki]) > 1e-3):\n",
    "                    changed = True\n",
    "        centroids = np.array(new_centroids)\n",
    "        if not changed:\n",
    "            break\n",
    "    order = np.argsort(centroids.prod(axis=1))\n",
    "    anchors = centroids[order]\n",
    "\n",
    "print(\"앵커(픽셀, WxH):\\n\", anchors)\n",
    "\n",
    "# 학습에서 사용할 텐서\n",
    "ANCHORS = torch.tensor(anchors, dtype=torch.float32).to(DEVICE)\n",
    "\n",
    "# (선택) 캐시\n",
    "# np.save(\"anchors.npy\", anchors)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "119cfc27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13.305293, 'M params')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ===== 셀 4: YOLOv2 간단 모델 =====\n",
    "def conv_bn_lrelu(c_in, c_out, k=3, s=1, p=1):\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(c_in, c_out, k, s, p, bias=False),\n",
    "        nn.BatchNorm2d(c_out),\n",
    "        nn.LeakyReLU(0.1, inplace=True),\n",
    "    )\n",
    "\n",
    "class YOLOv2Tiny(nn.Module):\n",
    "    def __init__(self, num_classes=4, num_anchors=5):\n",
    "        super().__init__()\n",
    "        c = [32, 64, 128, 256, 512, 1024]\n",
    "        self.layer1 = nn.Sequential(\n",
    "            conv_bn_lrelu(3, c[0], 3,1,1),\n",
    "            nn.MaxPool2d(2,2),   # 208\n",
    "            conv_bn_lrelu(c[0], c[1], 3,1,1),\n",
    "            nn.MaxPool2d(2,2),   # 104\n",
    "            conv_bn_lrelu(c[1], c[2], 3,1,1),\n",
    "            conv_bn_lrelu(c[2], c[1], 1,1,0),\n",
    "            conv_bn_lrelu(c[1], c[2], 3,1,1),\n",
    "            nn.MaxPool2d(2,2),   # 52\n",
    "            conv_bn_lrelu(c[2], c[3], 3,1,1),\n",
    "            conv_bn_lrelu(c[3], c[2], 1,1,0),\n",
    "            conv_bn_lrelu(c[2], c[3], 3,1,1),\n",
    "            nn.MaxPool2d(2,2),   # 26\n",
    "            conv_bn_lrelu(c[3], c[4], 3,1,1),\n",
    "            conv_bn_lrelu(c[4], c[3], 1,1,0),\n",
    "            conv_bn_lrelu(c[3], c[4], 3,1,1),\n",
    "            nn.MaxPool2d(2,2),   # 13\n",
    "            conv_bn_lrelu(c[4], c[5], 3,1,1),\n",
    "            conv_bn_lrelu(c[5], c[4], 1,1,0),\n",
    "            conv_bn_lrelu(c[4], c[5], 3,1,1),\n",
    "        )\n",
    "        out_ch = num_anchors * (5 + num_classes)\n",
    "        self.head = nn.Conv2d(c[5], out_ch, 1,1,0)\n",
    "\n",
    "        self.num_classes = num_classes\n",
    "        self.num_anchors = num_anchors\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layer1(x)\n",
    "        x = self.head(x)  # [B, A*(5+C), 13, 13]\n",
    "        return x\n",
    "\n",
    "model = YOLOv2Tiny(num_classes=NUM_CLASSES, num_anchors=NUM_ANCHORS).to(DEVICE)\n",
    "sum(p.numel() for p in model.parameters())/1e6, \"M params\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0fe74d79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== 셀 5: 타깃 할당 & 손실 (with logits) =====\n",
    "def build_targets(labels_list, anchors, S=13, num_classes=4):\n",
    "    B = len(labels_list)\n",
    "    A = anchors.size(0)\n",
    "    target = torch.zeros(B, A, S, S, 5+num_classes, device=DEVICE)\n",
    "    for b_idx, labels in enumerate(labels_list):\n",
    "        if labels is None or len(labels)==0:\n",
    "            continue\n",
    "        labels = labels.to(DEVICE).clone()\n",
    "        labels[:,1:] *= torch.tensor([IMG_SIZE, IMG_SIZE, IMG_SIZE, IMG_SIZE], device=DEVICE)\n",
    "        for cls, xc, yc, bw, bh in labels:\n",
    "            gi = int(xc // STRIDE); gj = int(yc // STRIDE)\n",
    "            gi = min(max(gi,0), S-1); gj = min(max(gj,0), S-1)\n",
    "            box = torch.tensor([bw, bh], device=DEVICE)[None,:]\n",
    "            inter = torch.min(box[:,0], anchors[:,0]) * torch.min(box[:,1], anchors[:,1])\n",
    "            area1 = box[:,0]*box[:,1]; area2 = anchors[:,0]*anchors[:,1]\n",
    "            iou = inter / (area1 + area2 - inter + 1e-16)\n",
    "            a = torch.argmax(iou).item()\n",
    "            tx = (xc / STRIDE) - gi\n",
    "            ty = (yc / STRIDE) - gj\n",
    "            tw = torch.log(bw / anchors[a,0] + 1e-16)\n",
    "            th = torch.log(bh / anchors[a,1] + 1e-16)\n",
    "            target[b_idx, a, gj, gi, 0] = tx\n",
    "            target[b_idx, a, gj, gi, 1] = ty\n",
    "            target[b_idx, a, gj, gi, 2] = tw\n",
    "            target[b_idx, a, gj, gi, 3] = th\n",
    "            target[b_idx, a, gj, gi, 4] = 1.0\n",
    "            target[b_idx, a, gj, gi, 5 + int(cls.item())] = 1.0\n",
    "    return target\n",
    "\n",
    "\n",
    "class YOLOv2Loss(nn.Module):\n",
    "    def __init__(self, anchors, num_classes=4, lambda_coord=5.0, lambda_noobj=0.5, ignore_iou=0.5):\n",
    "        super().__init__()\n",
    "        self.anchors = anchors\n",
    "        self.num_classes = num_classes\n",
    "        self.lambda_coord = lambda_coord\n",
    "        self.lambda_noobj = lambda_noobj\n",
    "        self.ignore_iou = ignore_iou\n",
    "\n",
    "    def forward(self, pred, target):\n",
    "        B, _, S, _ = pred.shape\n",
    "        A = self.anchors.size(0); C = self.num_classes\n",
    "\n",
    "        # [B,A,S,S,5+C]\n",
    "        pred = pred.reshape(B, A, 5+C, S, S).permute(0,1,3,4,2)\n",
    "\n",
    "        # 좌표: px,py는 sigmoid, pw,ph는 log-스페이스\n",
    "        px = torch.sigmoid(pred[...,0]); py = torch.sigmoid(pred[...,1])\n",
    "        pw = pred[...,2];               ph = pred[...,3]\n",
    "\n",
    "        # objectness/cls는 \"로짓\" 그대로 두고 BCEWithLogits 사용\n",
    "        lo          = pred[...,4]        # objectness logits\n",
    "        pcls_logits = pred[...,5:]       # class logits\n",
    "\n",
    "        # 타깃\n",
    "        tx,ty,tw,th = target[...,0],target[...,1],target[...,2],target[...,3]\n",
    "        tobj, tcls  = target[...,4], target[...,5:]\n",
    "        obj_mask = tobj.bool()\n",
    "\n",
    "        # coord loss\n",
    "        loss_x = F.mse_loss(px[obj_mask], tx[obj_mask], reduction='sum') if obj_mask.any() else torch.tensor(0., device=pred.device)\n",
    "        loss_y = F.mse_loss(py[obj_mask], ty[obj_mask], reduction='sum') if obj_mask.any() else torch.tensor(0., device=pred.device)\n",
    "        loss_w = F.mse_loss(pw[obj_mask], tw[obj_mask], reduction='sum') if obj_mask.any() else torch.tensor(0., device=pred.device)\n",
    "        loss_h = F.mse_loss(ph[obj_mask], th[obj_mask], reduction='sum') if obj_mask.any() else torch.tensor(0., device=pred.device)\n",
    "        loss_coord = self.lambda_coord * (loss_x + loss_y + loss_w + loss_h)\n",
    "\n",
    "        # cls loss (with logits)\n",
    "        loss_cls = F.binary_cross_entropy_with_logits(pcls_logits[obj_mask], tcls[obj_mask], reduction='sum') \\\n",
    "                   if obj_mask.any() else torch.tensor(0., device=pred.device)\n",
    "\n",
    "        # ignore_iou 계산용 보조\n",
    "        grid_y, grid_x = torch.meshgrid(torch.arange(S, device=pred.device), torch.arange(S, device=pred.device), indexing='ij')\n",
    "        grid_x = grid_x[None,None,:,:]; grid_y = grid_y[None,None,:,:]\n",
    "        ax = self.anchors[:,0].view(1,A,1,1); ay = self.anchors[:,1].view(1,A,1,1)\n",
    "        bx = (px + grid_x) * STRIDE\n",
    "        by = (py + grid_y) * STRIDE\n",
    "        bw = torch.exp(pw) * ax\n",
    "        bh = torch.exp(ph) * ay\n",
    "        def to_xyxy(cx,cy,w,h):\n",
    "            x1 = cx - w/2; y1 = cy - h/2\n",
    "            x2 = cx + w/2; y2 = cy + h/2\n",
    "            return x1,y1,x2,y2\n",
    "        px1,py1,px2,py2 = to_xyxy(bx,by,bw,bh)\n",
    "        ignore_mask = torch.zeros_like(tobj, dtype=torch.bool)\n",
    "        for b in range(B):\n",
    "            if not obj_mask[b].any():\n",
    "                continue\n",
    "            gtx = (tx[b] + grid_x) * STRIDE\n",
    "            gty = (ty[b] + grid_y) * STRIDE\n",
    "            gtw = torch.exp(tw[b]) * ax\n",
    "            gth = torch.exp(th[b]) * ay\n",
    "            gx1,gy1,gx2,gy2 = to_xyxy(gtx,gty,gtw,gth)\n",
    "            inter_x1 = torch.maximum(px1[b], gx1)\n",
    "            inter_y1 = torch.maximum(py1[b], gy1)\n",
    "            inter_x2 = torch.minimum(px2[b], gx2)\n",
    "            inter_y2 = torch.minimum(py2[b], gy2)\n",
    "            inter_w = torch.clamp(inter_x2 - inter_x1, min=0)\n",
    "            inter_h = torch.clamp(inter_y2 - inter_y1, min=0)\n",
    "            inter = inter_w * inter_h\n",
    "            area_p = (px2[b]-px1[b]) * (py2[b]-py1[b])\n",
    "            area_g = (gx2-gx1) * (gy2-gy1)\n",
    "            iou = inter / (area_p + area_g - inter + 1e-16)\n",
    "            ignore_mask[b] = iou.detach() > self.ignore_iou\n",
    "\n",
    "        # obj/noobj (with logits)\n",
    "        loss_obj   = F.binary_cross_entropy_with_logits(lo[obj_mask], tobj[obj_mask], reduction='sum') \\\n",
    "                     if obj_mask.any() else torch.tensor(0., device=pred.device)\n",
    "        noobj_mask = (~obj_mask) & (~ignore_mask)\n",
    "        loss_noobj = self.lambda_noobj * F.binary_cross_entropy_with_logits(lo[noobj_mask], tobj[noobj_mask], reduction='sum') \\\n",
    "                     if noobj_mask.any() else torch.tensor(0., device=pred.device)\n",
    "\n",
    "        loss = (loss_coord + loss_cls + loss_obj + loss_noobj) / max(1,B)\n",
    "        stats = dict(loss=loss.item(),\n",
    "                     coord=loss_coord.item()/max(1,B),\n",
    "                     cls=loss_cls.item()/max(1,B) if obj_mask.any() else 0.0,\n",
    "                     obj=loss_obj.item()/max(1,B) if obj_mask.any() else 0.0,\n",
    "                     noobj=loss_noobj.item()/max(1,B) if noobj_mask.any() else 0.0)\n",
    "        return loss, stats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e37bdc5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[PROBE] idx=0\n",
      "  name=나무_10_남_00013.jpg | img.shape=torch.Size([3, 416, 416]) | labels_type=<class 'torch.Tensor'>\n",
      "  labels.shape=torch.Size([1, 5])\n",
      "  load time: 0.013s\n",
      "\n",
      "[PROBE] idx=1\n",
      "  name=나무_10_남_00022.jpg | img.shape=torch.Size([3, 416, 416]) | labels_type=<class 'torch.Tensor'>\n",
      "  labels.shape=torch.Size([1, 5])\n",
      "  load time: 0.011s\n",
      "\n",
      "[PROBE] idx=22400\n",
      "  name=여자사람_10_남_00010.jpg | img.shape=torch.Size([3, 416, 416]) | labels_type=<class 'torch.Tensor'>\n",
      "  labels.shape=torch.Size([1, 5])\n",
      "  load time: 0.017s\n"
     ]
    }
   ],
   "source": [
    "# ===== 샘플 3개만 강제 로드해보기 (Dataset만 테스트) =====\n",
    "import time, traceback\n",
    "\n",
    "def probe_sample(ds, idx):\n",
    "    print(f\"\\n[PROBE] idx={idx}\")\n",
    "    t0 = time.time()\n",
    "    sample = ds[idx]           # <-- __getitem__이 여기서 멈추면 그 내부가 원인\n",
    "    t1 = time.time()\n",
    "    try:\n",
    "        name, img, labels = sample\n",
    "        print(f\"  name={name} | img.shape={getattr(img,'shape',None)} | labels_type={type(labels)}\")\n",
    "        if hasattr(labels, 'shape'):\n",
    "            print(f\"  labels.shape={labels.shape}\")\n",
    "        elif isinstance(labels, (list, tuple)):\n",
    "            print(f\"  labels_len={len(labels)} (first={labels[0].shape if len(labels)>0 and hasattr(labels[0],'shape') else type(labels[0])})\")\n",
    "    except Exception as e:\n",
    "        print(\"  unpack 실패:\", e)\n",
    "    print(f\"  load time: {t1 - t0:.3f}s\")\n",
    "\n",
    "# train_ds, val_ds가 이미 있다고 가정\n",
    "for i in [0, 1, len(train_ds)//2]:\n",
    "    try:\n",
    "        probe_sample(train_ds, i)\n",
    "    except Exception as e:\n",
    "        print(\"[ERROR in sample]\", e)\n",
    "        traceback.print_exc()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eb3d92e",
   "metadata": {},
   "source": [
    "디버깅"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fac2eec3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[BATCH TEST] OK: 16 torch.Size([16, 3, 416, 416]) <class 'list'> 16\n"
     ]
    }
   ],
   "source": [
    "# ===== 안전 collate_fn =====\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def yolo_collate(batch):\n",
    "    names, imgs, labels_list = [], [], []\n",
    "    for it in batch:\n",
    "        if it is None:\n",
    "            continue\n",
    "        n, im, lb = it\n",
    "        names.append(n)\n",
    "        imgs.append(im)\n",
    "        labels_list.append(lb)\n",
    "    return names, torch.stack(imgs, dim=0), labels_list\n",
    "\n",
    "DL_PIN = (DEVICE == 'cuda')\n",
    "\n",
    "# ★ 기존 DataLoader를 이걸로 반드시 교체\n",
    "train_dl = DataLoader(\n",
    "    train_ds, batch_size=BATCH_SIZE, shuffle=True,\n",
    "    num_workers=0, pin_memory=DL_PIN,\n",
    "    collate_fn=yolo_collate, persistent_workers=False, timeout=0\n",
    ")\n",
    "val_dl = DataLoader(\n",
    "    val_ds, batch_size=BATCH_SIZE, shuffle=False,\n",
    "    num_workers=0, pin_memory=DL_PIN,\n",
    "    collate_fn=yolo_collate, persistent_workers=False, timeout=0\n",
    ")\n",
    "\n",
    "# 배치 조립이 되는지 즉시 확인\n",
    "b = next(iter(train_dl))\n",
    "print(\"[BATCH TEST] OK:\",\n",
    "      len(b[0]),            # names\n",
    "      b[1].shape,           # imgs\n",
    "      type(b[2]),           # labels_list\n",
    "      len(b[2]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9f0b9949",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[BATCH TEST] OK: 16 torch.Size([16, 3, 416, 416]) <class 'list'> 16\n"
     ]
    }
   ],
   "source": [
    "# ===== 안전 collate_fn =====\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def yolo_collate(batch):\n",
    "    names, imgs, labels_list = [], [], []\n",
    "    for it in batch:\n",
    "        if it is None:\n",
    "            continue\n",
    "        n, im, lb = it\n",
    "        names.append(n)\n",
    "        imgs.append(im)\n",
    "        labels_list.append(lb)\n",
    "    return names, torch.stack(imgs, dim=0), labels_list\n",
    "\n",
    "DL_PIN = (DEVICE == 'cuda')\n",
    "\n",
    "# ★ 기존 DataLoader를 이걸로 반드시 교체\n",
    "train_dl = DataLoader(\n",
    "    train_ds, batch_size=BATCH_SIZE, shuffle=True,\n",
    "    num_workers=0, pin_memory=DL_PIN,\n",
    "    collate_fn=yolo_collate, persistent_workers=False, timeout=0\n",
    ")\n",
    "val_dl = DataLoader(\n",
    "    val_ds, batch_size=BATCH_SIZE, shuffle=False,\n",
    "    num_workers=0, pin_memory=DL_PIN,\n",
    "    collate_fn=yolo_collate, persistent_workers=False, timeout=0\n",
    ")\n",
    "\n",
    "# 배치 조립이 되는지 즉시 확인\n",
    "b = next(iter(train_dl))\n",
    "print(\"[BATCH TEST] OK:\",\n",
    "      len(b[0]),            # names\n",
    "      b[1].shape,           # imgs\n",
    "      type(b[2]),           # labels_list\n",
    "      len(b[2]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6801832c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== 셀 5-CPU: 타깃 생성 (CPU) =====\n",
    "import math\n",
    "import torch\n",
    "\n",
    "# ANCHORS를 CPU 텐서로 한번만 복제해두면 빠릅니다.\n",
    "ANCHORS_CPU = ANCHORS.detach().cpu()\n",
    "\n",
    "def build_targets_cpu(labels_list, anchors_cpu, S=13, num_classes=4):\n",
    "    \"\"\"\n",
    "    labels_list: List[Tensor(N,5)]  # (cls, xc, yc, w, h) 모두 0~1 정규화\n",
    "    anchors_cpu: Tensor(A,2)        # (aw, ah) in pixels\n",
    "    반환: Tensor(B, A, S, S, 5+num_classes) on CPU\n",
    "    \"\"\"\n",
    "    B = len(labels_list)\n",
    "    A = anchors_cpu.size(0)\n",
    "    target = torch.zeros(B, A, S, S, 5+num_classes)  # CPU 상에서 생성\n",
    "\n",
    "    for b_idx, labels in enumerate(labels_list):\n",
    "        if labels is None or len(labels) == 0:\n",
    "            continue\n",
    "\n",
    "        # CPU로 강제 + 스케일 픽셀 단위로 변환\n",
    "        lab = labels.detach().cpu().clone()\n",
    "        lab[:, 1:] *= torch.tensor([IMG_SIZE, IMG_SIZE, IMG_SIZE, IMG_SIZE], dtype=lab.dtype)\n",
    "\n",
    "        aw = anchors_cpu[:, 0]  # (A,)\n",
    "        ah = anchors_cpu[:, 1]  # (A,)\n",
    "\n",
    "        for row in lab:\n",
    "            cls = int(row[0].item() if row[0].numel() == 1 else int(row[0]))\n",
    "            xc, yc, bw, bh = map(float, row[1:].tolist())\n",
    "\n",
    "            gi = int(xc // STRIDE); gj = int(yc // STRIDE)\n",
    "            gi = 0 if gi < 0 else (S-1 if gi >= S else gi)\n",
    "            gj = 0 if gj < 0 else (S-1 if gj >= S else gj)\n",
    "\n",
    "            # IoU(너비/높이만)로 앵커 선택\n",
    "            # (cx,cy는 셀 할당에만 사용, 실제 IoU는 w,h로 근사)\n",
    "            bw_t = torch.tensor(bw)\n",
    "            bh_t = torch.tensor(bh)\n",
    "            inter = torch.minimum(bw_t, aw) * torch.minimum(bh_t, ah)\n",
    "            area1 = bw_t * bh_t\n",
    "            area2 = aw * ah\n",
    "            iou = inter / (area1 + area2 - inter + 1e-16)\n",
    "            a = int(torch.argmax(iou))\n",
    "\n",
    "            # 셀 좌표계 오프셋/로그 스페이스\n",
    "            tx = (xc / STRIDE) - gi\n",
    "            ty = (yc / STRIDE) - gj\n",
    "            tw = math.log(bw / float(anchors_cpu[a, 0]) + 1e-16)\n",
    "            th = math.log(bh / float(anchors_cpu[a, 1]) + 1e-16)\n",
    "\n",
    "            tgt = target[b_idx, a, gj, gi]\n",
    "            tgt[0] = tx; tgt[1] = ty; tgt[2] = tw; tgt[3] = th\n",
    "            tgt[4] = 1.0\n",
    "            if 0 <= cls < num_classes:\n",
    "                tgt[5 + cls] = 1.0\n",
    "\n",
    "    return target\n",
    "\n",
    "# (선택) 실수 방지를 위해 기존 build_targets 호출이 있어도 CPU 버전으로 강제 바인딩\n",
    "build_targets = lambda labels_list, anchors, S, num_classes: \\\n",
    "    build_targets_cpu(labels_list, ANCHORS_CPU, S=S, num_classes=num_classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "15206da6",
   "metadata": {},
   "outputs": [],
   "source": [
    "names, imgs, labels_list = next(iter(train_dl))\n",
    "imgs = imgs.to(DEVICE, non_blocking=True)\n",
    "target = build_targets_cpu(labels_list, ANCHORS_CPU, S=GRID_SIZE, num_classes=NUM_CLASSES).to(DEVICE, non_blocking=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "56277d2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from torch import amp\n",
    "\n",
    "scaler = amp.GradScaler(enabled=(DEVICE=='cuda'))\n",
    "\n",
    "def train_one_epoch(model, loader, optimizer):\n",
    "    model.train()\n",
    "    meters = {\"loss\":0,\"coord\":0,\"cls\":0,\"obj\":0,\"noobj\":0,\"n\":0}\n",
    "    t0 = time.time()\n",
    "    for ib, (names, imgs, labels_list) in enumerate(tqdm(loader, desc=\"Train\", leave=False)):\n",
    "        t_a = time.time()\n",
    "        imgs = imgs.to(DEVICE, non_blocking=True)\n",
    "        target = build_targets_cpu(labels_list, ANCHORS.cpu(), S=GRID_SIZE, num_classes=NUM_CLASSES)\\\n",
    "                    .to(DEVICE, non_blocking=True)\n",
    "        t_b = time.time()\n",
    "\n",
    "        with amp.autocast(device_type='cuda', dtype=torch.float16, enabled=(DEVICE=='cuda')):\n",
    "            pred = model(imgs)\n",
    "            # 느리면 여기서 동기화로 정확히 찍힘\n",
    "            if DEVICE=='cuda': torch.cuda.synchronize()\n",
    "            t_c = time.time()\n",
    "\n",
    "            loss, stats = criterion(pred, target)\n",
    "            if DEVICE=='cuda': torch.cuda.synchronize()\n",
    "            t_d = time.time()\n",
    "\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        if DEVICE=='cuda': torch.cuda.synchronize()\n",
    "        t_e = time.time()\n",
    "\n",
    "        # 1배치 시간 분해 출력 (초기 3~5배치만)\n",
    "        if ib < 5:\n",
    "            print(f\"[TIMING b{ib}] H2D+build_targets={(t_b-t_a):.3f}s | forward={(t_c-t_b):.3f}s | loss={(t_d-t_c):.3f}s | step={(t_e-t_d):.3f}s\")\n",
    "\n",
    "        for k in [\"loss\",\"coord\",\"cls\",\"obj\",\"noobj\"]:\n",
    "            meters[k] += stats[k]\n",
    "        meters[\"n\"] += 1\n",
    "\n",
    "    for k in [\"loss\",\"coord\",\"cls\",\"obj\",\"noobj\"]:\n",
    "        meters[k] /= max(1, meters[\"n\"])\n",
    "    print(f\"[EPOCH] total {(time.time()-t0):.1f}s\")\n",
    "    return meters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bf8aae8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 75\u001b[39m\n\u001b[32m     72\u001b[39m SAVE_PATH = \u001b[33m\"\u001b[39m\u001b[33myolov2_4cls.pt\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     74\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m ep \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[32m1\u001b[39m, EPOCHS+\u001b[32m1\u001b[39m):\n\u001b[32m---> \u001b[39m\u001b[32m75\u001b[39m     tr = \u001b[43mtrain_one_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     76\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m ep % VAL_EVERY == \u001b[32m0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m ep == EPOCHS:\n\u001b[32m     77\u001b[39m         va = validate(model, val_dl)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 32\u001b[39m, in \u001b[36mtrain_one_epoch\u001b[39m\u001b[34m(model, loader, optimizer)\u001b[39m\n\u001b[32m     30\u001b[39m model.train()\n\u001b[32m     31\u001b[39m meters = {\u001b[33m\"\u001b[39m\u001b[33mloss\u001b[39m\u001b[33m\"\u001b[39m:\u001b[32m0.0\u001b[39m,\u001b[33m\"\u001b[39m\u001b[33mcoord\u001b[39m\u001b[33m\"\u001b[39m:\u001b[32m0.0\u001b[39m,\u001b[33m\"\u001b[39m\u001b[33mcls\u001b[39m\u001b[33m\"\u001b[39m:\u001b[32m0.0\u001b[39m,\u001b[33m\"\u001b[39m\u001b[33mobj\u001b[39m\u001b[33m\"\u001b[39m:\u001b[32m0.0\u001b[39m,\u001b[33m\"\u001b[39m\u001b[33mnoobj\u001b[39m\u001b[33m\"\u001b[39m:\u001b[32m0.0\u001b[39m,\u001b[33m\"\u001b[39m\u001b[33mn\u001b[39m\u001b[33m\"\u001b[39m:\u001b[32m0\u001b[39m}\n\u001b[32m---> \u001b[39m\u001b[32m32\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mnames\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimgs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels_list\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtqdm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdesc\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mTrain\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mleave\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m     33\u001b[39m \u001b[43m    \u001b[49m\u001b[43mimgs\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mimgs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mDEVICE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m     34\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mmake_target\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabels_list\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\hubo0\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tqdm\\std.py:1181\u001b[39m, in \u001b[36mtqdm.__iter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1178\u001b[39m time = \u001b[38;5;28mself\u001b[39m._time\n\u001b[32m   1180\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1181\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   1182\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\n\u001b[32m   1183\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Update and possibly print the progressbar.\u001b[39;49;00m\n\u001b[32m   1184\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;49;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\hubo0\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:701\u001b[39m, in \u001b[36m_BaseDataLoaderIter.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    698\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    699\u001b[39m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[32m    700\u001b[39m     \u001b[38;5;28mself\u001b[39m._reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m701\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    702\u001b[39m \u001b[38;5;28mself\u001b[39m._num_yielded += \u001b[32m1\u001b[39m\n\u001b[32m    703\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    704\u001b[39m     \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable\n\u001b[32m    705\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    706\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._num_yielded > \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called\n\u001b[32m    707\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\hubo0\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:757\u001b[39m, in \u001b[36m_SingleProcessDataLoaderIter._next_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    755\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    756\u001b[39m     index = \u001b[38;5;28mself\u001b[39m._next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m757\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m    758\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._pin_memory:\n\u001b[32m    759\u001b[39m         data = _utils.pin_memory.pin_memory(data, \u001b[38;5;28mself\u001b[39m._pin_memory_device)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\hubo0\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001b[39m, in \u001b[36m_MapDatasetFetcher.fetch\u001b[39m\u001b[34m(self, possibly_batched_index)\u001b[39m\n\u001b[32m     50\u001b[39m         data = \u001b[38;5;28mself\u001b[39m.dataset.__getitems__(possibly_batched_index)\n\u001b[32m     51\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m52\u001b[39m         data = \u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpossibly_batched_index\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m     53\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     54\u001b[39m     data = \u001b[38;5;28mself\u001b[39m.dataset[possibly_batched_index]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\hubo0\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001b[39m, in \u001b[36m<listcomp>\u001b[39m\u001b[34m(.0)\u001b[39m\n\u001b[32m     50\u001b[39m         data = \u001b[38;5;28mself\u001b[39m.dataset.__getitems__(possibly_batched_index)\n\u001b[32m     51\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m52\u001b[39m         data = [\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[32m     53\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     54\u001b[39m     data = \u001b[38;5;28mself\u001b[39m.dataset[possibly_batched_index]\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 70\u001b[39m, in \u001b[36mYOLODataset.__getitem__\u001b[39m\u001b[34m(self, idx)\u001b[39m\n\u001b[32m     67\u001b[39m img, scale, padw, padh = letterbox(img, \u001b[38;5;28mself\u001b[39m.img_size)\n\u001b[32m     68\u001b[39m img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB).astype(np.float32) / \u001b[32m255.0\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m70\u001b[39m labels = \u001b[43mload_labels\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlbl_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     71\u001b[39m labels = np.array(labels, dtype=np.float32) \u001b[38;5;28;01mif\u001b[39;00m labels \u001b[38;5;28;01melse\u001b[39;00m np.zeros((\u001b[32m0\u001b[39m,\u001b[32m5\u001b[39m), dtype=np.float32)\n\u001b[32m     73\u001b[39m img = torch.from_numpy(img).permute(\u001b[32m2\u001b[39m,\u001b[32m0\u001b[39m,\u001b[32m1\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 20\u001b[39m, in \u001b[36mload_labels\u001b[39m\u001b[34m(txt_path)\u001b[39m\n\u001b[32m     18\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m txt_path.exists():\n\u001b[32m     19\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m boxes\n\u001b[32m---> \u001b[39m\u001b[32m20\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtxt_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mr\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mutf-8\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[32m     21\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m f:\n\u001b[32m     22\u001b[39m         ss = line.strip().split()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\hubo0\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:343\u001b[39m, in \u001b[36m_modified_open\u001b[39m\u001b[34m(file, *args, **kwargs)\u001b[39m\n\u001b[32m    336\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[32m0\u001b[39m, \u001b[32m1\u001b[39m, \u001b[32m2\u001b[39m}:\n\u001b[32m    337\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    338\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mIPython won\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m by default \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    339\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    340\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33myou can use builtins\u001b[39m\u001b[33m'\u001b[39m\u001b[33m open.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    341\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m343\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen codecs>:309\u001b[39m, in \u001b[36m__init__\u001b[39m\u001b[34m(self, errors)\u001b[39m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# ===== 셀 6: 학습 루프 & 저장 (AMP 최신화, dtype/device 정렬) =====\n",
    "import time\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from torch import amp\n",
    "\n",
    "# ----- 손실 함수 -----\n",
    "criterion = YOLOv2Loss(ANCHORS.to(DEVICE), num_classes=NUM_CLASSES)\n",
    "\n",
    "# ----- 모델/옵티마/스케줄러/AMP -----\n",
    "model = model.to(DEVICE).float()\n",
    "\n",
    "def build_optimizer(model, lr=1e-3, wd=5e-4):\n",
    "    return torch.optim.Adam(model.parameters(), lr=lr, weight_decay=wd)\n",
    "\n",
    "optimizer = build_optimizer(model, LR, WEIGHT_DECAY)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=EPOCHS)\n",
    "\n",
    "# AMP 스케일러 (CUDA에서만 활성)\n",
    "scaler = amp.GradScaler(enabled=(DEVICE == 'cuda'))\n",
    "\n",
    "# ----- 타깃 생성 헬퍼 (CPU에서 만들고 GPU로 한 번만 복사) -----\n",
    "def make_target(labels_list):\n",
    "    anchors_cpu = ANCHORS_CPU if 'ANCHORS_CPU' in globals() else ANCHORS.detach().cpu()\n",
    "    target_cpu = build_targets_cpu(labels_list, anchors_cpu, S=GRID_SIZE, num_classes=NUM_CLASSES)\n",
    "    return target_cpu.to(DEVICE, dtype=torch.float32, non_blocking=True)\n",
    "\n",
    "# ----- 한 epoch 학습 -----\n",
    "def train_one_epoch(model, loader, optimizer):\n",
    "    model.train()\n",
    "    meters = {\"loss\":0.0,\"coord\":0.0,\"cls\":0.0,\"obj\":0.0,\"noobj\":0.0,\"n\":0}\n",
    "    for names, imgs, labels_list in tqdm(loader, desc=\"Train\", leave=False):\n",
    "        imgs = imgs.to(DEVICE, non_blocking=True)\n",
    "        target = make_target(labels_list)\n",
    "\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        with amp.autocast(device_type='cuda', dtype=torch.float16, enabled=(DEVICE=='cuda')):\n",
    "            pred = model(imgs)\n",
    "            loss, stats = criterion(pred, target)\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        for k in [\"loss\",\"coord\",\"cls\",\"obj\",\"noobj\"]:\n",
    "            meters[k] += float(stats[k])\n",
    "        meters[\"n\"] += 1\n",
    "    for k in [\"loss\",\"coord\",\"cls\",\"obj\",\"noobj\"]:\n",
    "        meters[k] = meters[k] / max(1, meters[\"n\"])\n",
    "    return meters\n",
    "\n",
    "# ----- 검증 -----\n",
    "@torch.no_grad()\n",
    "def validate(model, loader):\n",
    "    model.eval()\n",
    "    meters = {\"loss\":0.0,\"coord\":0.0,\"cls\":0.0,\"obj\":0.0,\"noobj\":0.0,\"n\":0}\n",
    "    for names, imgs, labels_list in tqdm(loader, desc=\"Val\", leave=False):\n",
    "        imgs = imgs.to(DEVICE, non_blocking=True)\n",
    "        target = make_target(labels_list)\n",
    "        with amp.autocast(device_type='cuda', dtype=torch.float16, enabled=(DEVICE=='cuda')):\n",
    "            pred = model(imgs)\n",
    "            loss, stats = criterion(pred, target)\n",
    "        for k in [\"loss\",\"coord\",\"cls\",\"obj\",\"noobj\"]:\n",
    "            meters[k] += float(stats[k])\n",
    "        meters[\"n\"] += 1\n",
    "    for k in [\"loss\",\"coord\",\"cls\",\"obj\",\"noobj\"]:\n",
    "        meters[k] = meters[k] / max(1, meters[\"n\"])\n",
    "    return meters\n",
    "\n",
    "# ----- 학습 루프 -----\n",
    "BEST = float('inf')\n",
    "SAVE_PATH = \"yolov2_4cls.pt\"\n",
    "\n",
    "for ep in range(1, EPOCHS+1):\n",
    "    tr = train_one_epoch(model, train_dl, optimizer)\n",
    "    if ep % VAL_EVERY == 0 or ep == EPOCHS:\n",
    "        va = validate(model, val_dl)\n",
    "        if va[\"loss\"] < BEST:\n",
    "            BEST = va[\"loss\"]\n",
    "            torch.save({\n",
    "                \"model\": model.state_dict(),\n",
    "                \"anchors\": ANCHORS.detach().cpu().numpy(),\n",
    "                \"classes\": IDX2CLASS,\n",
    "                \"img_size\": IMG_SIZE\n",
    "            }, SAVE_PATH)\n",
    "            print(f\"[E{ep:02d}] Train {tr['loss']:.4f} | Val {va['loss']:.4f}  ↳ ✅ Saved {SAVE_PATH}\")\n",
    "        else:\n",
    "            print(f\"[E{ep:02d}] Train {tr['loss']:.4f} | Val {va['loss']:.4f}\")\n",
    "    else:\n",
    "        print(f\"[E{ep:02d}] Train {tr['loss']:.4f} (coord {tr['coord']:.3f} | cls {tr['cls']:.3f} | obj {tr['obj']:.3f} | noobj {tr['noobj']:.3f})\")\n",
    "    scheduler.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "abfd9359",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 현재 작업 폴더: C:\\team_project\\second_project\n",
      "[OK] 발견 → C:\\team_project\\second_project\\266.AI 기반 아동 미술심리 진단을 위한 그림 데이터 구축\\01-1.정식개방데이터\\Validation\\01.원천데이터\\VS_집\\집_13_여_09223.jpg\n",
      "[OK] 발견 → C:\\team_project\\second_project\\266.AI 기반 아동 미술심리 진단을 위한 그림 데이터 구축\\01-1.정식개방데이터\\Validation\\01.원천데이터\\VS_여자사람\\여자사람_11_여_05400.jpg\n",
      "[OK] 발견 → C:\\team_project\\second_project\\266.AI 기반 아동 미술심리 진단을 위한 그림 데이터 구축\\01-1.정식개방데이터\\Validation\\01.원천데이터\\VS_남자사람\\남자사람_11_남_07042.jpg\n",
      "[OK] 발견 → C:\\team_project\\second_project\\266.AI 기반 아동 미술심리 진단을 위한 그림 데이터 구축\\01-1.정식개방데이터\\Validation\\01.원천데이터\\VS_나무\\나무_8_여_01105.jpg\n",
      "[SAVE] 집_13_여_09223: boxes=2 | crops_saved=2 | vis='집_13_여_09223_vis.jpg'\n",
      "[SAVE] 여자사람_11_여_05400: boxes=2 | crops_saved=2 | vis='여자사람_11_여_05400_vis.jpg'\n",
      "[SAVE] 남자사람_11_남_07042: boxes=1 | crops_saved=1 | vis='남자사람_11_남_07042_vis.jpg'\n",
      "[SAVE] 나무_8_여_01105: boxes=2 | crops_saved=2 | vis='나무_8_여_01105_vis.jpg'\n"
     ]
    }
   ],
   "source": [
    "# ===== 셀 7 (강화판): 경로 자동탐색 + 안전 읽기/쓰기 + 디버그 로그 =====\n",
    "import os, math\n",
    "from pathlib import Path\n",
    "import torch, torchvision\n",
    "import cv2, numpy as np\n",
    "from torch import amp\n",
    "\n",
    "CKPT_PATH = \"yolov2_4cls.pt\"\n",
    "OUT_DIR   = Path(\"results_detect\")\n",
    "VIS_DIR   = OUT_DIR / \"vis\"\n",
    "CROP_DIR  = OUT_DIR / \"crops\"\n",
    "for d in [VIS_DIR, CROP_DIR]: d.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# 1) 테스트 파일 이름 (이 “파일명”을 프로젝트 내에서 자동으로 찾습니다)\n",
    "WANTED_BASENAMES = [\n",
    "    \"집_13_여_09223.jpg\",\n",
    "    \"여자사람_11_여_05400.jpg\",\n",
    "    \"남자사람_11_남_07042.jpg\",\n",
    "    \"나무_8_여_01105.jpg\",\n",
    "]\n",
    "\n",
    "# 2) 먼저 뒤져볼 시작 폴더 후보 (필요시 본인 데이터셋 최상위 경로를 하나 추가)\n",
    "START_DIRS = [\n",
    "    \".\", \"..\", \"../..\",\n",
    "    # 예시: 실제 데이터 폴더를 알고 있다면 아래 줄 주석 해제 후 경로 수정\n",
    "    # r\"C:\\team_project\\second_project\\266.AI 기반 아동 미술심리 진단을 위한 그림 데이터 구축\\01-1.정식개방데이터\",\n",
    "]\n",
    "\n",
    "cv2.setNumThreads(0)\n",
    "\n",
    "def safe_imread(path_str):\n",
    "    p = os.path.normpath(path_str)\n",
    "    try:\n",
    "        data = np.fromfile(p, dtype=np.uint8)\n",
    "        img  = cv2.imdecode(data, cv2.IMREAD_COLOR)\n",
    "        if img is None:\n",
    "            raise ValueError(\"imdecode returned None\")\n",
    "        return img\n",
    "    except Exception:\n",
    "        img = cv2.imread(p, cv2.IMREAD_COLOR)\n",
    "        if img is None:\n",
    "            raise FileNotFoundError(f\"이미지 읽기 실패: {p}\")\n",
    "        return img\n",
    "\n",
    "def safe_imwrite(path_str, img_bgr, ext=\".jpg\", params=None):\n",
    "    # 한글/공백 경로에서도 안전하게 저장\n",
    "    p = os.path.normpath(path_str)\n",
    "    ok, buf = cv2.imencode(ext, img_bgr, params or [])\n",
    "    if not ok:\n",
    "        raise IOError(\"imencode failed\")\n",
    "    buf.tofile(p)\n",
    "\n",
    "def find_file_anywhere(basename, start_dirs):\n",
    "    # 여러 시작 폴더에서 파일명을 재귀 탐색 (대소문자 무시)\n",
    "    lower = basename.lower()\n",
    "    for sd in start_dirs:\n",
    "        sd = Path(sd).resolve()\n",
    "        if not sd.exists():\n",
    "            continue\n",
    "        # 빠른 경로: exact name\n",
    "        exact = list(sd.glob(f\"**/{basename}\"))\n",
    "        if exact:\n",
    "            return str(exact[0])\n",
    "        # 느리지만 확실: 대소문자 무시\n",
    "        for p in sd.rglob(\"*\"):\n",
    "            if p.is_file() and p.name.lower() == lower:\n",
    "                return str(p)\n",
    "    return None\n",
    "\n",
    "# ----- 디코드 -----\n",
    "def yolo_decode(pred, anchors, S=13, stride=32, num_classes=4, conf_thr=0.9, nms_iou=0.18):\n",
    "    device = pred.device\n",
    "    B, _, _, _ = pred.shape\n",
    "    A = anchors.size(0); C = num_classes\n",
    "    p = pred.reshape(B, A, 5+C, S, S).permute(0,1,3,4,2).contiguous()\n",
    "\n",
    "    tx = torch.sigmoid(p[...,0]); ty = torch.sigmoid(p[...,1])\n",
    "    tw = p[...,2];               th = p[...,3]\n",
    "    tobj = torch.sigmoid(p[...,4])\n",
    "    tcls = torch.sigmoid(p[...,5:])\n",
    "\n",
    "    gy, gx = torch.meshgrid(torch.arange(S, device=device),\n",
    "                            torch.arange(S, device=device), indexing='ij')\n",
    "    gx = gx[None,None]; gy = gy[None,None]\n",
    "    aw = anchors[:,0].view(1,A,1,1).to(device)\n",
    "    ah = anchors[:,1].view(1,A,1,1).to(device)\n",
    "\n",
    "    cx = (tx + gx) * stride\n",
    "    cy = (ty + gy) * stride\n",
    "    w  = torch.exp(tw) * aw\n",
    "    h  = torch.exp(th) * ah\n",
    "\n",
    "    x1 = cx - w/2; y1 = cy - h/2\n",
    "    x2 = cx + w/2; y2 = cy + h/2\n",
    "\n",
    "    outs = []\n",
    "    for b in range(B):\n",
    "        scores, labels = tcls[b].max(dim=-1)\n",
    "        conf = tobj[b] * scores\n",
    "        m = conf > conf_thr\n",
    "        if m.sum() == 0:\n",
    "            outs.append(torch.zeros(0,6, device=device)); continue\n",
    "        boxes = torch.stack([x1[b][m], y1[b][m], x2[b][m], y2[b][m]], dim=1)\n",
    "        sc    = conf[m]\n",
    "        lb    = labels[m].float()\n",
    "        keep  = torchvision.ops.nms(boxes, sc, nms_iou)\n",
    "        outs.append(torch.cat([boxes[keep], sc[keep,None], lb[keep,None]], dim=1))\n",
    "    return outs\n",
    "\n",
    "def preprocess_img(img_bgr, img_size=IMG_SIZE):\n",
    "    h, w = img_bgr.shape[:2]\n",
    "    if (h, w) != (img_size, img_size):\n",
    "        img_bgr = cv2.resize(img_bgr, (img_size, img_size), interpolation=cv2.INTER_LINEAR)\n",
    "    img_rgb = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2RGB)\n",
    "    return torch.from_numpy(img_rgb).permute(2,0,1).float() / 255.0\n",
    "\n",
    "def draw_and_save(img_bgr, det, vis_path, crop_dir, base_name):\n",
    "    im  = img_bgr.copy()\n",
    "    H,W = im.shape[:2]\n",
    "    COLORS = [(255,0,0),(0,255,0),(0,0,255),(255,128,0),(0,255,255),(255,0,255)]\n",
    "    n_saved = 0\n",
    "    for i, (x1,y1,x2,y2,score,cls) in enumerate(det.cpu().numpy()):\n",
    "        x1 = int(max(0, min(W-1, x1))); y1 = int(max(0, min(H-1, y1)))\n",
    "        x2 = int(max(0, min(W-1, x2))); y2 = int(max(0, min(H-1, y2)))\n",
    "        c  = int(cls)\n",
    "        name = IDX2CLASS.get(c, str(c))\n",
    "        color = COLORS[c % len(COLORS)]\n",
    "        cv2.rectangle(im, (x1,y1), (x2,y2), color, 2)\n",
    "        cv2.putText(im, f\"{name}:{score:.2f}\", (x1, max(0,y1-7)), cv2.FONT_HERSHEY_SIMPLEX, 0.6, color, 2, cv2.LINE_AA)\n",
    "        crop = img_bgr[y1:y2, x1:x2]\n",
    "        if crop.size > 0:\n",
    "            crop_name = f\"{base_name}_det{i}_{name}.jpg\"\n",
    "            safe_imwrite(str(crop_dir / crop_name), crop)\n",
    "            n_saved += 1\n",
    "    # 박스가 0개라도 시각화 이미지는 저장(원본 그대로)\n",
    "    safe_imwrite(str(vis_path), im)\n",
    "    return n_saved\n",
    "\n",
    "# ----- 체크포인트 로드 -----\n",
    "ckpt = torch.load(CKPT_PATH, map_location=DEVICE, weights_only=False)\n",
    "model.load_state_dict(ckpt[\"model\"])\n",
    "model.eval()\n",
    "\n",
    "# ----- 파일 탐색 -----\n",
    "resolved = []\n",
    "print(\"[INFO] 현재 작업 폴더:\", Path(\".\").resolve())\n",
    "for bn in WANTED_BASENAMES:\n",
    "    fp = find_file_anywhere(bn, START_DIRS)\n",
    "    if fp is None:\n",
    "        print(f\"[WARN] 찾지 못함 → {bn}\")\n",
    "    else:\n",
    "        print(f\"[OK] 발견 → {fp}\")\n",
    "        resolved.append(fp)\n",
    "\n",
    "if not resolved:\n",
    "    print(\"[ERROR] 테스트 이미지를 아무 곳에서도 찾지 못했습니다. START_DIRS에 실제 데이터셋 최상위 경로를 추가해 주세요.\")\n",
    "else:\n",
    "    with torch.no_grad():\n",
    "        for p in resolved:\n",
    "            try:\n",
    "                img0 = safe_imread(p)\n",
    "            except Exception as e:\n",
    "                print(f\"[WARN] 이미지 읽기 실패: {p} | {e}\")\n",
    "                continue\n",
    "\n",
    "            inp = preprocess_img(img0, IMG_SIZE).unsqueeze(0).to(DEVICE, non_blocking=True)\n",
    "            with amp.autocast(device_type='cuda', dtype=torch.float16, enabled=(DEVICE=='cuda')):\n",
    "                pred = model(inp)\n",
    "\n",
    "            dets = yolo_decode(pred, ANCHORS.to(DEVICE), S=GRID_SIZE, stride=STRIDE, num_classes=NUM_CLASSES)\n",
    "            det  = dets[0]\n",
    "\n",
    "            H0, W0 = img0.shape[:2]\n",
    "            sx, sy = W0 / IMG_SIZE, H0 / IMG_SIZE\n",
    "            det_scaled = det.clone()\n",
    "            det_scaled[:, [0, 2]] *= sx\n",
    "            det_scaled[:, [1, 3]] *= sy\n",
    "            det = det_scaled\n",
    "\n",
    "            base = Path(p).stem\n",
    "            vis_path = VIS_DIR / f\"{base}_vis.jpg\"\n",
    "            n_crops = draw_and_save(img0, det, vis_path, CROP_DIR, base)\n",
    "            print(f\"[SAVE] {base}: boxes={len(det)} | crops_saved={n_crops} | vis='{vis_path.name}'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12873f9f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
