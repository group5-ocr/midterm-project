{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3835f402",
   "metadata": {},
   "source": [
    "## 전체 파이프라인\n",
    "데이터 준비 -> YOLOv2 탐지기 학습(4클래스) -> 탐지 결과로 이미지 크롭 -> 크롭 이미지를 ResNet 분류기로 재분류(보정) -> .pt 모델 저장 + 테스트 2장 결과 저장 -> 추가 이미지 테스트"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6639131",
   "metadata": {},
   "source": [
    "# 데이터 준비"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89a32783",
   "metadata": {},
   "outputs": [],
   "source": [
    "266.AI 기반 아동 미술심리 진단을 위한 그림 데이터 구축/01-1.정식개방데이터\n",
    "    Training\n",
    "        01.원천데이터\n",
    "            TS_나무\n",
    "            TS_남자사람\n",
    "            TS_여자사람\n",
    "            TS_집\n",
    "        02.라벨링데이터\n",
    "            TL_나무\n",
    "            TL_남자사람\n",
    "            TL_여자사람\n",
    "            TL_집\n",
    "    Validation\n",
    "        01.원천데이터\n",
    "            VS_나무\n",
    "            VS_남자사람\n",
    "            VS_여자사람\n",
    "            VS_집\n",
    "        02.라벨링데이터\n",
    "            VL_나무\n",
    "            VL_남자사람\n",
    "            VL_여자사람\n",
    "            VL_집"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0849ae56",
   "metadata": {},
   "source": [
    "# 1. YOLOv2 탐지기 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bda010b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch 버전: 2.5.1+cu121\n",
      "CUDA 사용 가능 여부: True\n",
      "사용 중인 GPU 개수: 1\n",
      "현재 선택된 GPU: 0\n",
      "GPU 이름: NVIDIA GeForce RTX 4060 Laptop GPU\n",
      "GPU 행렬 곱 소요 시간: 0.1362초\n"
     ]
    }
   ],
   "source": [
    "# GPU 사용 가능 여부 점검\n",
    "import torch\n",
    "\n",
    "print(\"PyTorch 버전:\", torch.__version__)\n",
    "print(\"CUDA 사용 가능 여부:\", torch.cuda.is_available())\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(\"사용 중인 GPU 개수:\", torch.cuda.device_count())\n",
    "    print(\"현재 선택된 GPU:\", torch.cuda.current_device())\n",
    "    print(\"GPU 이름:\", torch.cuda.get_device_name(torch.cuda.current_device()))\n",
    "\n",
    "    # 간단한 연산으로 GPU 동작 테스트\n",
    "    x = torch.rand((5000, 5000), device='cuda')\n",
    "    y = torch.rand((5000, 5000), device='cuda')\n",
    "    torch.cuda.synchronize()\n",
    "    import time\n",
    "    start = time.time()\n",
    "    z = torch.matmul(x, y)\n",
    "    torch.cuda.synchronize()\n",
    "    print(\"GPU 행렬 곱 소요 시간: {:.4f}초\".format(time.time() - start))\n",
    "else:\n",
    "    print(\"⚠ GPU(CUDA)를 사용할 수 없습니다. CPU를 사용 중입니다.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7acd0f9",
   "metadata": {},
   "source": [
    "## 1-1. YOLOv2 학습을 위한 라벨 변환\n",
    "YOLOv2 포맷(txt)는 이미지 크기로 정규화한 객체의 중심 형식이다.\n",
    "라벨링 데이터를 읽어, 원천 데이터의 이미지를 찾아 YOLO txt를 만든다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7569a75b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training JSON→YOLO: 100%|██████████| 44800/44800 [00:55<00:00, 802.55file/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Training] 변환: 44800개 | 스킵(라벨없음): 0 | 스킵(이미지없음): 0 | 스킵(해상도확인실패): 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation JSON→YOLO: 100%|██████████| 5600/5600 [00:06<00:00, 827.99file/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Validation] 변환: 5600개 | 스킵(라벨없음): 0 | 스킵(이미지없음): 0 | 스킵(해상도확인실패): 0\n",
      "\n",
      "== 전체 요약 ==\n",
      "Training: 44800개 (no_label 0, no_img 0, no_wh 0)\n",
      "Validation: 5600개 (no_label 0, no_img 0, no_wh 0)\n",
      "✅ 완료: yolo_training/, yolo_validation/ 에 images/ + labels/ 생성\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# JSON → YOLO txt 변환 (집/나무/남자/여자 4클래스), 이미지 링크/복사까지\n",
    "from pathlib import Path\n",
    "import json, os, shutil\n",
    "from tqdm import tqdm\n",
    "\n",
    "# === 경로 루트 ===\n",
    "ROOT = Path(\"266.AI 기반 아동 미술심리 진단을 위한 그림 데이터 구축/01-1.정식개방데이터\")\n",
    "\n",
    "# === 고정 클래스 인덱스(모델/라벨 일치용) ===\n",
    "CLASSES = {\"tree\":0, \"man\":1, \"woman\":2, \"house\":3}\n",
    "\n",
    "# === JSON 내부 라벨명 → 우리가 쓸 대상 라벨 키 ===\n",
    "TARGET_KEYS = {\n",
    "    \"house\":  (\"집전체\",   \"house\"),\n",
    "    \"tree\":   (\"나무전체\", \"tree\"),\n",
    "    \"man\":    (\"사람전체\", \"man\"),\n",
    "    \"woman\":  (\"사람전체\", \"woman\"),\n",
    "}\n",
    "\n",
    "# === 스플릿별 실제 폴더 위치 매핑 ===\n",
    "SPLITS = {\n",
    "    \"Training\": {\n",
    "        \"origin\": ROOT / \"Training\" / \"01.원천데이터\",         # TS_나무, TS_남자사람, TS_여자사람, TS_집\n",
    "        \"label\":  ROOT / \"Training\" / \"02.라벨링데이터\",        # TL_나무, TL_남자사람, TL_여자사람, TL_집\n",
    "        \"out\":    Path(\"yolo_training\"),\n",
    "        \"origin_sub_prefix\": \"TS_\",   # 원천데이터 하위 폴더 접두사\n",
    "        \"label_sub_prefix\":  \"TL_\",   # 라벨링데이터 하위 폴더 접두사\n",
    "    },\n",
    "    \"Validation\": {\n",
    "        \"origin\": ROOT / \"Validation\" / \"01.원천데이터\",       # VS_나무, VS_남자사람, VS_여자사람, VS_집\n",
    "        \"label\":  ROOT / \"Validation\" / \"02.라벨링데이터\",      # VL_나무, VL_남자사람, VL_여자사람, VL_집\n",
    "        \"out\":    Path(\"yolo_validation\"),\n",
    "        \"origin_sub_prefix\": \"VS_\",\n",
    "        \"label_sub_prefix\":  \"VL_\",\n",
    "    }\n",
    "}\n",
    "\n",
    "# === 한국어 클래스명 매핑(폴더명용) ===\n",
    "KO_CLASS = {\"house\":\"집\", \"tree\":\"나무\", \"man\":\"남자사람\", \"woman\":\"여자사람\"}\n",
    "\n",
    "# === 속도/용량 옵션: 복사 대신 하드링크(같은 드라이브일 때 매우 빠름) ===\n",
    "USE_HARDLINK = True   # 같은 파일시스템이면 권장\n",
    "USE_SYMLINK  = False  # Windows에선 권한 필요할 수 있음\n",
    "SKIP_COPY    = False  # True면 이미지 링크/복사 생략(원본만 사용하고 싶을 때)\n",
    "\n",
    "def make_link_or_copy(src: Path, dst: Path):\n",
    "    if SKIP_COPY or dst.exists():\n",
    "        return\n",
    "    dst.parent.mkdir(parents=True, exist_ok=True)\n",
    "    try:\n",
    "        if USE_HARDLINK:\n",
    "            os.link(src, dst)     # 같은 드라이브/파티션이어야 함\n",
    "        elif USE_SYMLINK:\n",
    "            os.symlink(src, dst)\n",
    "        else:\n",
    "            shutil.copy2(src, dst)\n",
    "    except Exception:\n",
    "        # 실패 시 안전하게 복사\n",
    "        shutil.copy2(src, dst)\n",
    "\n",
    "def yolo_line(cls_idx, x, y, w, h, W, H):\n",
    "    # YOLO 포맷: class xc yc w h (0~1 정규화)\n",
    "    xc = (x + w/2) / W\n",
    "    yc = (y + h/2) / H\n",
    "    nw = w / W\n",
    "    nh = h / H\n",
    "    return f\"{cls_idx} {xc:.6f} {yc:.6f} {nw:.6f} {nh:.6f}\\n\"\n",
    "\n",
    "def parse_wh_from_json(meta):\n",
    "    # \"1280x1280\" 같은 문자열을 우선 신뢰(이미지 열지 않아도 됨 → 매우 빠름)\n",
    "    res = (meta or {}).get(\"img_resolution\") or \"\"\n",
    "    if \"x\" in res:\n",
    "        try:\n",
    "            w, h = res.split(\"x\")\n",
    "            return int(w), int(h)\n",
    "        except Exception:\n",
    "            return None, None\n",
    "    return None, None\n",
    "\n",
    "def index_origin_images(origin_root: Path):\n",
    "    # 원천데이터 전체를 1회 스캔해 stem → 경로 인덱스 생성 (확장자 혼용 대비)\n",
    "    stem2path = {}\n",
    "    for p in origin_root.rglob(\"*\"):\n",
    "        if p.is_file() and p.suffix.lower() in {\".jpg\",\".jpeg\",\".png\"}:\n",
    "            # 동일 stem이 여러 번 나오면 첫 번째만 사용(일반적으로 중복 없음)\n",
    "            stem2path.setdefault(p.stem, p)\n",
    "    return stem2path\n",
    "\n",
    "summary = {}\n",
    "for split_name, cfg in SPLITS.items():\n",
    "    origin_root = cfg[\"origin\"]\n",
    "    label_root  = cfg[\"label\"]\n",
    "    out_root    = cfg[\"out\"]\n",
    "    out_img_dir = out_root / \"images\"\n",
    "    out_lbl_dir = out_root / \"labels\"\n",
    "    out_img_dir.mkdir(parents=True, exist_ok=True)\n",
    "    out_lbl_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    if not origin_root.exists() or not label_root.exists():\n",
    "        print(f\"[{split_name}] 경로 확인 필요 - origin:{origin_root.exists()} | label:{label_root.exists()}\")\n",
    "        continue\n",
    "\n",
    "    # 1) 원천데이터 인덱스(한 번만 스캔)\n",
    "    stem2path = index_origin_images(origin_root)\n",
    "\n",
    "    # 2) 라벨 JSON 경로 모으기\n",
    "    json_files = []\n",
    "    for cls in [\"house\",\"man\",\"tree\",\"woman\"]:\n",
    "        ko = KO_CLASS[cls]\n",
    "        # 스플릿에 맞는 접두사 폴더(TL_/VL_) 찾기\n",
    "        label_dir = label_root / f\"{cfg['label_sub_prefix']}{ko}\"\n",
    "        if label_dir.exists():\n",
    "            json_files += sorted(label_dir.glob(\"*.json\"))\n",
    "        else:\n",
    "            print(f\"[{split_name}] 라벨 폴더 없음: {label_dir}\")\n",
    "\n",
    "    # 3) 변환 루프\n",
    "    written, skip_no_label, skip_no_img, skip_no_wh = 0, 0, 0, 0\n",
    "\n",
    "    for js in tqdm(json_files, desc=f\"{split_name} JSON→YOLO\", unit=\"file\"):\n",
    "        try:\n",
    "            data = json.loads(js.read_text(encoding=\"utf-8\"))\n",
    "        except Exception:\n",
    "            continue\n",
    "\n",
    "        # (a) 대상 클래스/라벨 키 결정\n",
    "        # 현재 json이 어느 폴더(집/나무/남자/여자)에서 왔는지로 클래스 판정\n",
    "        if \"집\" in js.parts:\n",
    "            key_label, class_name = TARGET_KEYS[\"house\"]\n",
    "        elif \"나무\" in js.parts:\n",
    "            key_label, class_name = TARGET_KEYS[\"tree\"]\n",
    "        elif \"남자사람\" in js.parts:\n",
    "            key_label, class_name = TARGET_KEYS[\"man\"]\n",
    "        elif \"여자사람\" in js.parts:\n",
    "            key_label, class_name = TARGET_KEYS[\"woman\"]\n",
    "        else:\n",
    "            # 혹시 모를 예외: 파일명에 한글 클래스가 안 보이면 meta.class 참고\n",
    "            meta_cls = (data.get(\"annotations\") or {}).get(\"class\", \"\")\n",
    "            if meta_cls == \"집\":\n",
    "                key_label, class_name = TARGET_KEYS[\"house\"]\n",
    "            elif meta_cls == \"나무\":\n",
    "                key_label, class_name = TARGET_KEYS[\"tree\"]\n",
    "            elif meta_cls == \"남자사람\":\n",
    "                key_label, class_name = TARGET_KEYS[\"man\"]\n",
    "            elif meta_cls == \"여자사람\":\n",
    "                key_label, class_name = TARGET_KEYS[\"woman\"]\n",
    "            else:\n",
    "                continue\n",
    "        cls_idx = CLASSES[class_name]\n",
    "\n",
    "        # (b) 타깃 박스(집전체/나무전체/사람전체)만 추출\n",
    "        boxes = []\n",
    "        for b in (data.get(\"annotations\") or {}).get(\"bbox\", []):\n",
    "            if b.get(\"label\") == key_label:\n",
    "                boxes.append(b)\n",
    "        if not boxes:\n",
    "            skip_no_label += 1\n",
    "            continue\n",
    "\n",
    "        # (c) 이미지 찾기: stem 기반(라벨 파일명과 동일 stem)\n",
    "        stem = js.stem\n",
    "        img_path = stem2path.get(stem)\n",
    "        if img_path is None:\n",
    "            skip_no_img += 1\n",
    "            continue\n",
    "\n",
    "        # (d) 이미지 크기: JSON의 img_resolution 사용(빠름)\n",
    "        W, H = parse_wh_from_json(data.get(\"meta\") or {})\n",
    "        if not W or not H:\n",
    "            # 해상도 정보가 없으면 이미지 열어 크기 확인(느리지만 호환)\n",
    "            try:\n",
    "                from PIL import Image\n",
    "                with Image.open(img_path) as im:\n",
    "                    W, H = im.size\n",
    "            except Exception:\n",
    "                skip_no_wh += 1\n",
    "                continue\n",
    "\n",
    "        # (e) YOLO 라벨 작성(경계 클램프)\n",
    "        lines = []\n",
    "        for b in boxes:\n",
    "            x = float(b[\"x\"]); y = float(b[\"y\"])\n",
    "            w = float(b[\"w\"]); h = float(b[\"h\"])\n",
    "            x = max(0, min(x, W-1))\n",
    "            y = max(0, min(y, H-1))\n",
    "            w = max(1, min(w, W - x))\n",
    "            h = max(1, min(h, H - y))\n",
    "            lines.append(yolo_line(cls_idx, x, y, w, h, W, H))\n",
    "        if not lines:\n",
    "            skip_no_label += 1\n",
    "            continue\n",
    "\n",
    "        # (f) 저장: 이미지 링크/복사 + 라벨 txt\n",
    "        out_img = out_img_dir / img_path.name\n",
    "        out_lbl = out_lbl_dir / (img_path.stem + \".txt\")\n",
    "        make_link_or_copy(img_path, out_img)\n",
    "        out_lbl.write_text(\"\".join(lines), encoding=\"utf-8\")\n",
    "        written += 1\n",
    "\n",
    "    summary[split_name] = dict(\n",
    "        written=written,\n",
    "        skip_no_label=skip_no_label,\n",
    "        skip_no_img=skip_no_img,\n",
    "        skip_no_wh=skip_no_wh\n",
    "    )\n",
    "    print(f\"\\n[{split_name}] 변환: {written}개 | 스킵(라벨없음): {skip_no_label} | 스킵(이미지없음): {skip_no_img} | 스킵(해상도확인실패): {skip_no_wh}\")\n",
    "\n",
    "print(\"\\n== 전체 요약 ==\")\n",
    "for k, v in summary.items():\n",
    "    print(f\"{k}: {v['written']}개 (no_label {v['skip_no_label']}, no_img {v['skip_no_img']}, no_wh {v['skip_no_wh']})\")\n",
    "print(\"✅ 완료: yolo_training/, yolo_validation/ 에 images/ + labels/ 생성\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e43f800",
   "metadata": {},
   "source": [
    "## 1-2. 환경/경로/클래스 설정 + 하이퍼파라미터"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "01fd89b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEVICE: cuda\n"
     ]
    }
   ],
   "source": [
    "# ===== 셀 1: 설정/하이퍼파라미터 =====\n",
    "import os, math, json, random, time\n",
    "from pathlib import Path\n",
    "from typing import List, Tuple\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image, ImageDraw\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.ops import nms\n",
    "\n",
    "# 재현성\n",
    "SEED = 42\n",
    "random.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED); torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "# 경로 (변환된 YOLO 데이터)\n",
    "TRAIN_ROOT = Path(\"yolo_training\")\n",
    "VAL_ROOT   = Path(\"yolo_validation\")\n",
    "IMG_DIRNAME = \"images\"\n",
    "LBL_DIRNAME = \"labels\"\n",
    "\n",
    "# 클래스\n",
    "CLASS2IDX = {\"tree\":0, \"man\":1, \"woman\":2, \"house\":3}\n",
    "IDX2CLASS = {v:k for k,v in CLASS2IDX.items()}\n",
    "NUM_CLASSES = len(CLASS2IDX)\n",
    "\n",
    "# 입력/그리드\n",
    "IMG_SIZE  = 416\n",
    "GRID_SIZE = 13\n",
    "STRIDE    = IMG_SIZE // GRID_SIZE\n",
    "\n",
    "# 앵커 개수\n",
    "NUM_ANCHORS = 5\n",
    "\n",
    "# 하이퍼파라미터\n",
    "BATCH_SIZE    = 16\n",
    "EPOCHS        = 5            # 빠르게 확인하려면 5~10으로 시작\n",
    "LR            = 1e-3\n",
    "WEIGHT_DECAY  = 5e-4\n",
    "WARMUP_EPOCHS = 2\n",
    "VAL_EVERY     = 3             # 검증 주기(에폭 단위) – 시간 절약용\n",
    "\n",
    "LAMBDA_COORD  = 5.0\n",
    "LAMBDA_NOOBJ  = 0.5\n",
    "IGNORE_IOU    = 0.5\n",
    "\n",
    "# 디바이스\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"DEVICE:\", DEVICE)\n",
    "\n",
    "# 가속 옵션\n",
    "torch.backends.cudnn.benchmark = True   # 입력 크기 고정 시 속도↑\n",
    "cv2.setNumThreads(0)                    # OpenCV 스레드 줄여 워커와 경쟁↓\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5d3b271c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(44800, 5600)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ===== 셀 2: Dataset/전처리 + DataLoader =====\n",
    "from typing import Optional\n",
    "\n",
    "def letterbox(im: np.ndarray, new_size=416, color=(114,114,114)):\n",
    "    h, w = im.shape[:2]\n",
    "    scale = min(new_size / h, new_size / w)\n",
    "    nh, nw = int(round(h * scale)), int(round(w * scale))\n",
    "    im_resized = cv2.resize(im, (nw, nh), interpolation=cv2.INTER_LINEAR)\n",
    "    top = (new_size - nh) // 2\n",
    "    bottom = new_size - nh - top\n",
    "    left = (new_size - nw) // 2\n",
    "    right = new_size - nw - left\n",
    "    im_padded = cv2.copyMakeBorder(im_resized, top, bottom, left, right, cv2.BORDER_CONSTANT, value=color)\n",
    "    return im_padded, scale, left, top\n",
    "\n",
    "def load_labels(txt_path: Path):\n",
    "    boxes = []\n",
    "    if not txt_path.exists():\n",
    "        return boxes\n",
    "    with open(txt_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            ss = line.strip().split()\n",
    "            if len(ss) != 5:\n",
    "                continue\n",
    "            cls = int(ss[0]); xc = float(ss[1]); yc = float(ss[2]); w = float(ss[3]); h = float(ss[4])\n",
    "            boxes.append([cls, xc, yc, w, h])\n",
    "    return boxes\n",
    "\n",
    "# 견고한 이미지 로더: cv2 → imdecode → PIL\n",
    "def imread_robust(path: Path) -> Optional[np.ndarray]:\n",
    "    img = cv2.imread(str(path))\n",
    "    if img is not None:\n",
    "        return img\n",
    "    try:\n",
    "        data = np.fromfile(str(path), dtype=np.uint8)\n",
    "        img  = cv2.imdecode(data, cv2.IMREAD_COLOR)\n",
    "        if img is not None:\n",
    "            return img\n",
    "    except Exception:\n",
    "        pass\n",
    "    try:\n",
    "        img = Image.open(path).convert(\"RGB\")\n",
    "        img = np.array(img)[:, :, ::-1].copy()  # RGB→BGR\n",
    "        return img\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "class YOLODataset(Dataset):\n",
    "    def __init__(self, root: Path, img_size=416, augment=False):\n",
    "        self.img_dir = root / IMG_DIRNAME\n",
    "        self.lbl_dir = root / LBL_DIRNAME\n",
    "        self.img_paths = sorted([p for p in self.img_dir.glob(\"*\") if p.suffix.lower() in [\".jpg\",\".jpeg\",\".png\"]])\n",
    "        self.img_size = img_size\n",
    "        self.augment = augment\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.img_paths[idx]\n",
    "        lbl_path = (self.lbl_dir / img_path.stem).with_suffix(\".txt\")\n",
    "\n",
    "        img = imread_robust(img_path)\n",
    "        if img is None:\n",
    "            raise FileNotFoundError(f\"[imread_robust 실패] {img_path}\")\n",
    "\n",
    "        img, scale, padw, padh = letterbox(img, self.img_size)\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB).astype(np.float32) / 255.0\n",
    "\n",
    "        labels = load_labels(lbl_path)\n",
    "        labels = np.array(labels, dtype=np.float32) if labels else np.zeros((0,5), dtype=np.float32)\n",
    "\n",
    "        img = torch.from_numpy(img).permute(2,0,1)\n",
    "        labels = torch.from_numpy(labels)\n",
    "        return img_path.name, img, labels\n",
    "\n",
    "def collate_fn(batch):\n",
    "    names, imgs, labels = zip(*batch)\n",
    "    imgs = torch.stack(imgs, 0)\n",
    "    return names, imgs, labels\n",
    "\n",
    "train_ds = YOLODataset(TRAIN_ROOT, img_size=IMG_SIZE, augment=True)\n",
    "val_ds   = YOLODataset(VAL_ROOT,   img_size=IMG_SIZE, augment=False)\n",
    "\n",
    "# DataLoader – 윈도우면 2~4 권장\n",
    "NUM_WORKERS = max(2, min(8, (os.cpu_count() or 8) // 2))\n",
    "train_dl = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True,\n",
    "                      num_workers=NUM_WORKERS, pin_memory=True,\n",
    "                      persistent_workers=True, prefetch_factor=2,\n",
    "                      collate_fn=collate_fn)\n",
    "val_dl   = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False,\n",
    "                      num_workers=max(2, NUM_WORKERS//2), pin_memory=True,\n",
    "                      persistent_workers=True, prefetch_factor=2,\n",
    "                      collate_fn=collate_fn)\n",
    "\n",
    "len(train_ds), len(val_ds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "00525837",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "수집한 w,h 개수: 30001 (파일 30000/44800)\n",
      "앵커(픽셀, WxH):\n",
      " [[ 87.4249  153.4    ]\n",
      " [117.      257.72488]\n",
      " [159.24979 198.24979]\n",
      " [180.04979 313.6249 ]\n",
      " [268.1249  368.22488]]\n"
     ]
    }
   ],
   "source": [
    "# ===== 셀 3: 빠른 앵커 계산 (라벨 txt만, 멀티스레드) =====\n",
    "import random\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "LABELS_DIR = TRAIN_ROOT / LBL_DIRNAME   # yolo_training/labels\n",
    "IMG_SIZE_  = IMG_SIZE\n",
    "K          = NUM_ANCHORS\n",
    "MAX_FILES  = 30000       # 파일 샘플 상한(속도용)\n",
    "MAX_BOXES  = 300000      # 전체 박스 상한(속도/메모리용)\n",
    "WORKERS    = min(32, (os.cpu_count() or 8) * 2)\n",
    "\n",
    "all_txts = [p for p in LABELS_DIR.iterdir() if p.suffix.lower()==\".txt\"]\n",
    "random.shuffle(all_txts)\n",
    "txts = all_txts[:MAX_FILES]\n",
    "\n",
    "def parse_wh(txt_path: Path):\n",
    "    wh_local = []\n",
    "    try:\n",
    "        with open(txt_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            for line in f:\n",
    "                ss = line.strip().split()\n",
    "                if len(ss)==5:\n",
    "                    w = float(ss[3]) * IMG_SIZE_\n",
    "                    h = float(ss[4]) * IMG_SIZE_\n",
    "                    if w>0 and h>0:\n",
    "                        wh_local.append((w,h))\n",
    "    except Exception:\n",
    "        pass\n",
    "    return wh_local\n",
    "\n",
    "wh_list = []\n",
    "with ThreadPoolExecutor(max_workers=WORKERS) as ex:\n",
    "    futures = [ex.submit(parse_wh, p) for p in txts]\n",
    "    for fu in as_completed(futures):\n",
    "        wh_list.extend(fu.result())\n",
    "        if len(wh_list) >= MAX_BOXES:\n",
    "            break\n",
    "\n",
    "wh = np.array(wh_list, dtype=np.float32)\n",
    "print(f\"수집한 w,h 개수: {len(wh)} (파일 {len(txts)}/{len(all_txts)})\")\n",
    "\n",
    "def iou_wh(wh1, wh2):\n",
    "    w1, h1 = wh1[:,0][:,None], wh1[:,1][:,None]\n",
    "    w2, h2 = wh2[:,0][None,:], wh2[:,1][None,:]\n",
    "    inter  = np.minimum(w1, w2) * np.minimum(h1, h2)\n",
    "    area1  = w1*h1; area2 = w2*h2\n",
    "    return inter / (area1 + area2 - inter + 1e-9)\n",
    "\n",
    "def kmeanspp_init(data, k):\n",
    "    centroids = [data[np.random.randint(len(data))]]\n",
    "    for _ in range(1, k):\n",
    "        d2 = np.min([np.sum((data - c)**2, axis=1) for c in centroids], axis=0)\n",
    "        probs = d2 / (d2.sum() + 1e-9)\n",
    "        idx = np.random.choice(len(data), p=probs)\n",
    "        centroids.append(data[idx])\n",
    "    return np.stack(centroids, axis=0)\n",
    "\n",
    "if len(wh) < K:\n",
    "    print(\"라벨이 부족해서 기본 앵커 사용\")\n",
    "    anchors = np.array([[12,16],[19,36],[40,28],[36,75],[76,55]], dtype=np.float32)\n",
    "else:\n",
    "    centroids = kmeanspp_init(wh, K)\n",
    "    for _ in range(25):  # 이터레이션 단축(보통 충분)\n",
    "        iou = iou_wh(wh, centroids)\n",
    "        clusters = np.argmax(iou, axis=1)\n",
    "        new_centroids = []\n",
    "        changed = False\n",
    "        for ki in range(K):\n",
    "            pts = wh[clusters==ki]\n",
    "            if len(pts)==0:\n",
    "                new_centroids.append(centroids[ki])\n",
    "            else:\n",
    "                med = np.median(pts, axis=0)\n",
    "                new_centroids.append(med)\n",
    "                if np.any(np.abs(med - centroids[ki]) > 1e-3):\n",
    "                    changed = True\n",
    "        centroids = np.array(new_centroids)\n",
    "        if not changed:\n",
    "            break\n",
    "    order = np.argsort(centroids.prod(axis=1))\n",
    "    anchors = centroids[order]\n",
    "\n",
    "print(\"앵커(픽셀, WxH):\\n\", anchors)\n",
    "\n",
    "# 학습에서 사용할 텐서\n",
    "ANCHORS = torch.tensor(anchors, dtype=torch.float32).to(DEVICE)\n",
    "\n",
    "# (선택) 캐시\n",
    "# np.save(\"anchors.npy\", anchors)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "119cfc27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13.305293, 'M params')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ===== 셀 4: YOLOv2 간단 모델 =====\n",
    "def conv_bn_lrelu(c_in, c_out, k=3, s=1, p=1):\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(c_in, c_out, k, s, p, bias=False),\n",
    "        nn.BatchNorm2d(c_out),\n",
    "        nn.LeakyReLU(0.1, inplace=True),\n",
    "    )\n",
    "\n",
    "class YOLOv2Tiny(nn.Module):\n",
    "    def __init__(self, num_classes=4, num_anchors=5):\n",
    "        super().__init__()\n",
    "        c = [32, 64, 128, 256, 512, 1024]\n",
    "        self.layer1 = nn.Sequential(\n",
    "            conv_bn_lrelu(3, c[0], 3,1,1),\n",
    "            nn.MaxPool2d(2,2),   # 208\n",
    "            conv_bn_lrelu(c[0], c[1], 3,1,1),\n",
    "            nn.MaxPool2d(2,2),   # 104\n",
    "            conv_bn_lrelu(c[1], c[2], 3,1,1),\n",
    "            conv_bn_lrelu(c[2], c[1], 1,1,0),\n",
    "            conv_bn_lrelu(c[1], c[2], 3,1,1),\n",
    "            nn.MaxPool2d(2,2),   # 52\n",
    "            conv_bn_lrelu(c[2], c[3], 3,1,1),\n",
    "            conv_bn_lrelu(c[3], c[2], 1,1,0),\n",
    "            conv_bn_lrelu(c[2], c[3], 3,1,1),\n",
    "            nn.MaxPool2d(2,2),   # 26\n",
    "            conv_bn_lrelu(c[3], c[4], 3,1,1),\n",
    "            conv_bn_lrelu(c[4], c[3], 1,1,0),\n",
    "            conv_bn_lrelu(c[3], c[4], 3,1,1),\n",
    "            nn.MaxPool2d(2,2),   # 13\n",
    "            conv_bn_lrelu(c[4], c[5], 3,1,1),\n",
    "            conv_bn_lrelu(c[5], c[4], 1,1,0),\n",
    "            conv_bn_lrelu(c[4], c[5], 3,1,1),\n",
    "        )\n",
    "        out_ch = num_anchors * (5 + num_classes)\n",
    "        self.head = nn.Conv2d(c[5], out_ch, 1,1,0)\n",
    "\n",
    "        self.num_classes = num_classes\n",
    "        self.num_anchors = num_anchors\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layer1(x)\n",
    "        x = self.head(x)  # [B, A*(5+C), 13, 13]\n",
    "        return x\n",
    "\n",
    "model = YOLOv2Tiny(num_classes=NUM_CLASSES, num_anchors=NUM_ANCHORS).to(DEVICE)\n",
    "sum(p.numel() for p in model.parameters())/1e6, \"M params\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0fe74d79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== 셀 5: 타깃 할당 & 손실 (with logits) =====\n",
    "def build_targets(labels_list, anchors, S=13, num_classes=4):\n",
    "    B = len(labels_list)\n",
    "    A = anchors.size(0)\n",
    "    target = torch.zeros(B, A, S, S, 5+num_classes, device=DEVICE)\n",
    "    for b_idx, labels in enumerate(labels_list):\n",
    "        if labels is None or len(labels)==0:\n",
    "            continue\n",
    "        labels = labels.to(DEVICE).clone()\n",
    "        labels[:,1:] *= torch.tensor([IMG_SIZE, IMG_SIZE, IMG_SIZE, IMG_SIZE], device=DEVICE)\n",
    "        for cls, xc, yc, bw, bh in labels:\n",
    "            gi = int(xc // STRIDE); gj = int(yc // STRIDE)\n",
    "            gi = min(max(gi,0), S-1); gj = min(max(gj,0), S-1)\n",
    "            box = torch.tensor([bw, bh], device=DEVICE)[None,:]\n",
    "            inter = torch.min(box[:,0], anchors[:,0]) * torch.min(box[:,1], anchors[:,1])\n",
    "            area1 = box[:,0]*box[:,1]; area2 = anchors[:,0]*anchors[:,1]\n",
    "            iou = inter / (area1 + area2 - inter + 1e-16)\n",
    "            a = torch.argmax(iou).item()\n",
    "            tx = (xc / STRIDE) - gi\n",
    "            ty = (yc / STRIDE) - gj\n",
    "            tw = torch.log(bw / anchors[a,0] + 1e-16)\n",
    "            th = torch.log(bh / anchors[a,1] + 1e-16)\n",
    "            target[b_idx, a, gj, gi, 0] = tx\n",
    "            target[b_idx, a, gj, gi, 1] = ty\n",
    "            target[b_idx, a, gj, gi, 2] = tw\n",
    "            target[b_idx, a, gj, gi, 3] = th\n",
    "            target[b_idx, a, gj, gi, 4] = 1.0\n",
    "            target[b_idx, a, gj, gi, 5 + int(cls.item())] = 1.0\n",
    "    return target\n",
    "\n",
    "class YOLOv2Loss(nn.Module):\n",
    "    def __init__(self, anchors, num_classes=4, lambda_coord=5.0, lambda_noobj=0.5, ignore_iou=0.5):\n",
    "        super().__init__()\n",
    "        self.anchors = anchors\n",
    "        self.num_classes = num_classes\n",
    "        self.lambda_coord = lambda_coord\n",
    "        self.lambda_noobj = lambda_noobj\n",
    "        self.ignore_iou = ignore_iou\n",
    "\n",
    "    def forward(self, pred, target):\n",
    "        B, _, S, _ = pred.shape\n",
    "        A = self.anchors.size(0); C = self.num_classes\n",
    "\n",
    "        # [B,A,S,S,5+C]\n",
    "        pred = pred.reshape(B, A, 5+C, S, S).permute(0,1,3,4,2)\n",
    "\n",
    "        # 좌표: px,py는 sigmoid, pw,ph는 log-스페이스\n",
    "        px = torch.sigmoid(pred[...,0]); py = torch.sigmoid(pred[...,1])\n",
    "        pw = pred[...,2];               ph = pred[...,3]\n",
    "\n",
    "        # objectness/cls는 \"로짓\" 그대로 두고 BCEWithLogits 사용\n",
    "        lo          = pred[...,4]        # objectness logits\n",
    "        pcls_logits = pred[...,5:]       # class logits\n",
    "\n",
    "        # 타깃\n",
    "        tx,ty,tw,th = target[...,0],target[...,1],target[...,2],target[...,3]\n",
    "        tobj, tcls  = target[...,4], target[...,5:]\n",
    "        obj_mask = tobj.bool()\n",
    "\n",
    "        # coord loss\n",
    "        loss_x = F.mse_loss(px[obj_mask], tx[obj_mask], reduction='sum') if obj_mask.any() else torch.tensor(0., device=pred.device)\n",
    "        loss_y = F.mse_loss(py[obj_mask], ty[obj_mask], reduction='sum') if obj_mask.any() else torch.tensor(0., device=pred.device)\n",
    "        loss_w = F.mse_loss(pw[obj_mask], tw[obj_mask], reduction='sum') if obj_mask.any() else torch.tensor(0., device=pred.device)\n",
    "        loss_h = F.mse_loss(ph[obj_mask], th[obj_mask], reduction='sum') if obj_mask.any() else torch.tensor(0., device=pred.device)\n",
    "        loss_coord = self.lambda_coord * (loss_x + loss_y + loss_w + loss_h)\n",
    "\n",
    "        # cls loss (with logits)\n",
    "        loss_cls = F.binary_cross_entropy_with_logits(pcls_logits[obj_mask], tcls[obj_mask], reduction='sum') \\\n",
    "                   if obj_mask.any() else torch.tensor(0., device=pred.device)\n",
    "\n",
    "        # ignore_iou 계산용 보조\n",
    "        grid_y, grid_x = torch.meshgrid(torch.arange(S, device=pred.device), torch.arange(S, device=pred.device), indexing='ij')\n",
    "        grid_x = grid_x[None,None,:,:]; grid_y = grid_y[None,None,:,:]\n",
    "        ax = self.anchors[:,0].view(1,A,1,1); ay = self.anchors[:,1].view(1,A,1,1)\n",
    "        bx = (px + grid_x) * STRIDE\n",
    "        by = (py + grid_y) * STRIDE\n",
    "        bw = torch.exp(pw) * ax\n",
    "        bh = torch.exp(ph) * ay\n",
    "        def to_xyxy(cx,cy,w,h):\n",
    "            x1 = cx - w/2; y1 = cy - h/2\n",
    "            x2 = cx + w/2; y2 = cy + h/2\n",
    "            return x1,y1,x2,y2\n",
    "        px1,py1,px2,py2 = to_xyxy(bx,by,bw,bh)\n",
    "        ignore_mask = torch.zeros_like(tobj, dtype=torch.bool)\n",
    "        for b in range(B):\n",
    "            if not obj_mask[b].any():\n",
    "                continue\n",
    "            gtx = (tx[b] + grid_x) * STRIDE\n",
    "            gty = (ty[b] + grid_y) * STRIDE\n",
    "            gtw = torch.exp(tw[b]) * ax\n",
    "            gth = torch.exp(th[b]) * ay\n",
    "            gx1,gy1,gx2,gy2 = to_xyxy(gtx,gty,gtw,gth)\n",
    "            inter_x1 = torch.maximum(px1[b], gx1)\n",
    "            inter_y1 = torch.maximum(py1[b], gy1)\n",
    "            inter_x2 = torch.minimum(px2[b], gx2)\n",
    "            inter_y2 = torch.minimum(py2[b], gy2)\n",
    "            inter_w = torch.clamp(inter_x2 - inter_x1, min=0)\n",
    "            inter_h = torch.clamp(inter_y2 - inter_y1, min=0)\n",
    "            inter = inter_w * inter_h\n",
    "            area_p = (px2[b]-px1[b]) * (py2[b]-py1[b])\n",
    "            area_g = (gx2-gx1) * (gy2-gy1)\n",
    "            iou = inter / (area_p + area_g - inter + 1e-16)\n",
    "            ignore_mask[b] = iou.detach() > self.ignore_iou\n",
    "\n",
    "        # obj/noobj (with logits)\n",
    "        loss_obj   = F.binary_cross_entropy_with_logits(lo[obj_mask], tobj[obj_mask], reduction='sum') \\\n",
    "                     if obj_mask.any() else torch.tensor(0., device=pred.device)\n",
    "        noobj_mask = (~obj_mask) & (~ignore_mask)\n",
    "        loss_noobj = self.lambda_noobj * F.binary_cross_entropy_with_logits(lo[noobj_mask], tobj[noobj_mask], reduction='sum') \\\n",
    "                     if noobj_mask.any() else torch.tensor(0., device=pred.device)\n",
    "\n",
    "        loss = (loss_coord + loss_cls + loss_obj + loss_noobj) / max(1,B)\n",
    "        stats = dict(loss=loss.item(),\n",
    "                     coord=loss_coord.item()/max(1,B),\n",
    "                     cls=loss_cls.item()/max(1,B) if obj_mask.any() else 0.0,\n",
    "                     obj=loss_obj.item()/max(1,B) if obj_mask.any() else 0.0,\n",
    "                     noobj=loss_noobj.item()/max(1,B) if noobj_mask.any() else 0.0)\n",
    "        return loss, stats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dbc66b04",
   "metadata": {},
   "outputs": [],
   "source": [
    "try: del criterion\n",
    "except: pass\n",
    "\n",
    "criterion = YOLOv2Loss(ANCHORS, num_classes=NUM_CLASSES,\n",
    "                       lambda_coord=LAMBDA_COORD, lambda_noobj=LAMBDA_NOOBJ,\n",
    "                       ignore_iou=IGNORE_IOU)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7b27b252",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ smoke test OK | loss: 105.512939453125\n"
     ]
    }
   ],
   "source": [
    "from torch import amp\n",
    "\n",
    "names, imgs, labels_list = next(iter(train_dl))\n",
    "imgs = imgs.to(DEVICE)\n",
    "target = build_targets(labels_list, ANCHORS, S=GRID_SIZE, num_classes=NUM_CLASSES)\n",
    "\n",
    "with amp.autocast(device_type='cuda', dtype=torch.float16, enabled=(DEVICE=='cuda')):\n",
    "    pred = model(imgs)\n",
    "    loss, stats = criterion(pred, target)\n",
    "print(\"✅ smoke test OK | loss:\", float(loss))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6bf8aae8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[E01] Train 3.2194 (coord 0.928 | cls 0.972 | obj 0.495 | noobj 0.824)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[E02] Train 1.4309 (coord 0.622 | cls 0.466 | obj 0.203 | noobj 0.140)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[E03] Train 1.0866 | Val 1.1735  ↳ ✅ Saved yolov2_4cls.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[E04] Train 0.8449 (coord 0.412 | cls 0.274 | obj 0.091 | noobj 0.068)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                          "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[E05] Train 0.6425 | Val 0.7157  ↳ ✅ Saved yolov2_4cls.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "# ===== 셀 6: 학습 루프 & 저장 (AMP 최신화, dtype/device 정렬) =====\n",
    "from tqdm import tqdm\n",
    "from torch import amp\n",
    "\n",
    "# 혹시 모를 dtype 꼬임 방지: 모델을 항상 fp32로 고정해 GPU에 올려 둠\n",
    "model = model.to(DEVICE).float()\n",
    "\n",
    "def build_optimizer(model, lr=1e-3, wd=5e-4):\n",
    "    return torch.optim.Adam(model.parameters(), lr=lr, weight_decay=wd)\n",
    "\n",
    "optimizer = build_optimizer(model, LR, WEIGHT_DECAY)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=EPOCHS)\n",
    "\n",
    "# AMP 스케일러 (CUDA일 때만 의미 있음)\n",
    "scaler = amp.GradScaler('cuda' if DEVICE == 'cuda' else 'cpu')\n",
    "\n",
    "def train_one_epoch(model, loader, optimizer):\n",
    "    model.train()\n",
    "    meters = {\"loss\":0,\"coord\":0,\"cls\":0,\"obj\":0,\"noobj\":0,\"n\":0}\n",
    "    for names, imgs, labels_list in tqdm(loader, desc=\"Train\", leave=False):\n",
    "        # 입력을 반드시 디바이스로 (fp32 유지)\n",
    "        imgs = imgs.to(DEVICE, non_blocking=True)\n",
    "\n",
    "        # 타깃 생성은 내부에서 DEVICE 맞춰짐\n",
    "        target = build_targets(labels_list, ANCHORS, S=GRID_SIZE, num_classes=NUM_CLASSES)\n",
    "\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        # CUDA에서만 autocast 활성화 (dtype=fp16)\n",
    "        with amp.autocast(device_type='cuda', dtype=torch.float16, enabled=(DEVICE=='cuda')):\n",
    "            pred = model(imgs)\n",
    "            loss, stats = criterion(pred, target)\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        for k in [\"loss\",\"coord\",\"cls\",\"obj\",\"noobj\"]:\n",
    "            meters[k] += stats[k]\n",
    "        meters[\"n\"] += 1\n",
    "    for k in [\"loss\",\"coord\",\"cls\",\"obj\",\"noobj\"]:\n",
    "        meters[k] = meters[k] / max(1, meters[\"n\"])\n",
    "    return meters\n",
    "\n",
    "@torch.no_grad()\n",
    "def validate(model, loader):\n",
    "    model.eval()\n",
    "    meters = {\"loss\":0,\"coord\":0,\"cls\":0,\"obj\":0,\"noobj\":0,\"n\":0}\n",
    "    for names, imgs, labels_list in tqdm(loader, desc=\"Val\", leave=False):\n",
    "        imgs = imgs.to(DEVICE, non_blocking=True)\n",
    "        target = build_targets(labels_list, ANCHORS, S=GRID_SIZE, num_classes=NUM_CLASSES)\n",
    "        # 평가도 autocast로 빠르게 (CUDA일 때만)\n",
    "        with amp.autocast(device_type='cuda', dtype=torch.float16, enabled=(DEVICE=='cuda')):\n",
    "            pred = model(imgs)\n",
    "            loss, stats = criterion(pred, target)\n",
    "        for k in [\"loss\",\"coord\",\"cls\",\"obj\",\"noobj\"]:\n",
    "            meters[k] += stats[k]\n",
    "        meters[\"n\"] += 1\n",
    "    for k in [\"loss\",\"coord\",\"cls\",\"obj\",\"noobj\"]:\n",
    "        meters[k] = meters[k] / max(1, meters[\"n\"])\n",
    "    return meters\n",
    "\n",
    "BEST = 1e9\n",
    "SAVE_PATH = \"yolov2_4cls.pt\"\n",
    "\n",
    "for ep in range(1, EPOCHS+1):\n",
    "    tr = train_one_epoch(model, train_dl, optimizer)\n",
    "    if ep % VAL_EVERY == 0 or ep == EPOCHS:\n",
    "        va = validate(model, val_dl)\n",
    "        if va[\"loss\"] < BEST:\n",
    "            BEST = va[\"loss\"]\n",
    "            torch.save({\"model\": model.state_dict(),\n",
    "                        \"anchors\": ANCHORS.detach().cpu().numpy(),\n",
    "                        \"classes\": IDX2CLASS,\n",
    "                        \"img_size\": IMG_SIZE}, SAVE_PATH)\n",
    "            print(f\"[E{ep:02d}] Train {tr['loss']:.4f} | Val {va['loss']:.4f}  ↳ ✅ Saved {SAVE_PATH}\")\n",
    "    else:\n",
    "        print(f\"[E{ep:02d}] Train {tr['loss']:.4f} (coord {tr['coord']:.3f} | cls {tr['cls']:.3f} | obj {tr['obj']:.3f} | noobj {tr['noobj']:.3f})\")\n",
    "    scheduler.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "abfd9359",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved: runs_yolo2_vis\\나무_10_남_00013_pred.jpg\n",
      "saved: runs_yolo2_vis\\나무_10_남_00022_pred.jpg\n"
     ]
    }
   ],
   "source": [
    "# ===== 셀 7 (최종): 견고한 로더 + with-logits 디코딩 + 클래스별 NMS + 유니코드 안전 저장 =====\n",
    "from torch import amp\n",
    "from pathlib import Path\n",
    "import cv2, torch\n",
    "import numpy as np\n",
    "from torchvision.ops import nms\n",
    "\n",
    "# 간이 imread_robust (셀 2에 이미 동일 함수가 있으면 생략 가능)\n",
    "def imread_robust(path: Path):\n",
    "    img = cv2.imread(str(path))\n",
    "    if img is not None:\n",
    "        return img\n",
    "    # 유니코드 경로 대응\n",
    "    try:\n",
    "        data = np.fromfile(str(path), dtype=np.uint8)\n",
    "        img  = cv2.imdecode(data, cv2.IMREAD_COLOR)\n",
    "        if img is not None:\n",
    "            return img\n",
    "    except Exception:\n",
    "        pass\n",
    "    # PIL fallback\n",
    "    try:\n",
    "        from PIL import Image\n",
    "        img = Image.open(path).convert(\"RGB\")\n",
    "        return np.array(img)[:, :, ::-1].copy()  # RGB->BGR\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "# 유니코드(한글) 경로에서도 안전한 저장\n",
    "def imwrite_unicode(path: Path, img) -> bool:\n",
    "    path = str(path)\n",
    "    ext = Path(path).suffix or \".jpg\"\n",
    "    ok, buf = cv2.imencode(ext, img)\n",
    "    if not ok:\n",
    "        return False\n",
    "    try:\n",
    "        buf.tofile(path)\n",
    "        return True\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "@torch.no_grad()\n",
    "def yolo_decode(\n",
    "    pred,\n",
    "    conf_thres=0.95,\n",
    "    nms_thres=0.05,\n",
    "    max_det=300,\n",
    "    per_cell_top1=True,        # ★ 같은 셀에서는 앵커 1개만 허용\n",
    "    min_area_ratio=0.06,       # ★ 너무 작은 박스 제거 (416^2 대비 비율)\n",
    "    single_instance_per_class=True  # ★ 이미지당 클래스별 1개만 남기기 옵션\n",
    "):\n",
    "    \"\"\"\n",
    "    pred: [B, A*(5+C), S, S]\n",
    "    return: list of tensors, each [N,6] = [x1,y1,x2,y2,score,cls]\n",
    "    \"\"\"\n",
    "    B, _, S, _ = pred.shape\n",
    "    A, C = NUM_ANCHORS, NUM_CLASSES\n",
    "    img_area = (IMG_SIZE * IMG_SIZE)\n",
    "    min_area = img_area * float(min_area_ratio)\n",
    "\n",
    "    # [B, A, S, S, 5+C]\n",
    "    pred = pred.reshape(B, A, 5 + C, S, S).permute(0, 1, 3, 4, 2).contiguous()\n",
    "\n",
    "    # grid / anchors\n",
    "    gy, gx = torch.meshgrid(\n",
    "        torch.arange(S, device=pred.device),\n",
    "        torch.arange(S, device=pred.device),\n",
    "        indexing='ij'\n",
    "    )\n",
    "    ax = ANCHORS[:, 0].view(A, 1, 1)\n",
    "    ay = ANCHORS[:, 1].view(A, 1, 1)\n",
    "\n",
    "    outs = []\n",
    "    for b in range(B):\n",
    "        p = pred[b]  # [A,S,S,5+C]\n",
    "\n",
    "        # coords / logits\n",
    "        px = torch.sigmoid(p[..., 0])  # [A,S,S]\n",
    "        py = torch.sigmoid(p[..., 1])\n",
    "        pw = p[..., 2]\n",
    "        ph = p[..., 3]\n",
    "        lo = p[..., 4]                 # objectness logits\n",
    "        po = torch.sigmoid(lo)         # objectness prob\n",
    "        pcls = torch.softmax(p[..., 5:], -1)  # [A,S,S,C]\n",
    "\n",
    "        # absolute boxes (416 space)\n",
    "        bx = (px + gx) * STRIDE\n",
    "        by = (py + gy) * STRIDE\n",
    "        bw = torch.exp(pw) * ax\n",
    "        bh = torch.exp(ph) * ay\n",
    "\n",
    "        x1 = bx - bw / 2\n",
    "        y1 = by - bh / 2\n",
    "        x2 = bx + bw / 2\n",
    "        y2 = by + bh / 2\n",
    "\n",
    "        # scores\n",
    "        cls_scores, cls_idx = torch.max(pcls, dim=-1, keepdim=True)  # [A,S,S,1]\n",
    "        conf = (po.unsqueeze(-1) * cls_scores).squeeze(-1)           # [A,S,S]\n",
    "\n",
    "        # --- (1) per‑cell top‑1 suppression across anchors ---\n",
    "        if per_cell_top1:\n",
    "            # 각 (S,S) 위치에서 A개 앵커 중 최고 앵커만 True\n",
    "            best_a = torch.argmax(conf, dim=0)            # [S,S]\n",
    "            keep_anchor = torch.zeros_like(conf, dtype=torch.bool)  # [A,S,S]\n",
    "            for a in range(A):\n",
    "                keep_anchor[a] = (best_a == a)\n",
    "            conf = torch.where(keep_anchor, conf, torch.zeros_like(conf))\n",
    "\n",
    "        # threshold\n",
    "        mask = conf > conf_thres\n",
    "        if not mask.any():\n",
    "            outs.append(torch.zeros((0, 6), device=pred.device))\n",
    "            continue\n",
    "\n",
    "        # gather\n",
    "        xs = x1[mask].float(); ys = y1[mask].float()\n",
    "        xe = x2[mask].float(); ye = y2[mask].float()\n",
    "        sc = conf[mask].float()\n",
    "        cl = cls_idx.squeeze(-1)[mask].float()\n",
    "\n",
    "        # --- (2) too-small box filter (remove leaf/root fragments) ---\n",
    "        w = (xe - xs).clamp(min=0)\n",
    "        h = (ye - ys).clamp(min=0)\n",
    "        areas = w * h\n",
    "        big = areas >= min_area\n",
    "        if big.sum() == 0:\n",
    "            outs.append(torch.zeros((0, 6), device=pred.device))\n",
    "            continue\n",
    "        xs, ys, xe, ye, sc, cl = xs[big], ys[big], xe[big], ye[big], sc[big], cl[big]\n",
    "\n",
    "        boxes = torch.stack([xs, ys, xe, ye], dim=1)  # [N,4]\n",
    "\n",
    "        # --- (3) class-wise NMS ---\n",
    "        det_list = []\n",
    "        classes = cl.long()\n",
    "        for cval in torch.unique(classes):\n",
    "            idx = (classes == cval)\n",
    "            if idx.sum() == 0:\n",
    "                continue\n",
    "            keep = nms(boxes[idx], sc[idx], nms_thres)\n",
    "            det_c = torch.cat([boxes[idx][keep], sc[idx][keep][:, None], cl[idx][keep][:, None]], dim=1)\n",
    "            # optional: class-wise top-1\n",
    "            if single_instance_per_class and det_c.size(0) > 0:\n",
    "                best = torch.argmax(det_c[:, 4])\n",
    "                det_c = det_c[best:best+1]\n",
    "            det_list.append(det_c)\n",
    "\n",
    "        det = torch.cat(det_list, dim=0) if det_list else torch.zeros((0, 6), device=pred.device)\n",
    "\n",
    "        # max_det\n",
    "        if det.numel():\n",
    "            order = torch.argsort(det[:, 4], descending=True)\n",
    "            det = det[order][:max_det]\n",
    "\n",
    "        outs.append(det)\n",
    "    return outs\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def infer_and_draw(model, img_paths: list[Path], out_dir=\"runs_yolo2_vis\",\n",
    "                   conf=0.50, nms_thr=0.30, to_orig=True, max_det=300):\n",
    "    \"\"\"\n",
    "    to_orig=True: 레터박스 → 원본 좌표로 역변환하여 원본 이미지 위에 그림(권장)\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    out_dir = Path(out_dir); out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    for p in img_paths:\n",
    "        orig = imread_robust(p)\n",
    "        if orig is None:\n",
    "            print(f\"[skip] 이미지 로드 실패: {p}\")\n",
    "            continue\n",
    "\n",
    "        # 416 레터박스\n",
    "        img, scale, padw, padh = letterbox(orig, IMG_SIZE)\n",
    "        img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB).astype(np.float32) / 255.0\n",
    "        t = torch.from_numpy(img_rgb).permute(2, 0, 1)[None].to(DEVICE)\n",
    "\n",
    "        # CUDA일 때만 AMP 켬\n",
    "        with amp.autocast(device_type='cuda', dtype=torch.float16, enabled=(DEVICE=='cuda')):\n",
    "            pred = model(t)\n",
    "\n",
    "        dets = yolo_decode(pred, conf_thres=conf, nms_thres=nms_thr, max_det=max_det)[0].detach().cpu().numpy()\n",
    "\n",
    "        # 시각화: 원본 좌표로 역변환해서 그리기\n",
    "        vis = orig.copy()\n",
    "        for x1, y1, x2, y2, score, cls in dets:\n",
    "            if to_orig:\n",
    "                x1 = int(max(0, min(vis.shape[1]-1, (x1 - padw) / scale)))\n",
    "                y1 = int(max(0, min(vis.shape[0]-1, (y1 - padh) / scale)))\n",
    "                x2 = int(max(0, min(vis.shape[1],     (x2 - padw) / scale)))\n",
    "                y2 = int(max(0, min(vis.shape[0],     (y2 - padh) / scale)))\n",
    "            else:\n",
    "                x1, y1, x2, y2 = map(int, [x1, y1, x2, y2])\n",
    "\n",
    "            if x2 <= x1 or y2 <= y1:\n",
    "                continue\n",
    "            color = (0, 255, 0)\n",
    "            cv2.rectangle(vis, (x1, y1), (x2, y2), color, 2)\n",
    "            cv2.putText(vis, f\"{IDX2CLASS[int(cls)]}:{score:.2f}\",\n",
    "                        (x1, max(0, y1-5)), cv2.FONT_HERSHEY_SIMPLEX, 0.6, color, 1, cv2.LINE_AA)\n",
    "\n",
    "        out_path = out_dir / f\"{p.stem}_pred.jpg\"\n",
    "        ok = imwrite_unicode(out_path, vis)\n",
    "        print((\"saved:\" if ok else \"[FAIL SAVE]\"), out_path)\n",
    "\n",
    "# 샘플 2장 시각화 (중복 박스 억제를 위해 임계치/스레시홀드 보수적으로)\n",
    "sample_imgs = sorted((TRAIN_ROOT / IMG_DIRNAME).glob(\"*.jpg\"))[:2]\n",
    "infer_and_draw(model, sample_imgs, out_dir=\"runs_yolo2_vis\", conf=0.50, nms_thr=0.30, to_orig=True, max_det=300)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "46aa87b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 크롭 저장 완료: crops/(tree|man|woman|house) | 저장 771개 | 모드=orig\n"
     ]
    }
   ],
   "source": [
    "# ===== 셀 8: 검출 박스 크롭 저장 (ResNet 단계 대비, 원본좌표 역-레터박스 지원) =====\n",
    "from torch import amp\n",
    "from pathlib import Path\n",
    "import cv2, torch\n",
    "import numpy as np\n",
    "\n",
    "# 셀 7에서 이미 정의했다면 중복 정의 생략 가능\n",
    "def imread_robust(path: Path):\n",
    "    img = cv2.imread(str(path))\n",
    "    if img is not None:\n",
    "        return img\n",
    "    try:\n",
    "        data = np.fromfile(str(path), dtype=np.uint8)     # 유니코드 경로 대응\n",
    "        img  = cv2.imdecode(data, cv2.IMREAD_COLOR)\n",
    "        if img is not None:\n",
    "            return img\n",
    "    except Exception:\n",
    "        pass\n",
    "    try:\n",
    "        from PIL import Image\n",
    "        img = Image.open(path).convert(\"RGB\")\n",
    "        return np.array(img)[:, :, ::-1].copy()           # RGB->BGR\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def imwrite_unicode(path: Path, img) -> bool:\n",
    "    path = str(path)\n",
    "    ext = Path(path).suffix or \".jpg\"\n",
    "    ok, buf = cv2.imencode(ext, img)\n",
    "    if not ok:\n",
    "        return False\n",
    "    try:\n",
    "        buf.tofile(path)\n",
    "        return True\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "@torch.no_grad()\n",
    "def save_crops_for_resnet(\n",
    "    model,\n",
    "    img_paths: list[Path],\n",
    "    out_root=\"crops\",\n",
    "    conf=0.25,\n",
    "    nms_thr=0.45,\n",
    "    size=224,\n",
    "    space='orig'  # 'orig' = 원본 이미지 좌표로 자르기, 'lbox' = 416 레터박스 좌표로 자르기\n",
    "):\n",
    "    \"\"\"\n",
    "    각 이미지에서 탐지 박스를 크롭하여 클래스별 폴더에 저장.\n",
    "    파일명: <원본stem>_<i>_<cls>_<score>.jpg\n",
    "    \"\"\"\n",
    "    assert space in ('orig', 'lbox')\n",
    "    model.eval()\n",
    "    out_root = Path(out_root)\n",
    "\n",
    "    # 클래스 폴더 생성\n",
    "    for c in IDX2CLASS.values():\n",
    "        (out_root / c).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    saved = 0\n",
    "    for p in img_paths:\n",
    "        orig = imread_robust(p)\n",
    "        if orig is None:\n",
    "            print(f\"[skip] 이미지 로드 실패: {p}\")\n",
    "            continue\n",
    "\n",
    "        # 416 레터박스\n",
    "        img, scale, padw, padh = letterbox(orig, IMG_SIZE)\n",
    "        img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB).astype(np.float32) / 255.0\n",
    "        t = torch.from_numpy(img_rgb).permute(2, 0, 1)[None].to(DEVICE)\n",
    "\n",
    "        # CUDA일 때만 AMP 켬\n",
    "        with amp.autocast(device_type='cuda', dtype=torch.float16, enabled=(DEVICE=='cuda')):\n",
    "            pred = model(t)\n",
    "\n",
    "        dets = yolo_decode(pred, conf_thres=conf, nms_thres=nms_thr)[0].detach().cpu().numpy()\n",
    "\n",
    "        for i, (x1, y1, x2, y2, score, cls) in enumerate(dets):\n",
    "            cls = int(cls)\n",
    "            x1, y1, x2, y2 = float(x1), float(y1), float(x2), float(y2)\n",
    "\n",
    "            if space == 'orig':\n",
    "                # 레터박스 → 원본 좌표 역변환\n",
    "                x1o = int(max(0, min(orig.shape[1]-1, (x1 - padw) / scale)))\n",
    "                y1o = int(max(0, min(orig.shape[0]-1, (y1 - padh) / scale)))\n",
    "                x2o = int(max(0, min(orig.shape[1],     (x2 - padw) / scale)))\n",
    "                y2o = int(max(0, min(orig.shape[0],     (y2 - padh) / scale)))\n",
    "                if x2o <= x1o or y2o <= y1o:\n",
    "                    continue\n",
    "                crop = orig[y1o:y2o, x1o:x2o].copy()\n",
    "            else:\n",
    "                # 레터박스 공간에서 바로 크롭\n",
    "                xi1, yi1, xi2, yi2 = map(int, [x1, y1, x2, y2])\n",
    "                xi1 = max(0, min(xi1, img.shape[1]-1))\n",
    "                yi1 = max(0, min(yi1, img.shape[0]-1))\n",
    "                xi2 = max(0, min(xi2, img.shape[1]))\n",
    "                yi2 = max(0, min(yi2, img.shape[0]))\n",
    "                if xi2 <= xi1 or yi2 <= yi1:\n",
    "                    continue\n",
    "                crop = img[yi1:yi2, xi1:xi2].copy()\n",
    "\n",
    "            if crop.size == 0:\n",
    "                continue\n",
    "\n",
    "            crop = cv2.resize(crop, (size, size), interpolation=cv2.INTER_LINEAR)\n",
    "            out_path = out_root / IDX2CLASS[cls] / f\"{p.stem}_{i}_{IDX2CLASS[cls]}_{score:.2f}.jpg\"\n",
    "            ok = imwrite_unicode(out_path, crop)\n",
    "            if not ok:\n",
    "                print(\"[FAIL SAVE]\", out_path)\n",
    "            else:\n",
    "                saved += 1\n",
    "\n",
    "    print(f\"✅ 크롭 저장 완료: {out_root}/(tree|man|woman|house) | 저장 {saved}개 | 모드={space}\")\n",
    "\n",
    "# 예시: 학습셋 앞 200장으로 크롭 생성 (원본 좌표로 자르기)\n",
    "sample_imgs_more = sorted((TRAIN_ROOT / IMG_DIRNAME).glob(\"*.jpg\"))[:200]\n",
    "save_crops_for_resnet(model, sample_imgs_more, out_root=\"crops\", conf=0.25, nms_thr=0.45, size=224, space='orig')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12873f9f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
