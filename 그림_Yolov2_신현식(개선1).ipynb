{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "f585edce",
      "metadata": {
        "id": "f585edce"
      },
      "source": [
        "\n",
        "# YOLOv2\n",
        "> Darknet-19 + Passthrough(reorg) + Anchor 기반 YOLOv2 구현\n",
        "\n",
        "**Pipeline**\n",
        "1. 환경 체크 & 설정\n",
        "2. 데이터셋 unzip → JSON 라벨 파싱 (bbox: x,y,w,h)\n",
        "3. 클래스 자동 생성, 데이터 통계\n",
        "4. K-means로 앵커(5개) 추정 (IoU distance)\n",
        "5. YOLOv2 모델(Darknet-19 유사 + passthrough) 구현\n",
        "6. 손실 함수(YOLOv2 스타일) 구현\n",
        "7. 학습/검증 루프\n",
        "8. 추론 및 시각화 (NMS 포함)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a7beae92",
      "metadata": {
        "id": "a7beae92"
      },
      "source": [
        "## 1) 환경 체크 & 기본 설정"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "8bb1f7e7",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8bb1f7e7",
        "outputId": "c2cfa090-69b0-4290-c97b-472643ffc4e2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PyTorch: 2.8.0+cu126\n",
            "CUDA available: False\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cpu')"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "# (선택) GPU 체크\n",
        "import torch, os, sys, math, random, json, zipfile, pathlib, time, shutil\n",
        "import numpy as np\n",
        "from PIL import Image, ImageDraw, ImageFont\n",
        "import torch.nn as nn\n",
        "import torchvision.transforms as T\n",
        "from torchvision.models import resnet18\n",
        "import torchvision.models as models\n",
        "import torchvision\n",
        "from torchvision import models, transforms\n",
        "import torch.optim as optim\n",
        "import cv2\n",
        "\n",
        "print(\"PyTorch:\", torch.__version__)\n",
        "print(\"CUDA available:\", torch.cuda.is_available())\n",
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "DEVICE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "GczUgYbh0wIn",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GczUgYbh0wIn",
        "outputId": "c8c0945c-7d65-4334-e886-3f5d4a42c065"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7534bf0b",
      "metadata": {
        "id": "7534bf0b"
      },
      "source": [
        "## 2) 경로 설정 & 데이터셋 Unzip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "1b14c1bf",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1b14c1bf",
        "outputId": "f69afeb1-8261-441d-d36e-0c1082fc757c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using zip: /content/drive/MyDrive/AI활용 소프트웨어 개발/13. 생성형 AI/data/picture.zip\n",
            "Extracted to: data\n",
            "원천: data/Sample/01.원천데이터\n",
            "라벨: data/Sample/02.라벨링데이터\n"
          ]
        }
      ],
      "source": [
        "# pathlib 라이브러리에서 Path 객체 불러오기 (경로를 객체처럼 다룰 수 있음)\n",
        "from pathlib import Path\n",
        "\n",
        "# 탐색할 경로 리스트 (drawings.zip 이 있는 위치를 미리 지정)\n",
        "zip_candidates = [Path(\"/content/drive/MyDrive/AI활용 소프트웨어 개발/13. 생성형 AI/data/picture.zip\")]\n",
        "\n",
        "# 실제 사용할 zip 경로 변수 (아직 없음)\n",
        "zip_path = None\n",
        "\n",
        "# 존재하는 파일을 찾으면 zip_path에 할당\n",
        "for c in zip_candidates:\n",
        "    if c.exists():\n",
        "        zip_path = c\n",
        "        break\n",
        "\n",
        "# zip_path를 못 찾으면 에러 발생\n",
        "if zip_path is None:\n",
        "    raise FileNotFoundError(\"picture.zip 파일을 찾을 수 없습니다. 업로드 또는 경로 확인!\")\n",
        "print(\"Using zip:\", zip_path)\n",
        "\n",
        "# 압축 해제할 루트 폴더 지정 (./data)\n",
        "root = Path(\"./data\")\n",
        "\n",
        "# 기존 폴더가 있으면 삭제 (재실행 대비)\n",
        "if root.exists():\n",
        "    print(\"기존 추출 폴더 삭제:\", root)\n",
        "    shutil.rmtree(root)\n",
        "\n",
        "# data 폴더 새로 생성\n",
        "root.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# zipfile 라이브러리로 압축 해제\n",
        "import zipfile\n",
        "with zipfile.ZipFile(zip_path, 'r') as zf:\n",
        "    zf.extractall(root) # ./data 폴더에 압축 풀기\n",
        "\n",
        "print(\"Extracted to:\", root)\n",
        "\n",
        "# 추출한 폴더 내부에서 \"01.원천데이터\" 폴더 찾기\n",
        "src_dir = next(root.glob(\"**/01.원천데이터\"))\n",
        "# 추출한 폴더 내부에서 \"02.라벨링데이터\" 폴더 찾기\n",
        "lbl_dir = next(root.glob(\"**/02.라벨링데이터\"))\n",
        "\n",
        "print(\"원천:\", src_dir)\n",
        "print(\"라벨:\", lbl_dir)\n",
        "\n",
        "# 이미지 확장자 모음 (이미지 필터링에 사용)\n",
        "IMG_EXTS = {\".jpg\",\".jpeg\",\".png\",\".bmp\"}"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "becba3da",
      "metadata": {
        "id": "becba3da"
      },
      "source": [
        "## 3) 라벨 파싱: 클래스/박스 통계"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "12d5eec7",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "12d5eec7",
        "outputId": "b46771de-8a1e-4e30-a459-87b327fcc8c4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "라벨 파일 수: 560\n",
            "유효 이미지 수: 560\n",
            "라벨 분포 상위 20개: [('꽃', 637), ('눈', 560), ('팔', 560), ('발', 560), ('운동화', 560), ('다리', 559), ('귀', 557), ('손', 557), ('잔디', 525), ('주머니', 517), ('나뭇잎', 509), ('별', 485), ('열매', 477), ('단추', 290), ('사람전체', 280), ('머리', 280), ('얼굴', 280), ('코', 280), ('입', 280), ('머리카락', 280)]\n",
            "총 클래스 수: 47\n",
            "예시 클래스: ['가지', '구름', '굴뚝', '귀', '그네', '기둥', '길', '꽃', '나무', '나무전체', '나뭇잎', '남자구두', '눈', '다람쥐', '다리', '단추', '달', '머리', '머리카락', '목']\n",
            "앞 20장 크기 샘플: [(1280, 1280), (1280, 1280), (1280, 1280), (1280, 1280), (1280, 1280)]\n"
          ]
        }
      ],
      "source": [
        "from collections import defaultdict, Counter\n",
        "from PIL import Image\n",
        "\n",
        "# JSON 라벨 파일을 파싱하는 함수\n",
        "def parse_json(json_path):\n",
        "    with open(json_path, 'r', encoding='utf-8') as f:\n",
        "        d = json.load(f) # JSON 로드\n",
        "    meta = d.get(\"meta\", {}) # 이미지 메타데이터 (경로, 해상도 등)\n",
        "    ann = d.get(\"annotations\", {}) # 어노테이션 정보\n",
        "    bboxes = ann.get(\"bbox\", []) # 바운딩박스 리스트 (x,y,w,h,label 포함)\n",
        "    img_rel = meta.get(\"img_path\") # 이미지 상대 경로\n",
        "    # JSON 파일 위치 기준으로 실제 이미지 절대 경로 생성\n",
        "    img_path = (Path(json_path).parent / Path(img_rel)).resolve()\n",
        "    img_path = Path(str(img_path)).resolve()\n",
        "    return {\n",
        "        \"img_path\": img_path, # 이미지 실제 경로\n",
        "        \"bboxes\": bboxes,  # [{x,y,w,h,label} ...]\n",
        "        \"img_resolution\": meta.get(\"img_resolution\",\"\"), # 해상도 정보 (없으면 빈 문자열)\n",
        "        \"class\": ann.get(\"class\",\"\"), # 전체 클래스 이름 (없으면 빈 문자열)\n",
        "    }\n",
        "\n",
        "# 이미지 크기를 (width, height) 형태로 반환\n",
        "def image_size(path):\n",
        "    try:\n",
        "        with Image.open(path) as im:\n",
        "            return im.size # (W, H)\n",
        "    except Exception:\n",
        "        return None # 이미지가 깨졌거나 없으면 None 반환\n",
        "\n",
        "# 라벨 JSON 파일 리스트 수집\n",
        "json_list = sorted([p for p in lbl_dir.rglob(\"*.json\")])\n",
        "print(\"라벨 파일 수:\", len(json_list))\n",
        "\n",
        "items = [] # 최종 이미지 + 어노테이션 모음\n",
        "label_counter = Counter() # 라벨별 개수를 세기 위한 Counter\n",
        "\n",
        "for jp in json_list:\n",
        "    info = parse_json(jp) # JSON 파싱\n",
        "    if not info[\"img_path\"].exists(): # 이미지 파일이 실제로 없으면\n",
        "        cand = list(src_dir.rglob(info[\"img_path\"].name)) # 원천데이터에서 이름으로 재검색\n",
        "        if cand:\n",
        "            info[\"img_path\"] = cand[0] # 발견 시 해당 경로로 교체\n",
        "        else:\n",
        "            continue # 이미지 못 찾으면 스킵\n",
        "    # 바운딩박스 안의 라벨들을 세기\n",
        "    for bb in info[\"bboxes\"]:\n",
        "        label_counter[bb.get(\"label\",\"UNKNOWN\")] += 1\n",
        "    items.append(info) # 유효 데이터만 리스트에 추가\n",
        "\n",
        "print(\"유효 이미지 수:\", len(items))\n",
        "print(\"라벨 분포 상위 20개:\", label_counter.most_common(20))\n",
        "\n",
        "# 클래스 인덱싱 (클래스명 <-> 숫자 ID 매핑)\n",
        "classes = sorted(label_counter.keys()) # 전체 클래스 정렬\n",
        "cls2idx = {c:i for i,c in enumerate(classes)} # 클래스명 → 인덱스\n",
        "idx2cls = {i:c for c,i in cls2idx.items()} # 인덱스 → 클래스명\n",
        "num_classes = len(classes)\n",
        "print(\"총 클래스 수:\", num_classes)\n",
        "print(\"예시 클래스:\", classes[:20])\n",
        "\n",
        "# 앞 20장 이미지 크기 확인\n",
        "sizes = []\n",
        "for it in items[:20]:\n",
        "    sizes.append(image_size(it[\"img_path\"]))\n",
        "print(\"앞 20장 크기 샘플:\", sizes[:5])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "986b42e6",
      "metadata": {
        "id": "986b42e6"
      },
      "source": [
        "## 4) K-means로 Anchor 5개 추정"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "8580f023",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8580f023",
        "outputId": "311ceb77-052a-4aa2-d9a6-55e47cd2d499"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "수집 박스 수: 13822\n",
            "Anchors (ratio):\n",
            " [[0.02265625 0.02421875]\n",
            " [0.35546875 0.3265625 ]\n",
            " [0.07421875 0.07421875]\n",
            " [0.146875   0.159375  ]\n",
            " [0.04140625 0.04453125]]\n",
            "Anchors (px @416):\n",
            " [[  9.425     10.075   ]\n",
            " [147.875    135.84999 ]\n",
            " [ 30.875     30.875   ]\n",
            " [ 61.1       66.299995]\n",
            " [ 17.225     18.525   ]]\n"
          ]
        }
      ],
      "source": [
        "# YOLO 입력 이미지 크기 (416x416)\n",
        "INPUT_SIZE = 416\n",
        "# anchor 개수 (클러스터 개수)\n",
        "K = 5\n",
        "\n",
        "whs = [] # 바운딩박스의 (width, height) 비율 저장 리스트\n",
        "for it in items:\n",
        "    sz = image_size(it[\"img_path\"]) # 이미지 크기 가져오기\n",
        "    if sz is None:\n",
        "        continue\n",
        "    w_img, h_img = sz\n",
        "    for bb in it[\"bboxes\"]:\n",
        "        # 바운딩박스의 w,h를 이미지 크기로 나누어 정규화 (0~1 범위)\n",
        "        bw = bb[\"w\"]/w_img\n",
        "        bh = bb[\"h\"]/h_img\n",
        "        if bw>0 and bh>0: # 유효한 박스만 저장\n",
        "            whs.append([bw,bh])\n",
        "whs = np.array(whs, dtype=np.float32)\n",
        "print(\"수집 박스 수:\", len(whs))\n",
        "\n",
        "# (w,h) 기반 IOU 계산 함수\n",
        "def bbox_iou_wh(box, centers):\n",
        "    w,h = box\n",
        "    ww = centers[:,0]; hh=centers[:,1]\n",
        "    inter = np.minimum(w,ww)*np.minimum(h,hh) # 교집합 면적\n",
        "    union = w*h + ww*hh - inter + 1e-9 # 합집합 면적\n",
        "    return inter/union # IOU 값 반환\n",
        "\n",
        "# K-means 기반 anchor box 추출 함수\n",
        "def kmeans_anchors(data, k=5, seed=42, iters=100):\n",
        "    n = data.shape[0]\n",
        "    np.random.seed(seed)\n",
        "    # 초기 중심값 무작위 선택\n",
        "    centers = data[np.random.choice(n, k, replace=False)].copy()\n",
        "    for _ in range(iters):\n",
        "        # --- assign 단계 ---\n",
        "        assign = []\n",
        "        for i in range(n):\n",
        "            ious = bbox_iou_wh(data[i], centers) # 각 박스와 중심들의 IOU\n",
        "            assign.append(int(np.argmax(ious))) # IOU가 가장 큰 중심에 할당\n",
        "        assign = np.array(assign)\n",
        "\n",
        "        # --- update 단계 ---\n",
        "        new_centers = []\n",
        "        for j in range(k):\n",
        "            pts = data[assign==j] # 클러스터에 속한 박스들\n",
        "            if len(pts)==0:\n",
        "                # 비어있으면 임의의 박스를 다시 선택\n",
        "                new_centers.append(data[np.random.randint(0,n)])\n",
        "            else:\n",
        "                # 중앙값(median)을 새 중심으로 업데이트\n",
        "                new_centers.append(np.median(pts, axis=0))\n",
        "        new_centers = np.array(new_centers)\n",
        "\n",
        "        # 중심이 변하지 않으면 종료\n",
        "        if np.allclose(centers, new_centers):\n",
        "            break\n",
        "        centers = new_centers\n",
        "    return centers\n",
        "\n",
        "# 실제로 anchor 추출 (데이터 충분하면 kmeans 실행)\n",
        "if len(whs) >= K:\n",
        "    anchors_wh = kmeans_anchors(whs, k=K)\n",
        "else:\n",
        "    # 데이터가 부족할 경우 기본 anchor 값 사용\n",
        "    anchors_wh = np.array([[0.04,0.05],[0.1,0.12],[0.12,0.19],[0.23,0.12],[0.26,0.25]], dtype=np.float32)\n",
        "\n",
        "# anchor 비율을 실제 입력 크기(416)로 변환\n",
        "anchors_px = (anchors_wh * INPUT_SIZE).astype(np.float32)\n",
        "print(\"Anchors (ratio):\\n\", anchors_wh) # 정규화된 비율\n",
        "print(\"Anchors (px @416):\\n\", anchors_px) # 416 입력 크기 기준 px 단위\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "67e429ba",
      "metadata": {
        "id": "67e429ba"
      },
      "source": [
        "## 5) 데이터셋 & 전처리"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "fda78850",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fda78850",
        "outputId": "40173a7b-7b0b-43ea-8eda-0a751bf9398c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(504, 56)"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "import random\n",
        "import numpy as np\n",
        "from PIL import Image, ImageOps, ImageEnhance\n",
        "import torch\n",
        "\n",
        "# YOLO grid 설정\n",
        "GRID_SIZE = 13\n",
        "STRIDE = INPUT_SIZE // GRID_SIZE  # 416/13 = 32 → 하나의 grid cell 크기\n",
        "\n",
        "# letterbox: 비율 유지한 resize 후 여백 채우기 (YOLO에서 자주 사용)\n",
        "def letterbox(im: Image.Image, new_size=INPUT_SIZE, color=(114,114,114)):\n",
        "    w,h = im.size\n",
        "    scale = min(new_size/w, new_size/h) # 가로/세로 비율 중 작은 쪽에 맞춰 축소\n",
        "    nw, nh = int(w*scale), int(h*scale) # 축소된 크기\n",
        "    im_resized = im.resize((nw,nh), Image.BILINEAR)  # 축소\n",
        "    canvas = Image.new('RGB', (new_size,new_size), color) # 빈 배경 생성\n",
        "    pad_x = (new_size - nw)//2\n",
        "    pad_y = (new_size - nh)//2\n",
        "    canvas.paste(im_resized, (pad_x, pad_y)) # 중앙에 배치\n",
        "    return canvas, scale, pad_x, pad_y\n",
        "\n",
        "# 색상/밝기 증강 (HSV augmentation)\n",
        "def augment_hsv(im: Image.Image, hgain=0.015, sgain=0.7, vgain=0.4):\n",
        "    s_factor = 1 + (random.random()*2-1)*sgain\n",
        "    v_factor = 1 + (random.random()*2-1)*vgain\n",
        "    im = ImageEnhance.Color(im).enhance(s_factor) # 채도 변화\n",
        "    im = ImageEnhance.Brightness(im).enhance(v_factor) # 밝기 변화\n",
        "    return im\n",
        "\n",
        "# 수평 반전 augmentation\n",
        "def random_hflip(im: Image.Image, boxes):\n",
        "    if random.random()<0.5:\n",
        "        w,h = im.size\n",
        "        im = ImageOps.mirror(im) # 좌우 반전\n",
        "        nboxes = []\n",
        "        for x,y,bw,bh,cls_id in boxes:\n",
        "            nx = w - (x + bw) # 반전된 좌표 계산\n",
        "            nboxes.append([nx,y,bw,bh,cls_id])\n",
        "        return im, nboxes\n",
        "    return im, boxes\n",
        "\n",
        "# target tensor 빌드 (YOLO ground truth format)\n",
        "def build_target_tensor(boxes, anchors, num_classes, grid_size=GRID_SIZE, img_size=INPUT_SIZE):\n",
        "    target = np.zeros((grid_size, grid_size, anchors.shape[0], 5 + num_classes), dtype=np.float32)\n",
        "    for (x,y,w,h,cls) in boxes:\n",
        "        cx, cy = x + w/2, y + h/2 # 바운딩박스 중심 좌표\n",
        "        gx, gy = int(cx//STRIDE), int(cy//STRIDE) # 해당 box가 속하는 grid cell\n",
        "        if not(0 <= gx < grid_size and 0 <= gy < grid_size):\n",
        "            continue\n",
        "        gt = np.array([w,h], dtype=np.float32)\n",
        "\n",
        "        # anchor 선택 (wh 기준 IOU 최대인 anchor)\n",
        "        ious = []\n",
        "        for a in anchors:\n",
        "            inter = np.minimum(gt[0], a[0]) * np.minimum(gt[1], a[1])\n",
        "            union = gt[0]*gt[1] + a[0]*a[1] - inter + 1e-9\n",
        "            ious.append(inter/union)\n",
        "        aidx = int(np.argmax(ious)) # 가장 IOU 큰 anchor 선택\n",
        "        aw, ah = anchors[aidx]\n",
        "\n",
        "        # 좌표 변환 (YOLO format)\n",
        "        cx_cell = (cx/STRIDE) - gx\n",
        "        cy_cell = (cy/STRIDE) - gy\n",
        "        tx, ty = cx_cell, cy_cell\n",
        "        tw = np.log((w/(aw+1e-9))+1e-9)\n",
        "        th = np.log((h/(ah+1e-9))+1e-9)\n",
        "\n",
        "        # target tensor 채우기\n",
        "        target[gy,gx,aidx,0]=tx\n",
        "        target[gy,gx,aidx,1]=ty\n",
        "        target[gy,gx,aidx,2]=tw\n",
        "        target[gy,gx,aidx,3]=th\n",
        "        target[gy,gx,aidx,4]=1.0  # objectness\n",
        "        target[gy,gx,aidx,5+cls]=1.0 # one-hot class\n",
        "    return target\n",
        "\n",
        "# --- Dataset 클래스 정의 ---\n",
        "class DrawingsYOLODataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, items, cls2idx, input_size=INPUT_SIZE, anchors_px=None, augment=True):\n",
        "        self.items = items\n",
        "        self.cls2idx = cls2idx\n",
        "        self.input_size = input_size\n",
        "        self.anchors = np.array(anchors_px, dtype=np.float32)\n",
        "        self.augment = augment\n",
        "    def __len__(self): return len(self.items)\n",
        "    def __getitem__(self, idx):\n",
        "        info = self.items[idx]\n",
        "        im = Image.open(info[\"img_path\"]).convert(\"RGB\") # 이미지 로드\n",
        "        boxes = []\n",
        "        # json에서 바운딩박스 읽기\n",
        "        for bb in info[\"bboxes\"]:\n",
        "            x,y,w,h = bb[\"x\"], bb[\"y\"], bb[\"w\"], bb[\"h\"]\n",
        "            cls_id = self.cls2idx.get(bb.get(\"label\",\"\"), -1)\n",
        "            if cls_id<0 or w<=0 or h<=0: continue\n",
        "            boxes.append([x,y,w,h,cls_id])\n",
        "        # augmentation\n",
        "        if self.augment:\n",
        "            im, boxes = random_hflip(im, boxes)\n",
        "            im = augment_hsv(im)\n",
        "        # letterbox resize\n",
        "        im_lb, scale, pad_x, pad_y = letterbox(im, self.input_size)\n",
        "        # resize/패딩된 좌표로 보정\n",
        "        nboxes = []\n",
        "        for x,y,w,h,cls_id in boxes:\n",
        "            x = x*scale + pad_x; y = y*scale + pad_y\n",
        "            w = w*scale; h = h*scale\n",
        "            nboxes.append([x,y,w,h,cls_id])\n",
        "        # target tensor 생성\n",
        "        target = build_target_tensor(nboxes, self.anchors, num_classes=len(self.cls2idx))\n",
        "        # 이미지 → [C,H,W] tensor\n",
        "        arr = np.array(im_lb).astype(np.float32)/255.0\n",
        "        arr = np.transpose(arr,(2,0,1))\n",
        "        img = torch.from_numpy(arr)\n",
        "        tgt = torch.from_numpy(target)\n",
        "        return img, tgt\n",
        "\n",
        "# collate_fn: dataloader가 batch 구성할 때 사용\n",
        "def collate_fn(b):\n",
        "    imgs = torch.stack([x[0] for x in b])\n",
        "    tgts = torch.stack([x[1] for x in b])\n",
        "    return imgs, tgts\n",
        "\n",
        "# --- 데이터 분할 ---\n",
        "random.seed(42)\n",
        "perm = list(range(len(items)))\n",
        "random.shuffle(perm)\n",
        "n_train = int(len(perm)*0.9)  # 90% train, 10% val\n",
        "train_idx, val_idx = perm[:n_train], perm[n_train:]\n",
        "train_items = [items[i] for i in train_idx]\n",
        "val_items = [items[i] for i in val_idx]\n",
        "\n",
        "# Dataset 생성\n",
        "train_ds = DrawingsYOLODataset(train_items, cls2idx, INPUT_SIZE, anchors_px, augment=True)\n",
        "val_ds   = DrawingsYOLODataset(val_items, cls2idx, INPUT_SIZE, anchors_px, augment=False)\n",
        "\n",
        "# DataLoader 생성\n",
        "train_loader = torch.utils.data.DataLoader(train_ds, batch_size=8, shuffle=True, num_workers=2, pin_memory=True, collate_fn=collate_fn)\n",
        "val_loader   = torch.utils.data.DataLoader(val_ds,   batch_size=8, shuffle=False, num_workers=2, pin_memory=True, collate_fn=collate_fn)\n",
        "\n",
        "# 데이터셋 크기 확인\n",
        "len(train_ds), len(val_ds)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f8d862ac",
      "metadata": {
        "id": "f8d862ac"
      },
      "source": [
        "## 6) YOLOv2 모델 (Darknet-19 유사 + Passthrough)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "019eb185",
      "metadata": {
        "id": "019eb185"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Conv → BN → LeakyReLU 블록 (YOLO에서 반복적으로 사용)\n",
        "def conv_bn_leaky(c_in, c_out, k=3, s=1, p=None):\n",
        "    if p is None: p = (k-1)//2 # 기본 padding: same\n",
        "    return nn.Sequential(\n",
        "        nn.Conv2d(c_in, c_out, k, s, p, bias=False), # Conv\n",
        "        nn.BatchNorm2d(c_out), # BatchNorm\n",
        "        nn.LeakyReLU(0.1, inplace=True) # LeakyReLU(0.1)\n",
        "    )\n",
        "\n",
        "# YOLOv2의 Backbone: Darknet-19\n",
        "class Darknet19(nn.Module):\n",
        "    # 416x416 입력 → 13x13 feature\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.layer1 = nn.Sequential(\n",
        "            conv_bn_leaky(3,32,3,1),\n",
        "            nn.MaxPool2d(2,2),  # 416 → 208\n",
        "        )\n",
        "        self.layer2 = nn.Sequential(\n",
        "            conv_bn_leaky(32,64,3,1),\n",
        "            nn.MaxPool2d(2,2),  # 208 → 104\n",
        "        )\n",
        "        self.layer3 = nn.Sequential(\n",
        "            conv_bn_leaky(64,128,3,1),\n",
        "            conv_bn_leaky(128,64,1,1),\n",
        "            conv_bn_leaky(64,128,3,1),\n",
        "            nn.MaxPool2d(2,2),  # 104 → 52\n",
        "        )\n",
        "        self.layer4 = nn.Sequential(\n",
        "            conv_bn_leaky(128,256,3,1),\n",
        "            conv_bn_leaky(256,128,1,1),\n",
        "            conv_bn_leaky(128,256,3,1),\n",
        "            nn.MaxPool2d(2,2),   # 52 → 26\n",
        "        )\n",
        "        self.layer5 = nn.Sequential(\n",
        "            conv_bn_leaky(256,512,3,1),\n",
        "            conv_bn_leaky(512,256,1,1),\n",
        "            conv_bn_leaky(256,512,3,1),\n",
        "            conv_bn_leaky(512,256,1,1),\n",
        "            conv_bn_leaky(256,512,3,1),\n",
        "        )  # 26x26 feature map\n",
        "        self.maxpool5 = nn.MaxPool2d(2,2) # 26 → 13\n",
        "        self.layer6 = nn.Sequential(\n",
        "            conv_bn_leaky(512,1024,3,1),\n",
        "            conv_bn_leaky(1024,512,1,1),\n",
        "            conv_bn_leaky(512,1024,3,1),\n",
        "            conv_bn_leaky(1024,512,1,1),\n",
        "            conv_bn_leaky(512,1024,3,1),\n",
        "        )  # 최종 13x13 feature map\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "        x = self.layer4(x)\n",
        "        route = self.layer5(x)    # 26x26 feature (passthrough 용)\n",
        "        x = self.maxpool5(route)  # 13x13\n",
        "        x = self.layer6(x)        # 13x13\n",
        "        return route, x # (26x26, 13x13) 둘 다 반환\n",
        "\n",
        "# Reorg Layer (YOLOv2 passthrough connection)\n",
        "# stride=2 → 26x26 feature → 13x13로 줄이면서 채널 수 4배 증가\n",
        "class Reorg(nn.Module):\n",
        "    def __init__(self, stride=2):\n",
        "        super().__init__()\n",
        "        self.stride = stride\n",
        "    def forward(self, x):\n",
        "        B,C,H,W = x.shape\n",
        "        s = self.stride\n",
        "        assert H%s==0 and W%s==0\n",
        "        # (B, C, H//s, s, W//s, s) → (B, C*s*s, H//s, W//s)\n",
        "        x = x.view(B, C, H//s, s, W//s, s).permute(0,1,3,5,2,4).contiguous()\n",
        "        x = x.view(B, C*s*s, H//s, W//s)\n",
        "        return x\n",
        "\n",
        "# YOLOv2 Head\n",
        "class YOLOv2(nn.Module):\n",
        "    def __init__(self, num_classes, anchors, grid_size=13):\n",
        "        super().__init__()\n",
        "        self.num_classes = num_classes\n",
        "        self.K = anchors.shape[0] # anchor 개수\n",
        "        self.S = grid_size  # grid 크기\n",
        "        self.anchors = torch.tensor(anchors, dtype=torch.float32) # anchor 리스트\n",
        "        self.backbone = Darknet19() # feature extractor (Darknet19)\n",
        "        self.passthrough_conv = conv_bn_leaky(512,64,1,1,0) # 26x26 → 채널 축소\n",
        "        self.reorg = Reorg(2) # 26x26 → 13x13, 채널 4배\n",
        "        self.head = nn.Sequential(\n",
        "            conv_bn_leaky(1024 + 64*4, 1024, 3,1), # concat 후 conv\n",
        "            nn.Conv2d(1024, self.K*(5+num_classes), 1,1,0) # 최종 예측\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        route26, x13 = self.backbone(x) # backbone 출력\n",
        "        p = self.passthrough_conv(route26) # 26x26 conv\n",
        "        p = self.reorg(p) # 26x26 → 13x13 (채널 증가)\n",
        "        x = torch.cat([x13, p], dim=1) # skip connection concat\n",
        "        out = self.head(x)  # (B, K*(5+C), 13, 13)\n",
        "        B,C,H,W = out.shape\n",
        "        # (B, 13, 13, K, 5+C) 로 reshape\n",
        "        out = out.permute(0,2,3,1).contiguous().view(B, H, W, self.K, 5+self.num_classes)\n",
        "        return out"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "20d07394",
      "metadata": {
        "id": "20d07394"
      },
      "source": [
        "## 7) 손실 함수"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "e1ee2374",
      "metadata": {
        "id": "e1ee2374"
      },
      "outputs": [],
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "class YOLOv2Loss(nn.Module):\n",
        "    def __init__(self, anchors, num_classes, lambda_coord=6.0, lambda_noobj=0.5, iou_ignore=0.5): # lambda_coord = 5.0 에서 6.0으로 변경\n",
        "        \"\"\"\n",
        "        개선 반영:\n",
        "          1) tx, ty에 sigmoid 적용 후 MSE\n",
        "          2) noobj ignore 마스크 (예측-모든 GT 최대 IoU > thresh 인 위치는 noobj 손실에서 제외)\n",
        "          3) positive의 objectness 타깃 = 예측-해당 GT IoU (detach)\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        # anchor box 크기 (px 단위)\n",
        "        self.anchors = torch.tensor(anchors, dtype=torch.float32)  # (K,2) in px@416\n",
        "        self.num_classes = num_classes\n",
        "        # 논문에서 사용된 손실 가중치\n",
        "        self.lambda_coord = lambda_coord # 좌표 손실 가중치 (기본 5)\n",
        "        self.lambda_noobj = lambda_noobj # noobj 손실 가중치 (기본 0.5)\n",
        "        self.iou_ignore = iou_ignore  # noobj 무시 임계값(기본 0.5)\n",
        "\n",
        "    def _xyxy_iou_pairwise(self, a, b):\n",
        "        \"\"\"\n",
        "        a: (Na,4), b:(Nb,4)  -> IoU (Na,Nb)\n",
        "        \"\"\"\n",
        "        if b.numel() == 0:\n",
        "            return a.new_zeros((a.shape[0], 0))\n",
        "        ax1, ay1, ax2, ay2 = a[:,0:1], a[:,1:2], a[:,2:3], a[:,3:4]\n",
        "        bx1, by1, bx2, by2 = b[:,0].unsqueeze(0), b[:,1].unsqueeze(0), b[:,2].unsqueeze(0), b[:,3].unsqueeze(0)\n",
        "\n",
        "        inter_x1 = torch.maximum(ax1, bx1)\n",
        "        inter_y1 = torch.maximum(ay1, by1)\n",
        "        inter_x2 = torch.minimum(ax2, bx2)\n",
        "        inter_y2 = torch.minimum(ay2, by2)\n",
        "        iw = (inter_x2 - inter_x1).clamp(min=0)\n",
        "        ih = (inter_y2 - inter_y1).clamp(min=0)\n",
        "        inter = iw * ih\n",
        "\n",
        "        area_a = (ax2 - ax1).clamp(min=0) * (ay2 - ay1).clamp(min=0)\n",
        "        area_b = (bx2 - bx1).clamp(min=0) * (by2 - by1).clamp(min=0)\n",
        "        union = area_a + area_b - inter + 1e-9\n",
        "        return inter / union\n",
        "\n",
        "    def _xyxy_iou_elementwise(self, a, b):\n",
        "        \"\"\"\n",
        "        a,b: (N,4) -> IoU(N,)  (같은 인덱스끼리 IoU)\n",
        "        \"\"\"\n",
        "        ax1, ay1, ax2, ay2 = a[:,0], a[:,1], a[:,2], a[:,3]\n",
        "        bx1, by1, bx2, by2 = b[:,0], b[:,1], b[:,2], b[:,3]\n",
        "        inter_x1 = torch.maximum(ax1, bx1)\n",
        "        inter_y1 = torch.maximum(ay1, by1)\n",
        "        inter_x2 = torch.minimum(ax2, bx2)\n",
        "        inter_y2 = torch.minimum(ay2, by2)\n",
        "        iw = (inter_x2 - inter_x1).clamp(min=0)\n",
        "        ih = (inter_y2 - inter_y1).clamp(min=0)\n",
        "        inter = iw * ih\n",
        "        area_a = (ax2 - ax1).clamp(min=0) * (ay2 - ay1).clamp(min=0)\n",
        "        area_b = (bx2 - bx1).clamp(min=0) * (by2 - by1).clamp(min=0)\n",
        "        union = area_a + area_b - inter + 1e-9\n",
        "        return inter / union\n",
        "\n",
        "    def forward(self, pred, target):\n",
        "        \"\"\"\n",
        "        pred   : (B, S, S, K, 5+C) raw\n",
        "        target : (B, S, S, K, 5+C) (tx,ty,tw,th,obj, one-hot class)\n",
        "        \"\"\"\n",
        "        device = pred.device\n",
        "        B, S, _, K, _ = pred.shape\n",
        "        anchors = self.anchors.to(device)  # (K,2)\n",
        "\n",
        "        # --- 분해 ---\n",
        "        p_tx, p_ty = pred[...,0], pred[...,1]\n",
        "        p_tw, p_th = pred[...,2], pred[...,3]\n",
        "        p_to      = pred[...,4]\n",
        "        p_cls     = pred[...,5:]\n",
        "\n",
        "        t_tx, t_ty = target[...,0], target[...,1]\n",
        "        t_tw, t_th = target[...,2], target[...,3]\n",
        "        t_to       = target[...,4]\n",
        "        t_cls      = target[...,5:]\n",
        "\n",
        "        obj   = t_to                # 1/0\n",
        "        noobj = 1.0 - obj\n",
        "\n",
        "        # ===== 1) 좌표 손실: tx,ty에 sigmoid 적용 후 MSE =====\n",
        "        p_tx_sig = torch.sigmoid(p_tx)\n",
        "        p_ty_sig = torch.sigmoid(p_ty)\n",
        "        pos_mask = obj.bool()\n",
        "        if pos_mask.any():\n",
        "            l_tx = F.mse_loss(p_tx_sig[pos_mask], t_tx[pos_mask], reduction='mean')\n",
        "            l_ty = F.mse_loss(p_ty_sig[pos_mask], t_ty[pos_mask], reduction='mean')\n",
        "            l_tw = F.mse_loss(p_tw[pos_mask], t_tw[pos_mask], reduction='mean')\n",
        "            l_th = F.mse_loss(p_th[pos_mask], t_th[pos_mask], reduction='mean')\n",
        "            l_coord = l_tx + l_ty + l_tw + l_th\n",
        "        else:\n",
        "            l_coord = torch.tensor(0.0, device=device)\n",
        "        # ===== 디코딩(예측/GT) → IoU 계산용 =====\n",
        "        # stride: 416 / S (전역 INPUT_SIZE를 쓰되, 없으면 32로 폴백)\n",
        "        try:\n",
        "            stride = float(INPUT_SIZE) / float(S)\n",
        "        except NameError:\n",
        "            stride = 32.0\n",
        "\n",
        "        # grid (S,S,K)\n",
        "        gx = torch.arange(S, device=device).view(1, S).repeat(S, 1)\n",
        "        gy = torch.arange(S, device=device).view(S, 1).repeat(1, S)\n",
        "        grid_x = gx.unsqueeze(-1).repeat(1, 1, K).float()\n",
        "        grid_y = gy.unsqueeze(-1).repeat(1, 1, K).float()\n",
        "\n",
        "        # anchors (1,1,K)\n",
        "        aw = anchors[:,0].view(1,1,K)\n",
        "        ah = anchors[:,1].view(1,1,K)\n",
        "\n",
        "        # 예측 박스 디코딩 (cx,cy,w,h) → (x1,y1,x2,y2)\n",
        "        pred_cx = (p_tx_sig + grid_x) * stride\n",
        "        pred_cy = (p_ty_sig + grid_y) * stride\n",
        "        pred_w  = aw * torch.exp(p_tw)\n",
        "        pred_h  = ah * torch.exp(p_th)\n",
        "        pred_x1 = pred_cx - pred_w/2\n",
        "        pred_y1 = pred_cy - pred_h/2\n",
        "        pred_x2 = pred_cx + pred_w/2\n",
        "        pred_y2 = pred_cy + pred_h/2\n",
        "\n",
        "        # GT 박스 디코딩 (tx,ty는 이미 0~1 오프셋)\n",
        "        gt_cx = (t_tx + grid_x) * stride\n",
        "        gt_cy = (t_ty + grid_y) * stride\n",
        "        gt_w  = aw * torch.exp(t_tw)\n",
        "        gt_h  = ah * torch.exp(t_th)\n",
        "        gt_x1 = gt_cx - gt_w/2\n",
        "        gt_y1 = gt_cy - gt_h/2\n",
        "        gt_x2 = gt_cx + gt_w/2\n",
        "        gt_y2 = gt_cy + gt_h/2\n",
        "\n",
        "        # 펼치기 (B, S,S,K,4)\n",
        "        pred_xyxy = torch.stack([pred_x1, pred_y1, pred_x2, pred_y2], dim=-1)\n",
        "        gt_xyxy   = torch.stack([gt_x1,   gt_y1,   gt_x2,   gt_y2  ], dim=-1)\n",
        "\n",
        "        # ===== 2) noobj ignore 마스크 생성 =====\n",
        "        # 각 배치별로 \"모든 예측\" vs \"해당 배치의 모든 GT( obj==1 )\" 최대 IoU 계산\n",
        "        max_iou = torch.zeros((B, S, S, K), device=device)\n",
        "        for b in range(B):\n",
        "            gt_mask_b = obj[b].bool()                        # (S,S,K)\n",
        "            if gt_mask_b.any():\n",
        "                gt_b = gt_xyxy[b][gt_mask_b]                 # (M,4)\n",
        "                pred_b = pred_xyxy[b].reshape(-1, 4)         # (S*S*K,4)\n",
        "                ious_b = self._xyxy_iou_pairwise(pred_b, gt_b)   # (S*S*K, M)\n",
        "                max_iou_b = ious_b.max(dim=1).values.reshape(S, S, K)\n",
        "                max_iou[b] = max_iou_b\n",
        "            # else: 그대로 0\n",
        "\n",
        "        ignore_mask = (max_iou > self.iou_ignore)  # (B,S,S,K) , self.iou_ignore 대신 상수(0.4 ~ 0.5) 적용하여 배경 손실 감소 1) 0.5 에서 0.4로 더 내림\n",
        "        eff_noobj = (~obj.bool()) & (~ignore_mask)\n",
        "\n",
        "        # ===== 3) objectness 타깃을 IoU로 =====\n",
        "        # positive 위치에서: pred vs 해당 GT의 IoU를 타깃으로 사용 (detach)\n",
        "        pos_mask = obj.bool()\n",
        "        if pos_mask.any():\n",
        "            pred_pos = pred_xyxy[pos_mask]                   # (Npos,4)\n",
        "            gt_pos   = gt_xyxy[pos_mask]                     # (Npos,4)  - 같은 인덱스의 GT\n",
        "            iou_pos  = self._xyxy_iou_elementwise(pred_pos, gt_pos).detach().clamp(0,1)\n",
        "            l_obj = F.binary_cross_entropy_with_logits(p_to[pos_mask], iou_pos, reduction='mean') # sum에서 mean으로 바꿔서 배치 크기 변화에 덜 민감하게\n",
        "        else:\n",
        "            l_obj = torch.tensor(0.0, device=device)\n",
        "\n",
        "        if eff_noobj.any():\n",
        "            l_noobj = F.binary_cross_entropy_with_logits(p_to[eff_noobj], torch.zeros_like(p_to[eff_noobj]), reduction='sum')\n",
        "        else:\n",
        "            l_noobj = torch.tensor(0.0, device=device)\n",
        "\n",
        "        # l_conf = l_obj + self.lambda_noobj * l_noobj -> YOLO v1 방식에서\n",
        "        # l_conf = F.binary_cross_entropy_with_logits(\n",
        "        #     p_conf, obj.float(), reduction=\"none\"\n",
        "        # )\n",
        "        # l_conf = (l_conf * ignore_mask).mean() # YOLO v2 방식으로 변경\n",
        "\n",
        "        # ===== 클래스 손실 (obj 위치에서만) =====\n",
        "        if self.num_classes > 1 and pos_mask.any():\n",
        "            smooth = 0.05 # 개선 : label smoothing → 오버피팅 완화 1) 0.1에서 0.05\n",
        "            t_cls_smoothed = t_cls[pos_mask] * (1 - smooth) + smooth / self.num_classes # → 기존 one-hot에 살짝 균등 확률을 섞어서 과적합 완화\n",
        "            # soft target 대응이 필요하면 KL divergence 사용 가능\n",
        "            l_cls = F.kl_div(F.log_softmax(p_cls[pos_mask], dim=-1), t_cls_smoothed, reduction='batchmean') # → soft target(부드러운 라벨)과 예측 분포의 KL divergence 계산\n",
        "        else:\n",
        "            l_cls = torch.tensor(0.0, device=device)\n",
        "\n",
        "            # 기존 cross-entropy 대신 KL divergence + label smoothing로 교체\n",
        "\n",
        "        # 최종 손실\n",
        "        loss = (\n",
        "            self.lambda_coord * l_coord +\n",
        "            l_obj +\n",
        "            self.lambda_noobj * l_noobj +\n",
        "            l_cls\n",
        "        )\n",
        "\n",
        "        return loss, {\n",
        "            \"coord\": l_coord.item(),\n",
        "            \"obj\": l_obj.item(),\n",
        "            \"noobj\": l_noobj.item(),\n",
        "            \"cls\": l_cls.item(),\n",
        "            \"total\": loss.item()\n",
        "        }\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "30236463",
      "metadata": {
        "id": "30236463"
      },
      "source": [
        "## 8) 학습 / 검증"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "7169ee10",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 502
        },
        "collapsed": true,
        "id": "7169ee10",
        "outputId": "32d92ebd-8036-49f2-f5cf-91a46df83974"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 44.7M/44.7M [00:00<00:00, 94.7MB/s]\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
            "  warnings.warn(warn_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train batch shape: torch.Size([8, 3, 416, 416])\n",
            "Target batch shape: torch.Size([8, 13, 13, 5, 52])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "train:  87%|████████▋ | 55/63 [1:05:43<09:33, 71.70s/it, total=234.773, yolo=234.138, coord=0.46, obj=0.94, noobj=454.79, cls(Y)=3.03, cls(C)=1.27]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2774309238.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[0mbest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1e9\u001b[0m \u001b[0;31m# 최적 검증 손실 초기값\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mEPOCHS\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 155\u001b[0;31m     \u001b[0mtl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_one_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# train loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    156\u001b[0m     \u001b[0mvl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalidate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# val loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-2774309238.py\u001b[0m in \u001b[0;36mtrain_one_epoch\u001b[0;34m(model, loader, optimizer, alpha_cls)\u001b[0m\n\u001b[1;32m    100\u001b[0m             \u001b[0mcrops\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcrops\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m             \u001b[0mcls_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mDEVICE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m             \u001b[0mcls_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcrops\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m             \u001b[0mcls_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcross_entropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1773\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torchvision/models/resnet.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    283\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    284\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 285\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    286\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    287\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torchvision/models/resnet.py\u001b[0m in \u001b[0;36m_forward_impl\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    272\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    273\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 274\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    275\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer4\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1773\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    242\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 244\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    245\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1773\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torchvision/models/resnet.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     90\u001b[0m         \u001b[0midentity\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1773\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    546\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    547\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 548\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    549\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    550\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    541\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroups\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    542\u001b[0m             )\n\u001b[0;32m--> 543\u001b[0;31m         return F.conv2d(\n\u001b[0m\u001b[1;32m    544\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdilation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroups\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    545\u001b[0m         )\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "# 모델/손실/최적화기 정의\n",
        "model = YOLOv2(num_classes=num_classes, anchors=anchors_px, grid_size=GRID_SIZE).to(DEVICE)\n",
        "criterion = YOLOv2Loss(anchors=anchors_px, num_classes=num_classes, iou_ignore=0.5).to(DEVICE)\n",
        "\n",
        "classifier = models.resnet18(weights=models.ResNet18_Weights.IMAGENET1K_V1)\n",
        "classifier.fc = nn.Linear(classifier.fc.in_features, num_classes)\n",
        "classifier = classifier.to(DEVICE)\n",
        "\n",
        "cls_transform = transforms.Compose([\n",
        "    transforms.Resize((224,224)),  # Tensor도 처리 가능\n",
        "    transforms.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225]),\n",
        "])\n",
        "\n",
        "LEARNING_RATE = 1e-4\n",
        "optimizer = optim.Adam(\n",
        "    list(model.parameters()) + list(classifier.parameters()),\n",
        "    lr=LEARNING_RATE\n",
        ")\n",
        "\n",
        "\n",
        "def decode_gt_and_make_crops(imgs, tgts, anchors_px, grid_size, input_size, device):\n",
        "    B, C, H, W = imgs.shape\n",
        "    S = grid_size\n",
        "    K = anchors_px.shape[0]\n",
        "    stride = float(input_size) / float(S)\n",
        "\n",
        "    gx = torch.arange(S, device=device).view(1,S).repeat(S,1)\n",
        "    gy = torch.arange(S, device=device).view(S,1).repeat(1,S)\n",
        "    grid_x = gx.unsqueeze(-1).repeat(1,1,K).float()\n",
        "    grid_y = gy.unsqueeze(-1).repeat(1,1,K).float()\n",
        "\n",
        "    anchors = torch.tensor(anchors_px, dtype=torch.float32, device=device)\n",
        "    aw = anchors[:,0].view(1,1,K)\n",
        "    ah = anchors[:,1].view(1,1,K)\n",
        "\n",
        "    crops, labels = [], []\n",
        "\n",
        "    for b in range(B):\n",
        "        t = tgts[b]  # (S,S,K,5+C)\n",
        "        obj = t[...,4] > 0.5\n",
        "        if not obj.any():\n",
        "            continue\n",
        "\n",
        "        cls_id = t[...,5:].argmax(dim=-1) if t.shape[-1]>5 else torch.zeros_like(obj, dtype=torch.long)\n",
        "\n",
        "        gt_cx = (t[...,0] + grid_x) * stride\n",
        "        gt_cy = (t[...,1] + grid_y) * stride\n",
        "        gt_w  = aw * torch.exp(t[...,2])\n",
        "        gt_h  = ah * torch.exp(t[...,3])\n",
        "\n",
        "        x1 = (gt_cx - gt_w/2).clamp(0,W-1)\n",
        "        y1 = (gt_cy - gt_h/2).clamp(0,H-1)\n",
        "        x2 = (gt_cx + gt_w/2).clamp(0,W-1)\n",
        "        y2 = (gt_cy + gt_h/2).clamp(0,H-1)\n",
        "\n",
        "        ys, xs, ks = torch.where(obj)\n",
        "        for yy, xx, kk in zip(ys.tolist(), xs.tolist(), ks.tolist()):\n",
        "            ix1, iy1, ix2, iy2 = int(x1[yy,xx,kk].item()), int(y1[yy,xx,kk].item()), int(x2[yy,xx,kk].item()), int(y2[yy,xx,kk].item())\n",
        "            if ix2-ix1 < 2 or iy2-iy1 < 2:\n",
        "                continue\n",
        "\n",
        "            # 항상 3채널 slice\n",
        "            crop = imgs[b, :3, iy1:iy2, ix1:ix2].detach()  # 이미 Tensor\n",
        "            crop = F.interpolate(crop.unsqueeze(0), size=(224,224), mode='bilinear', align_corners=False).squeeze(0)\n",
        "            crop = cls_transform(crop)\n",
        "            if crop.numel() == 0:\n",
        "                continue\n",
        "\n",
        "            # PIL 변환 없이 바로 tensor로 resize/normalize\n",
        "            crop = transforms.functional.resize(crop, (224,224))\n",
        "            crop = cls_transform(crop)\n",
        "            crops.append(crop.to(device))\n",
        "            labels.append(int(cls_id[yy,xx,kk].item()))\n",
        "\n",
        "    return crops, labels\n",
        "\n",
        "\n",
        "imgs, tgts = next(iter(train_loader))\n",
        "print(\"Train batch shape:\", imgs.shape)   # (B, C, H, W)\n",
        "print(\"Target batch shape:\", tgts.shape) # (B, S, S, K, 5+C)\n",
        "\n",
        "# --- 학습 루프 ---\n",
        "def train_one_epoch(model, loader, optimizer, alpha_cls=0.5):\n",
        "    model.train()\n",
        "    classifier.train()\n",
        "    total = 0.0\n",
        "    pbar = tqdm(loader, desc=\"train\")\n",
        "\n",
        "    for imgs, targets in pbar:\n",
        "        imgs = imgs.to(DEVICE).float()\n",
        "        targets = targets.to(DEVICE).float()\n",
        "\n",
        "        preds = model(imgs)\n",
        "        yolo_loss, comp = criterion(preds, targets)\n",
        "\n",
        "        crops, cls_labels = decode_gt_and_make_crops(imgs, targets, anchors_px, GRID_SIZE, INPUT_SIZE, DEVICE)\n",
        "        if len(crops) > 0:\n",
        "            crops = torch.stack(crops, dim=0)\n",
        "            cls_labels = torch.tensor(cls_labels, device=DEVICE, dtype=torch.long)\n",
        "            cls_outputs = classifier(crops)\n",
        "            cls_loss = F.cross_entropy(cls_outputs, cls_labels)\n",
        "        else:\n",
        "            cls_loss = torch.tensor(0.0, device=DEVICE)\n",
        "\n",
        "        loss = yolo_loss + alpha_cls * cls_loss\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        nn.utils.clip_grad_norm_(list(model.parameters()) + list(classifier.parameters()), 5.0)\n",
        "        optimizer.step()\n",
        "\n",
        "        total += loss.item()\n",
        "        pbar.set_postfix({\n",
        "            \"total\": f\"{loss.item():.3f}\",\n",
        "            \"yolo\":  f\"{yolo_loss.item():.3f}\",\n",
        "            \"coord\": f\"{comp['coord']:.2f}\",\n",
        "            \"obj\":   f\"{comp['obj']:.2f}\",\n",
        "            \"noobj\": f\"{comp['noobj']:.2f}\",\n",
        "            \"cls(Y)\":f\"{comp['cls']:.2f}\",\n",
        "            \"cls(C)\":f\"{cls_loss.item():.2f}\",\n",
        "        })\n",
        "\n",
        "    return total/len(loader)\n",
        "\n",
        "\n",
        "# --- 2. 검증 루프 ---\n",
        "@torch.no_grad()\n",
        "def validate(model, loader):\n",
        "    model.eval() # 평가 모드\n",
        "    total = 0.0\n",
        "    for imgs, tgts in loader:\n",
        "        imgs = imgs.to(DEVICE, non_blocking=True).float()\n",
        "        tgts = tgts.to(DEVICE, non_blocking=True).float()\n",
        "        preds = model(imgs)\n",
        "        loss, comp = criterion(preds, tgts)\n",
        "        total += loss.item()\n",
        "    return total/len(loader) # 평균 손실 반환\n",
        "\n",
        "@torch.no_grad()\n",
        "def save_val_visuals(model, val_loader, classifier, save_dir=\"val_vis\", max_images=4, conf_th=0.15, iou_th=0.45):\n",
        "    os.makedirs(save_dir, exist_ok=True)\n",
        "    model.eval()\n",
        "\n",
        "    n = min(max_images, len(val_items))\n",
        "    for i in range(n):\n",
        "        img_path = val_items[i][\"img_path\"]\n",
        "        drawn, _ = infer_image(model, img_path, conf_th=conf_th, iou_th=iou_th)\n",
        "        drawn.save(os.path.join(save_dir, f\"val_{i}.png\"))\n",
        "\n",
        "# --- 3. 전체 학습 반복 ---\n",
        "EPOCHS = 50\n",
        "best = 1e9 # 최적 검증 손실 초기값\n",
        "for ep in range(1, EPOCHS+1):\n",
        "    tl = train_one_epoch(model, train_loader, optimizer) # train loss\n",
        "    vl = validate(model, val_loader)  # val loss\n",
        "\n",
        "    save_val_visuals(model, val_loader, classifier, save_dir=\"val_vis\", max_images=4)\n",
        "\n",
        "    print(f\"[{ep:03d}] train={tl:.4f}  val={vl:.4f}\")\n",
        "\n",
        "    # 베스트 모델 저장\n",
        "    if vl < best:\n",
        "        best = vl\n",
        "        torch.save({\"model\":model.state_dict(), \"anchors\":anchors_px, \"classes\":classes}, \"yolov2_drawings_best.pt\")\n",
        "        print(\"  ↳ Best model saved.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aa7y6acxAveq",
      "metadata": {
        "collapsed": true,
        "id": "aa7y6acxAveq"
      },
      "outputs": [],
      "source": [
        "# 1. Noto CJK (중국어, 일본어, 한국어) 폰트 설치\n",
        "#    - Google에서 배포하는 CJK 전용 폰트 패키지\n",
        "!apt-get -y install fonts-noto-cjk\n",
        "# 2. 폰트 캐시 갱신\n",
        "#    - 새로 설치한 폰트를 시스템에 반영하기 위해 실행\n",
        "!fc-cache -fv"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2eb70c70",
      "metadata": {
        "id": "2eb70c70"
      },
      "source": [
        "## 9) 추론 디코딩 + NMS + 시각화"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7f982161",
      "metadata": {
        "id": "7f982161"
      },
      "outputs": [],
      "source": [
        "import numpy as np, math, torch\n",
        "from PIL import Image, ImageDraw, ImageFont\n",
        "\n",
        "# 시그모이드 (numpy 버전)\n",
        "def sigmoid_np(x):\n",
        "    return 1/(1+np.exp(-x))\n",
        "\n",
        "# (cx,cy,w,h) → (x1,y1,x2,y2) 변환\n",
        "def xywh_to_xyxy(box):\n",
        "    cx,cy,w,h = box\n",
        "    return [cx-w/2, cy-h/2, cx+w/2, cy+h/2]\n",
        "\n",
        "# 두 박스(xyxy)의 IoU 계산\n",
        "def iou_xyxy(a,b):\n",
        "    ax1,ay1,ax2,ay2 = a\n",
        "    bx1,by1,bx2,by2 = b\n",
        "    inter_x1 = max(ax1,bx1); inter_y1=max(ay1,by1)\n",
        "    inter_x2 = min(ax2,bx2); inter_y2=min(ay2,by2)\n",
        "    inter_w = max(0.0, inter_x2-inter_x1); inter_h=max(0.0, inter_y2-inter_y1)\n",
        "    inter = inter_w*inter_h\n",
        "    area_a = (ax2-ax1)*(ay2-ay1); area_b=(bx2-bx1)*(by2-by1)\n",
        "    union = area_a + area_b - inter + 1e-9\n",
        "    return inter/union\n",
        "\n",
        "# 클래스별 NMS (scores 내림차순 정렬 → IoU 임계 초과 항목 제거)\n",
        "def nms(boxes, scores, iou_th=0.5):\n",
        "    idxs = np.argsort(-scores)\n",
        "    keep=[]\n",
        "    while len(idxs)>0:\n",
        "        i = idxs[0]; keep.append(i)\n",
        "        if len(idxs)==1: break\n",
        "        rest = idxs[1:]\n",
        "        ious = np.array([iou_xyxy(boxes[i], boxes[j]) for j in rest])\n",
        "        idxs = rest[ious < iou_th]\n",
        "    return keep\n",
        "\n",
        "# ---------- 한글 폰트 & 가독성 좋은 드로잉 ----------\n",
        "def _get_font(size=16):\n",
        "    paths = [\n",
        "        \"/usr/share/fonts/truetype/noto/NotoSansCJK-Regular.ttc\",  # Colab 권장\n",
        "        \"/usr/share/fonts/truetype/noto/NotoSansCJK-Regular.otf\",\n",
        "        \"/Library/Fonts/AppleGothic.ttf\",\n",
        "        \"C:/Windows/Fonts/malgun.ttf\",\n",
        "    ]\n",
        "    for p in paths:\n",
        "        try:\n",
        "            return ImageFont.truetype(p, size)\n",
        "        except Exception:\n",
        "            continue\n",
        "    return ImageFont.load_default() # 최후의 수단\n",
        "\n",
        "def _draw_boxes_pretty(img_pil, boxes, scores, labels, idx2cls,\n",
        "                       base_thickness=2, base_font=16, pad=3):\n",
        "    \"\"\"\n",
        "    - 박스는 두 겹 테두리(검정→초록)로 시인성↑\n",
        "    - 라벨 배경은 흰색 박스 + 검정 테두리 + 검정 글씨\n",
        "    - 해상도에 따라 선굵기/폰트 자동 스케일링\n",
        "    \"\"\"\n",
        "    W, H = img_pil.size\n",
        "    scale = max(W, H) / 512.0\n",
        "    th = max(2, int(base_thickness * scale))\n",
        "    font = _get_font(max(12, int(base_font * scale)))\n",
        "    draw = ImageDraw.Draw(img_pil)\n",
        "\n",
        "    for (x1, y1, x2, y2), s, c in zip(boxes, scores, labels):\n",
        "        # 두 겹 테두리(검정 -> 초록)\n",
        "        draw.rectangle([x1, y1, x2, y2], outline=(0, 0, 0), width=th+2)\n",
        "        draw.rectangle([x1, y1, x2, y2], outline=(0, 255, 0), width=th)\n",
        "\n",
        "        # 라벨 텍스트 크기 계산\n",
        "        text = f\"{idx2cls[c]} {s:.2f}\"\n",
        "        tx1, ty1, tx2, ty2 = draw.textbbox((0, 0), text, font=font)\n",
        "        tw, th_text = tx2 - tx1, ty2 - ty1\n",
        "\n",
        "        # 박스 위에 공간이 없으면 박스 내부(아래쪽)에 표시\n",
        "        bx1 = max(0, int(x1))\n",
        "        by1 = int(y1 - th_text - 2*pad)\n",
        "        if by1 < 0:\n",
        "            by1 = int(y1)\n",
        "        bx2 = min(W, bx1 + tw + 2*pad)\n",
        "        by2 = min(H, by1 + th_text + 2*pad)\n",
        "\n",
        "        # 라벨 배경(흰색) + 테두리(검정) + 글씨(검정)\n",
        "        draw.rectangle([bx1, by1, bx2, by2], fill=(255, 255, 255))\n",
        "        draw.rectangle([bx1, by1, bx2, by2], outline=(0, 0, 0), width=1)\n",
        "        draw.text((bx1 + pad, by1 + pad), text, fill=(0, 0, 0), font=font)\n",
        "    return img_pil\n",
        "# ---------------------------------------------------\n",
        "\n",
        "@torch.no_grad()\n",
        "def infer_image(model, img_path, conf_th=0.25, iou_th=0.5):\n",
        "    \"\"\"\n",
        "    단일 이미지 추론 + 디코딩 + 클래스별 NMS + 시각화까지 수행.\n",
        "    필요 전역: INPUT_SIZE, GRID_SIZE, STRIDE, anchors_px, DEVICE, idx2cls, letterbox\n",
        "    반환: (시각화된 PIL.Image, (final_boxes, final_scores, final_labels))\n",
        "    \"\"\"\n",
        "    # 1) 이미지 로드 & letterbox\n",
        "    im = Image.open(img_path).convert(\"RGB\")\n",
        "    im_lb, scale, pad_x, pad_y = letterbox(im, INPUT_SIZE) # 416x416 캔버스\n",
        "    arr = np.array(im_lb).astype(np.float32)/255.0\n",
        "    arr = np.transpose(arr,(2,0,1))\n",
        "    t = torch.from_numpy(arr).unsqueeze(0).to(DEVICE)\n",
        "\n",
        "    # 2) 모델 추론\n",
        "    pred = model(t)[0].cpu().numpy()  # (S,S,K,5+C)\n",
        "\n",
        "    boxes_all=[]; scores_all=[]; labels_all=[]\n",
        "    # 3) 그리드/앵커별 디코딩 & confidence 필터링\n",
        "    for gy in range(GRID_SIZE):\n",
        "        for gx in range(GRID_SIZE):\n",
        "            for a in range(anchors_px.shape[0]):\n",
        "                tx,ty,tw,th,to = pred[gy,gx,a,:5]\n",
        "                cls_logits = pred[gy,gx,a,5:]\n",
        "\n",
        "                # objectness(sigmoid)와 class softmax를 곱해 최종 신뢰도\n",
        "                obj = sigmoid_np(to)\n",
        "                exp_logits = np.exp(cls_logits - cls_logits.max())\n",
        "                cls_prob = exp_logits/exp_logits.sum()\n",
        "                cidx = int(cls_prob.argmax())\n",
        "                conf = obj * cls_prob[cidx]\n",
        "                if conf < conf_th:\n",
        "                    continue\n",
        "\n",
        "                # YOLOv2 좌표 디코딩:\n",
        "                # cx,cy: 셀 내부 오프셋(sigmoid) + 셀 좌표 → 픽셀 단위로 복원\n",
        "                cx = (sigmoid_np(tx) + gx) * STRIDE\n",
        "                cy = (sigmoid_np(ty) + gy) * STRIDE\n",
        "                # w,h: anchor * exp(tw,th)\n",
        "                aw,ah = anchors_px[a]\n",
        "                bw = aw * math.exp(tw)\n",
        "                bh = ah * math.exp(th)\n",
        "                # xyxy 변환\n",
        "                x1,y1,x2,y2 = xywh_to_xyxy([cx,cy,bw,bh])\n",
        "\n",
        "                boxes_all.append([x1,y1,x2,y2]); scores_all.append(float(conf)); labels_all.append(cidx)\n",
        "\n",
        "\n",
        "    # 감지 없음 → 원본(letterboxed) 이미지 반환\n",
        "    if len(boxes_all)==0:\n",
        "        return im_lb, ([],[],[])\n",
        "\n",
        "    boxes_all = np.array(boxes_all, dtype=np.float32)\n",
        "    scores_all = np.array(scores_all, dtype=np.float32)\n",
        "    labels_all = np.array(labels_all, dtype=np.int32)\n",
        "\n",
        "    # 4) 클래스별 NMS\n",
        "    final_boxes, final_scores, final_labels = [], [], []\n",
        "    for c in np.unique(labels_all):\n",
        "        idxs = np.where(labels_all==c)[0]\n",
        "        if len(idxs)==0: continue\n",
        "        keep = nms(boxes_all[idxs], scores_all[idxs], iou_th)\n",
        "        for k in keep:\n",
        "            final_boxes.append(boxes_all[idxs][k].tolist())\n",
        "            final_scores.append(float(scores_all[idxs][k]))\n",
        "            final_labels.append(int(c))\n",
        "\n",
        "    # 5) 한글 폰트 + 고가독 라벨로 그리기\n",
        "    draw = im_lb.copy()\n",
        "    draw = _draw_boxes_pretty(draw, final_boxes, final_scores, final_labels, idx2cls)\n",
        "    return draw, (final_boxes, final_scores, final_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "gIV0T9S4BAyr",
      "metadata": {
        "id": "gIV0T9S4BAyr"
      },
      "outputs": [],
      "source": [
        "# 샘플 이미지 경로 (train_items, val_items 같은 리스트에서 하나 뽑아오기)\n",
        "sample_img = train_items[90][\"img_path\"]   # dataset 로딩 코드에서 만든 train_items 사용\n",
        "\n",
        "# 추론 및 시각화\n",
        "drawn, (boxes, scores, labels) = infer_image(\n",
        "    model,\n",
        "    sample_img,\n",
        "    conf_th=0.15,   # 낮추면 더 많은 박스 출력\n",
        "    iou_th=0.45\n",
        ")\n",
        "\n",
        "# 화면에 크게 표시\n",
        "drawn_resized = drawn.resize((700, 700))\n",
        "display(drawn_resized)\n",
        "\n",
        "# 박스/라벨 결과도 출력\n",
        "for b, s, l in zip(boxes, scores, labels):\n",
        "    print(f\"클래스: {idx2cls[l]}, 점수: {s:.3f}, 박스좌표: {b}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "71432384",
      "metadata": {
        "id": "71432384"
      },
      "source": [
        "## 10) 배치 추론 & 저장"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "36f703b9",
      "metadata": {
        "id": "36f703b9"
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "# 결과 저장할 폴더 생성\n",
        "out_dir = Path(\"./predictions_yolov2\"); out_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# 검증 데이터(val_items) 중 30장만 추론해서 저장\n",
        "for it in val_items[:30]:\n",
        "    img_p = it[\"img_path\"] # 원본 이미지 경로\n",
        "    drawn, _ = infer_image(model, img_p, conf_th=0.25, iou_th=0.45)  # YOLOv2 추론 + 시각화\n",
        "    save_p = out_dir / (Path(img_p).stem + \"_pred.jpg\") # 저장 경로 지정\n",
        "    drawn.save(save_p) # 결과 이미지 저장\n",
        "\n",
        "print(\"Saved to:\", out_dir.resolve()) # 저장 폴더 경로 출력"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Le4TFVcT4tbp",
      "metadata": {
        "id": "Le4TFVcT4tbp"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}